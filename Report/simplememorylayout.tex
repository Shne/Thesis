\section{Simple Algorithm with controlled Memory Layout and Skew}
\label{sec:memorylayout}
Mostly the same algorithm as the Simple, Na√Øve approach, but with a controlled memory layout and a skew of the tree.

We want to reduce the number of cache misses in our rank and select queries. To do this we to try and make it so that the next piece of memory the algorithm accesses is already loaded into a cacheline. One way of doing this is to have the data ordered in memory such that it will be accessed consecutively in memory, as this will mean full utilization of each cacheline as well as enabling prefetching of the next piece of memory to be effective.

\subsection{Prefetching}
Prefetching is a feature of the CPU whereby it can fetch other parts of the memory into cachelines even though it was not requested yet, if it believes it will be requested soon, to avoid having the program waiting for this fetching.
In more advanced versions, it can even look at the access into memory of the running program and try to determine a pattern and prefetch memory according to this pattern.
Looking at the Intel Optimization Manual~\cite{intel-optimization-manual} for our architecture~\footnote{Our architecture is Ivy Bridge, but the optimization manual sections for Sandy Bridge holds true for Ivy Bridge as well, as stated in section 2.2.7 in \cite{intel-optimization-manual}.} we find that it has a streaming prefetchers loading into L1, L2, and last level cache. The streaming prefetchers detect accesses to ascending or descending addresses and can prefetch up to 20 lines ahead or behind. It also has a prefetcher that can detect strides in memory access, as well as a \"Next Page Prefetcher\" when detecting memory accesses near the page boundary~\footnote{See section 2.2.7 of ~\cite{intel-optimization-manual}}.


\subsection{Skewing The Tree}
\label{sec:SkewingTheTree}
A way of achieving the sequential memory access in our case would be to skew the entire tree to one side, so that it is most likely that the queries will go to one side for most of the traversal and then control the memory layout so we can put that side of the tree next in memory.
Skewing the tree is done by changing the way we find which character in the alphabet to split on in each node's construction.
The split character is the last character in the alphabet of the left child node and to be able to skew the calculation we calculate it as
\[ \frac{alphabetSize-1}{skew} + alphabetMin \]
where $alphabetSize$ is the size of the alphabet at this node, $alphabetMin$ is the first character in the alphabet at this node, and $skew$ is the skew parameter which is 2 for a balanced tree and higher for skewed trees. E.g. a $skew$ value of 4 will mean we skew the tree by, in each node, putting 25\% of the alphabet into the left child node and 75\% into the right.
Notice also that the division is integer division, meaning anything past the decimal point is thrown away, or in other words, like calling a \texttt{floor()} function on the result of the division.

\subsection{Controlled Memory Layout}
We still want to support dynamic input and alphabet sizes without recompilation, so the nodes must be dynamically allocated on the heap.
It is impossible to know the size of the bitmap of each node before its input string is found by its parent, because the bitmap length in each node is the size of its input string.
Because of this, we cannot store the bitmaps as part of the nodes and neither can we simply use an array of bitmaps to ensure they are in consecutive order in memory.

The size of a node (not including its bitmap) is known at compile time as it contains simply pointers to parent node and left and right child nodes, as well as a boolean to flag it as a leaf node.
As such, we can and do allocate the memory for the nodes by allocating an array, then instantiating nodes into that array.
We pass a reference to a pointer into the array from parent to child nodes during construction, so they know where to allocate their child nodes.
The pointer points to the position of the last node in the array, and so before each instantiation of a new node, we increment the pointer so it points to free space, then place the new node there.

\subsection{Preallocating the Bitmaps}
Because the size of each bitmap is unknown at compile time, we cannot use an array, and so we must do it in another way. We still want the bitmaps stored consecutively in memory and because the bitmaps take up the most space, we would like to ensure that the bits are tightly packed.

We allocate the bitmaps as one giant bitmap the size of the maximum possible size required to store all the bitmaps for all the nodes. The sum of the size of all bitmaps on one layer of the tree can at most be $n$ and we can at most have $h$ layers, so the maximum size becomes
\[n \cdot h\]
where $n$ is the number of characters in the string and $h$ is the max height of a skewed binary tree and is defined in~\cite{Nievergelt:1972:BST:800152.804906} to be:
\[ h = \frac{log(2\sigma+1)}{ log(\frac{1}{1-\frac{1}{skew}})}\]
where $skew$ is the number we divide by to skew our tree.  We then store an offset and a size for the bitmap in each node, so we can index into the giant bitmap and access the bits corresponding to the node.
After having constructed the entire tree, we then shrink the giant bitmap to fit its actual size, to not waste the memory when the tree is in use for querying. Shrinking the bitmap takes less than a microsecond so it does not impact the construction time in any significant way. We use the \texttt{resize()} method to shrink the bitmap to the size of \texttt{bitmapOffset}, the counter that has been incremented to always point at the next free space in the bitmap during the construction and so is also the actual filled size of the bitmap.
If we had allocated a bitmap for each node individually, they would have been word-aligned, and the bits between the end of one bitmap and the start of another would have gone unused and so, wasted.

The nodes now contain, in addition to the previously mentioned pointers, a pointer to the large bitmap, an offset and a size.



\subsection{Experiments}

\subsubsection{Query time when skewing the Wavelet Tree using uncontrolled and controlled memory-layout}

\begin{figure}
\caption{Rank and Select Running times on Wavelet Tree with increasing skew for naive construction and preallocated construction}
\label{fig:NaiveRankSelectSkewRunningTime}
\input{naiveRankSelectSkewRunningTime}
\end{figure}


In this experiment we want to test whether querying our controlled memory version of the Wavelet Tree is faster than querying the uncontrolled memory version for increasingly skewed trees. 

We have run 100 \texttt{rank} queries and 100 \texttt{select} queries for each skew and registered the total running time for those 100 queries. 
We did 5 sample runs for each skew and then used the median running time of these. 
This was done to minimize the effect of outliers in the test. The 100 runs was simply done to increase the general size of the running time to better see the differences. The results can be seen in Figure ~\ref{fig:NaiveRankSelectSkewRunningTime}.

We skew the Wavelet Tree (see section~\ref{sec:SkewingTheTree}) because we believe it has better performance than not skewing it since data can then be stored sequentially in memory and reduce cache-misses and should then be faster to access when querying. 
This was not the case meaning that storing data sequentially gives significant 
The query time increases linearly with increasing skew for both controlled and uncontrolled memory layout and looking at the results in Figure ~\ref{fig:NaiveRankSelectSkewRunningTime} we can also see that the two approaches are quite equal for both rank and select in terms of running time when comparing the two construction methods.

\subsubsection{Query cache misses when skewing the Wavelet Tree using uncontrolled and controlled memory-layout}
To see why controlling the memory ourselves is not better than not controlling it we look at cache misses, which are shown in Figure ~\ref{fig:NaivePreallocatedRankSkewCacheMisses} for \texttt{Rank} and in Figure ~\ref{fig:NaivePreallocatedSelectSkewCacheMisses} for \texttt{Select}.
The cache misses for \textit{Level 1} (when comparing the two construction methods) are almost equal while \textit{Level 2} and \textit{Level 3} cache misses varies a bit with respect to each construction method. 
The same tendency as in Figure ~\ref{fig:NaiveRankSelectSkewRunningTime} is observed: 
The cache misses also increases linearly with the skew factor. 

The reason why there are more \textit{Level 1} cache misses than \textit{Level 2} and \textit{Level 3} cache misses is because the size of the caches are larger in higher levels than in lower levels. 
This can also explain why the difference in \textit{Level 1} cache misses is so small. 
The \textit{Level 1} cache simply gets filled up quickly and is then often missed. \textit{Level 2} and \textit{Level 3} are larger, which means that they do not reach their limit as quickly, resulting in fewer and more varied cache miss values.

\begin{figure}
\caption{Rank Cache Misses on Wavelet Tree with increasing skew}
\label{fig:NaivePreallocatedRankSkewCacheMisses}
\centering
\begin{subfigure}{\textwidth}
	\caption{Naive L1 vs. Preallocated L1}
	\label{fig:L1NaivePreallocatedRankSkewCacheMisses}
	\input{L1NaiveVsPreallocatedSkewRankQueryCacheMisses}
	\vspace*{5 mm}
\end{subfigure}
\begin{subfigure}{\textwidth}
	\caption{Naive L2, L3 vs. Preallocated L2, L3}
	\label{fig:L2L3NaivePreallocatedRankSkewCacheMisses}
 	\input{L2L3NaiveVsPreallocatedSkewRankQueryCacheMisses}
\end{subfigure}
\end{figure}

\begin{figure}
\caption{Select Cache Misses on Wavelet Tree with increasing skew}
\label{fig:NaivePreallocatedSelectSkewCacheMisses}
\centering
\begin{subfigure}{\textwidth}
	\caption{Naive L1 vs. Preallocated L1}
	\label{fig:L1NaivePreallocatedSelectSkewCacheMisses}
	\input{L1NaiveVsPreallocatedSkewSelectQueryCacheMisses}
	\vspace*{5 mm}
\end{subfigure}
\begin{subfigure}{\textwidth}
	\caption{Naive L2, L3 vs. Preallocated L2, L3}
	\label{fig:L2L3NaivePreallocatedSelectSkewCacheMisses}
 	\input{L2L3NaiveVsPreallocatedSkewSelectQueryCacheMisses}
\end{subfigure}
\end{figure}

Controlling the memory is in essence not better than letting the OS control it, which indicates that the effect of how we control memory is equal to the effect of how OS does it.
Skewing the tree is not an improvement over not skewing it. 
The hight of the Wavelet Tree increases so much that query overhead is too high to gain anything from the possible faster data access.


\subsubsection{Query vs. Build Time when skewing controlled memory-layout}





