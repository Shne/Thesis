\section{Simple Algorithm with controlled Memory Layout and Skew}
\label{sec:memorylayout}
In this section, we describe our attempt to improve the query times for the wavelet tree by controlling the memory layout and skewing the tree.
Brodal et al.~\citeA{gerthSkewedBinarySearchTrees} showed that skewing a binary search tree could reduce the amount of cache misses and branch mispredictions considerately. Enough, in fact, to increase the speed of searching the tree manyfold, even though the skewing increased the depth of the tree structure.

Skewing the tree can reduce branch mispredictions by giving one branch, in the direction the tree is skewed, much higher probability of being the correct than the other, and so enabling the branch predicter unit to more often predict correctly. 
Skewing the tree can also reduce cache misses by increasing the probability that the next piece of memory the algorithm accesses is already loaded into a cacheline by the time it is accessed because of prefetching.

To reduce cache misses by skewing the tree we must control the memory layout, because by skewing the tree to the right, we increase the likelihood of a traversal similar to a depth-first-search going down the right side first (DFSr). So we want to place the data in memory so that a DFSr traversal through the tree would result in sequential address accesses.
Allocating memory dynamically as we go might produce a similar layout and controlling the memory may not lead to increased performance, but it is the only way to ensure the memory layout is as we want it.

Skewing the tree has the disadvantage of increasing the height, or depth, of the tree 
and is defined for a skewed binary tree by J.~Nievergelt and E.~M. Reingold~\citeB{Nievergelt:1972:BST:800152.804906} to be:
\[ h = \frac{\log(2\sigma+1)}{ \log\frac{1}{1-\alpha}}\;,\]
where $\alpha = \frac{1}{\mathit{skew}}$ and \textit{skew} is the number that is divided by to skew our tree, see Section~\ref{sec:SkewingTheTree}.

The algorithms in this implementation remain mostly the same algorithms as in the Simple, Naïve approach, but with modifications to handle a controlled memory layout and a skew of the tree.
The Naïve approach has also been altered to support skewing in order to be able to compare whether the controlled memory layout has the desired effect.

\subsection{Prefetching}
Prefetching is a feature of the CPU whereby it can fetch other parts of the memory into cachelines even though it was not requested yet, if it believes it will be requested soon, to avoid having the program waiting for this fetching.
In more advanced versions, it can even look at the access into memory of the running program and try to determine a pattern and prefetch memory according to this pattern.
Looking at the Intel Optimization Manual~\citeA{intel-optimization-manual} for our architecture\footnote{Our architecture is Ivy Bridge, but the optimization manual sections for Sandy Bridge holds true for Ivy Bridge as well, as stated in section 2.2.7 in~\citeA{intel-optimization-manual}.} we find that it has streaming prefetchers loading into L1, L2, and last level cache. The streaming prefetchers detect accesses to ascending or descending addresses and can prefetch up to 20 lines ahead or behind. 
Our architecture also has a prefetcher that can detect strides in memory access, as well as a “Next Page Prefetcher” that can load another memory page when detecting memory accesses near the page boundary~\footnote{See section 2.2.7 of~\citeA{intel-optimization-manual}}.


\subsection{Skewing The Tree}
\label{sec:SkewingTheTree}
Skewing the tree is done by changing the way we find which character in the alphabet to split on in each node's construction.
The split character is the last character in the alphabet of the left child node and to be able to skew the calculation we calculate it as
\[\mathit{SplitCharacter} = \left\lfloor \frac{\mathit{alphabetSize}-1}{\mathit{skew}} + \mathit{alphabetMin} \right\rfloor \]
where \textit{alphabetSize} is the size of the alphabet at this node, \textit{alphabetMin} is the first character in the alphabet at this node, and \textit{skew} is the skew parameter which is 2 for a balanced tree and higher for right-skewed trees. E.g. a \textit{skew} value of 4 skews the tree by 75\,\% to the right so that, in each node, 25\,\% of the alphabet is put into the left child node and 75\,\% is put into the right child node.
We only use integer values as characters, so the calculated split character is rounded down.


\subsection{Controlled Memory Layout}
We still want to support dynamic input and alphabet sizes without recompilation, so the nodes must be dynamically allocated on the heap.

The size of a node is known at compile time as it contains fixed-size pointers to the parent node and left and right child nodes, as well as a boolean to flag it as a leaf node and its bitmap as a vector, which internally stores a pointer to its backing array.
As such, the memory for the nodes is allocated by allocating an array and then instantiating the nodes into that array.
A reference to a pointer is passed into the array from parent to child nodes during construction, so they know where to allocate their child nodes.
The pointer points to the position of the last node in the array, and so before each instantiation of a new node, we increment the pointer so it points to free space, then place the new node there.

The memory layout of the bitmaps is not controlled (yet), because skewing the tree will not help the prefetcher with regards to the bitmaps, except in a few specific cases, because of the way the bitmaps are used and the resulting access patterns.
The algorithms for rank and select stop querying each bitmap at some position inside the bitmap and then continue to the next bitmap in the next node.
Rank stops when it reaches the position the query was searching up to, given as a parameter.
Select stops when it has found the sought number of occurrences in the bitmap.
In both these cases the rest of the bitmap is not used and any such data the prefetcher has fetched would have been in vain.
The prefetcher cannot tell from the algorithms access pattern when it will jump ahead to the next bitmap, and every such jump will therefore give rise to a cache miss.
The problem is shown in Figure~\ref{fig:QueryPrefetchFigure}. The drawing assumes the bitmap is stored sequentially and the prefetcher prefetches the next cache line (colored green), but the algorithm stops at some position and skips ahead to the next bitmap. 
This makes it unable to utilize the prefetched data and will try to access memory that is not in the cache yet; a cache miss.
So regardless of where the bitmap that is accessed next is stored, following right after the first or elsewhere in memory, a cache miss will occur.

The exceptions to this are when either the entire bitmap is used for the query, that is, when the rank query is for the entire string, or the bitmap is small enough that the beginning of the next bitmap can fit within the same word.
The first case is not a very common query in most use cases, and the second case is rare when the input string is much bigger than the alphabet, and would only happen near the leaf nodes.
Neither scenario happens often enough to warrant controlling the bitmap memory layouts.

\figureBegin

\includegraphics[width=\textwidth]{QueryPrefetchFigure.pdf}
\caption{How access patterns in a concatenated bitmap can defeat cache prefetching}
\label{fig:QueryPrefetchFigure}
\figureEnd



\subsection{Experiments}

\subsubsection{Queries when skewing the Wavelet Tree using uncontrolled and controlled memory layout}
In this experiment we want to test whether querying our controlled memory version of the Wavelet Tree is faster than querying the uncontrolled memory version for increasingly skewed trees. 

\paragraph{Test Setup}~\\
The general setup is as described in Section~\ref{sec:ExpNotesGeneralSetup}.
The query parameters were chosen as described in Section~\ref{sec:choiceOfQueryParameters}.
The results can be seen in Figure~\ref{fig:NaiveRankSkewRunningTime} and Figure~\ref{fig:NaiveSelectSkewRunningTime}.
We skew the wavelet tree as described in Section~\ref{sec:SkewingTheTree}).

\paragraph{Results}~\\
If we look at Figure~\ref{fig:NaiveRankSkewRunningTime} and Figure~\ref{fig:NaiveSelectSkewRunningTime} we can see that the query time increases linearly with increasing skew for both controlled and uncontrolled memory layout. We also observe that the two memory layout approaches are almost equal for both rank and select queries in terms of running time.
We can conclude that skewing the tree is not an improvement on performance.

To understand why skewing the tree is no improvement we need to figure out what causes the increase in query time. 
First we look at branch mispredictions to see if they actually decrease when we increase the skew factor.%

 
Figure~\ref{fig:NaiveVsControlledNodeMemorySkewRankQueryBMrate} and Figure~\ref{fig:NaiveVsControlledNodeMemorySkewSelectQueryBMrate} shows the branch misprediction rate for rank and select queries on a wavelet tree with increasing skew factor for both the controlled and uncontrolled memory approach. 
The branch misprediction rate is calculated by dividing the number of branch mispredictions with the total number of conditional branches.

We observe that the branch misprediction rate is decreasing as we expected. 
This means that it is not branch mispredictions that causes the increase in query time. 

We now look at cache misses to see if they can explain the increase in query time. 
The values are shown in Figure~\ref{fig:L1NaiveControlledNodeMemoryRankSkewCacheMisses} and Figure~\ref{fig:L2L3NaiveControlledNodeMemoryRankSkewCacheMisses} for Rank and in Figure~\ref{fig:L1NaiveControlledNodeMemorySelectSkewCacheMisses} and Figure~\ref{fig:L2L3NaiveControlledNodeMemorySelectSkewCacheMisses} for Select.
The same tendency as in Figure~\ref{fig:NaiveRankSkewRunningTime} and Figure~\ref{fig:NaiveSelectSkewRunningTime} is observed: 
The cache misses increase linearly with the skew factor.

The cache misses for level 1 for Naive and Controlled Memory are almost exactly equal which might be because even the smallest bitmaps fills out the cache line resulting in cache misses even when those bitmaps are accessed. 
The cache misses of level 2 and 3 for Naive and Controlled Memory are also equal. 
Based on this we can conclude that controlling the memory ourselves has no advantage over not doing so with regards to the performance of queries.

Looking at the level 2 cache miss rates for rank and select, as plotted in Figure~\ref{fig:NaiveVsControlledNodeMemorySkewRankQuery_L2_DCMrate} and Figure~\ref{fig:NaiveVsControlledNodeMemorySkewSelectQuery_L2_DCMrate} we can see that the Data Cache Miss Rate for the level 2 cache increases for higher values of skew, meaning that it is not only a matter of an increased amount of cycles caused by the higher amount of cache misses, but that more cache accesses miss as skew increase meaning that the percentage of cache misses are increasing.

Figure~\ref{fig:NaiveVsControlledNodeMemorySkewRankQueryTLB} and Figure~\ref{fig:NaiveVsControlledNodeMemorySkewSelectQueryTLB} shows TLB misses for \textproc{Rank} and \textproc{Select}. 
The TLB misses varies a lot for increasing skew and does not seem to follow any discernible pattern which makes it difficult to say anything conclusive about these graphs. 
Figure~\ref{fig:NaiveVsControlledNodeMemorySkewSelectQueryTLB} does have two peaks at skew 2.8 and 4.2 that looks somewhat alike, but we have not been able to find any good explanation as to why they occur and they do not seem to have any visible effect on the query time when looking at Figure~\ref{fig:NaiveRankSkewRunningTime} and Figure~\ref{fig:NaiveSelectSkewRunningTime}.
There are no peaks in query time when skew is 2.8 or~4.2.

Skewing the tree is not an improvement. 
Controlling the memory layout of the nodes does not improve query time because all the work is being done in the bitmaps of the nodes.
It is when accessing these bitmaps that the cache misses occur.
When the tree is skewed its height is increased and as a result more bitmaps needs to be accessed before a query is finished. 
This produces more cache misses than when the tree is binary balanced.

It is worth mentioning that a Huffman shaped Wavelet Tree is skewed and increases query performance but only for non-uniform data.
Since we use uniform data a balanced tree seems to be the best solution.





When the tree is skewed the alphabet is not split into equal sizes and therefore more data is added to the right side of the tree than to the left. 
This increases the hight of the tree and we need to look at more nodes to find the result than if the tree was balanced and as a result of this we do more work because we have to access more nodes and bitmaps.

We had hoped to reduce the amount of branch mispredictions and cache misses enough that they would reduce the running time more than the increase from having a deeper tree structure.
Sadly, this was not the case. We believe the reason it worked so well for Brodal et. al.\citeA{gerthSkewedBinarySearchTrees} but not for us, is that the binary search trees they worked on have much less work involved in traversing each node and so most of the time is spent traversing from node to node.
In rank and select queries on a wavelet tree, most of the time is spent calculating the binary ranks of the bitmaps of each node as well as other calculations performed for each node.

Skewing the tree can help reduce the time spent between nodes by reducing the performance hits from branch mispredictions and cache misses that arise because of the unpredictable nature of a balanced tree traversal, by making the path of the traversal much more predictable and pre-loadable.
It can not, however, help reduce the work performed when reaching each node, and when the total time spent on that work is proportional to the height of the tree, as it is with a wavelet tree, it is a great reduction in speed to skew the tree and thereby increase the height of the tree.
The total intra-node work is proportional to the height of the tree because that work is proportional to the bitmap size and the bitmap sizes are inversely proportional to the number of nodes in the same layer, as the size of all the bitmaps in one layer sum up to $n$. A skewed tree does not have each layer filled with nodes and so the rest of the bitmaps on those levels become larger, as well as any extra layer produced by skewing the tree means another number of bitmaps must appear with combined size $n$.



\newgeometry{left=2cm,right=2cm, top=2cm, bottom=3cm}

\begin{figure}\tiny
\begin{subfigure}{0.48\textwidth}
	\input{naiveRankSkewRunningTime}
	\caption{Wall Time}
	\label{fig:NaiveRankSkewRunningTime}
\end{subfigure}
\hfill
\begin{subfigure}{0.48\textwidth}
	\input{NaiveVsControlledNodeMemorySkewRankQuery_BMrate}
	\caption{Branch Misprediction Rate}
	\label{fig:NaiveVsControlledNodeMemorySkewRankQueryBMrate}
\end{subfigure}


\begin{subfigure}{0.48\textwidth}
	\input{L1NaiveVsControlledNodeMemorySkewRankQueryCacheMisses}
	\caption{Level 1 Data Cache Misses}
	\label{fig:L1NaiveControlledNodeMemoryRankSkewCacheMisses}
\end{subfigure}
\hfill
\begin{subfigure}{0.48\textwidth}
 	\input{L2L3NaiveVsControlledNodeMemorySkewRankQueryCacheMisses}
	\caption{Level 2 Data and 3 Total Cache Misses}
	\label{fig:L2L3NaiveControlledNodeMemoryRankSkewCacheMisses}
\end{subfigure}

\begin{subfigure}{0.48\textwidth}
	\input{NaiveVsControlledNodeMemorySkewRankQuery_L2_DCMrate}
	\caption{Level 2 Data Cache Miss Rate}
	\label{fig:NaiveVsControlledNodeMemorySkewRankQuery_L2_DCMrate}
\end{subfigure}
\hfill
\begin{subfigure}{0.48\textwidth}
	\input{NaiveVsControlledNodeMemorySkewRankQueryTLB}
	\caption{TLB Misses}
	\label{fig:NaiveVsControlledNodeMemorySkewRankQueryTLB}
\end{subfigure}

\caption{Various measurements for Rank queries on SimpleNaive and ControlledMemory wavelet trees with various skew.}
\label{fig:NaiveVsControlledNodeMemorySkewRankQuery}

\end{figure}






\begin{figure}\tiny
\begin{subfigure}{0.48\textwidth}
	\input{naiveSelectSkewRunningTime}
	\caption{Wall Time}
	\label{fig:NaiveSelectSkewRunningTime}
\end{subfigure}
\hfill
\begin{subfigure}{0.48\textwidth}
	\input{NaiveVsControlledNodeMemorySkewSelectQuery_BMrate}
	\caption{Branch Misprediction Rate}
	\label{fig:NaiveVsControlledNodeMemorySkewSelectQueryBMrate}
\end{subfigure}

\begin{subfigure}{0.48\textwidth}
	\input{L1NaiveVsControlledNodeMemorySkewSelectQueryCacheMisses}
	\caption{Level 1 Data Cache Misses}
	\label{fig:L1NaiveControlledNodeMemorySelectSkewCacheMisses}
\end{subfigure}
\hfill
\begin{subfigure}{0.48\textwidth}
 	\input{L2L3NaiveVsControlledNodeMemorySkewSelectQueryCacheMisses}
	\caption{Level 2 Data and 3 Total Cache Misses}
	\label{fig:L2L3NaiveControlledNodeMemorySelectSkewCacheMisses}
\end{subfigure}

\begin{subfigure}{0.48\textwidth}
	\input{NaiveVsControlledNodeMemorySkewSelectQuery_L2_DCMrate}
	\caption{Level 2 Data Cache Miss Rate}
	\label{fig:NaiveVsControlledNodeMemorySkewSelectQuery_L2_DCMrate}
\end{subfigure}
\hfill
\begin{subfigure}{0.48\textwidth}
	\input{NaiveVsControlledNodeMemorySkewSelectQueryTLB}
	\caption{TLB Misses}
	\label{fig:NaiveVsControlledNodeMemorySkewSelectQueryTLB}
\end{subfigure}

\caption{Various measurements for Select queries on SimpleNaive and ControlledMemory wavelet trees with various skew.}
\label{fig:NaiveVsControlledNodeMemorySkewSelectQuery}

\end{figure}


\restoregeometry