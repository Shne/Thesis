\section{Simple Algorithm with controlled Memory Layout and Skew}
\label{sec:memorylayout}
In this section, we describe our attempt to improve the query times for the wavelet tree by controlling the memory layout and skewing the tree.
Brodal et al.~\cite{gerthSkewedBinarySearchTrees} showed that skewing a binary search tree could reduce the amount of cache misses and branch mispredictions considerately. Enough, in fact, to increase the speed of searching the tree manyfold, even though the skewing increased the depth of the tree structure.

Skewing the tree can reduce branch mispredictions by making one branch, in the direction the tree is skewed, much more likely that the other, and so enabling the branch predicter unit to more often predict correctly. 
Skewing the tree can reduce and cache misses by making it very likely that the next piece of memory the algorithm accesses is already loaded into a cacheline by the time it is accessed.

To reduce cache misses by skewing the tree we must control the memory layout, because by skewing the tree to the right, we make a DFSr-like traversal more likely and so we want to place the data in memory so that a DFSr traversal through the tree would be sequential address accesses.
Allocating memory dynamically as we go might produce a similar layout and controlling the memory may not lead to increased performance, but it is the only way to ensure the memory layout is as we want it.

Skewing the tree has the disadvantage of increasing the height, or depth, of the tree 
and is defined for a skewed binary tree by J.~Nievergelt and E.~M. Reingold~\cite{Nievergelt:1972:BST:800152.804906} to be:
\[ h = \frac{log(2\sigma+1)}{ log(\frac{1}{1-\alpha})}\]
where $\alpha = \frac{1}{skew}$ and $skew$ is the number we divide by to skew our tree, see section~\ref{sec:SkewingTheTree}.

The algorithms remain mostly the same algorithm as the Simple, Naïve approach, but with modifications to handle a controlled memory layout and a skew of the tree.
We have also altered the Naïve approach to support skewing, so we can compare whether the controlled memory layout has the desired effect.

\subsection{Prefetching}
Prefetching is a feature of the CPU whereby it can fetch other parts of the memory into cachelines even though it was not requested yet, if it believes it will be requested soon, to avoid having the program waiting for this fetching.
In more advanced versions, it can even look at the access into memory of the running program and try to determine a pattern and prefetch memory according to this pattern.
Looking at the Intel Optimization Manual~\cite{intel-optimization-manual} for our architecture\footnote{Our architecture is Ivy Bridge, but the optimization manual sections for Sandy Bridge holds true for Ivy Bridge as well, as stated in section 2.2.7 in \cite{intel-optimization-manual}.} we find that it has streaming prefetchers loading into L1, L2, and last level cache. The streaming prefetchers detect accesses to ascending or descending addresses and can prefetch up to 20 lines ahead or behind. It also has a prefetcher that can detect strides in memory access, as well as a "Next Page Prefetcher" that can load another memory page when detecting memory accesses near the page boundary~\footnote{See section 2.2.7 of ~\cite{intel-optimization-manual}}.


\subsection{Skewing The Tree}
\label{sec:SkewingTheTree}
Skewing the tree is done by changing the way we find which character in the alphabet to split on in each node's construction.
The split character is the last character in the alphabet of the left child node and to be able to skew the calculation we calculate it as
\[SplitCharacter = \lfloor \frac{alphabetSize-1}{skew} + alphabetMin \rfloor \]
where $alphabetSize$ is the size of the alphabet at this node, $alphabetMin$ is the first character in the alphabet at this node, and $skew$ is the skew parameter which is 2 for a balanced tree and higher for right-skewed trees. E.g. a $skew$ value of 4 will mean we skew the tree by, in each node, putting 25\% of the alphabet into the left child node and 75\% into the right.
We then round the value down so it is an actual integer character instead of a decimal.



\subsection{Controlled Memory Layout}
We still want to support dynamic input and alphabet sizes without recompilation, so the nodes must be dynamically allocated on the heap.

The size of a node is known at compile time as it contains simply pointers to parent node and left and right child nodes, as well as a boolean to flag it as a leaf node and its bitmap as a vector, which internally stores a pointer to its backing array.
As such, we can and do allocate the memory for the nodes by allocating an array, then instantiating nodes into that array.
We pass a reference to a pointer into the array from parent to child nodes during construction, so they know where to allocate their child nodes.
The pointer points to the position of the last node in the array, and so before each instantiation of a new node, we increment the pointer so it points to free space, then place the new node there.

We don't control the memory layout of the bitmaps (yet), because skewing the tree will not help the prefetcher with regards to the bitmaps, except in a few specific cases, because of the way the bitmaps are used and the resulting access patterns.
The algorithms for rank and select stop querying each bitmap at some position inside the bitmap and then continue to the next bitmap in the next node.
Rank stops when it reaches the position the query was searching up to, given as a parameter.
Select stops when it has found the sought number of occurrences in the bitmap.
In both these cases the rest of the bitmap is not used and any such data the prefetcher has fetched would have been in vain.
The prefetcher cannot tell from the algorithms access pattern when it will jump ahead to the next bitmap, and every such jump will therefore give rise to a cache miss.
The problem is shown in figure ~\ref{fig:QueryPrefetchFigure}. The drawing assumes the bitmap is stored sequentially and the prefetcher prefetches the next cache line (colored green), but the algorithm stops at some position and skips ahead to the next bitmap. This makes it not utilize the prefetched data (green) and try to access memory not yet in the cache; a cache miss.
So regardless of where the bitmap accessed next is stored, following right after the first or elsewhere in memory, a cache miss will occur.

The exceptions to this are when either the entire bitmap is used for the query, that is, when the rank query is for the entire string, or the bitmap is small enough that the beginning of the next bitmap can fit within the same word.
There are better data structures for answering rank queries on the entire string, and we believe this type of query to be rarely used on any practical use of a wavelet tree.
The second case is rare when the input string is bigger than the alphabet, and would only happen near the leaf nodes.

\begin{figure}[h]
\caption{Why concatenating bitmaps doesn't enable cache prefetching between bitmaps}
\label{fig:QueryPrefetchFigure}
\includegraphics[width=\textwidth]{QueryPrefetchFigure.pdf}
\end{figure}



\subsection{Experiments}

\subsubsection{Queries when skewing the Wavelet Tree using uncontrolled and controlled memory layout}

\begin{figure}
\caption{Rank and Select Running times on Wavelet Tree with increasing skew for naive construction and ControlledNodeMemory construction}
\label{fig:NaiveRankSelectSkewRunningTime}
\input{naiveRankSelectSkewRunningTime}
\end{figure}


In this experiment we want to test whether querying our controlled memory version of the Wavelet Tree is faster than querying the uncontrolled memory version for increasingly skewed trees. 

\paragraph{Test Setup}~\\
We have run 1000 \texttt{rank} queries and 1000 \texttt{select} queries for each skew and registered the total running time for each set of 1000 queries. 
We did 5 such sets for each skew and then used the average running time of these. 
This was done to minimize the effect of outliers in the test. The 1000 queries was done for characters even spread across the alphabet for both rank and select. The results can be seen in Figure ~\ref{fig:NaiveRankSelectSkewRunningTime}.

We skew the Wavelet Tree (see section~\ref{sec:SkewingTheTree}) because we believe it has better performance than not skewing it since data can then be stored sequentially in memory and reduce cache-misses because of prefetching and should then be faster to access when querying. %%
Skewing should also reduce branch mispredictions since branching to the right become more probable than branching to the left. 

If we look at figure ~\ref{fig:NaiveRankSelectSkewRunningTime} we can see that the query time increases linearly with increasing skew for both controlled and uncontrolled memory layout. We also observe that the two memory layout approaches are almost equal for both rank and select queries in terms of running time.

This means we can conclude that skewing the tree is not an improvement on performance.

\paragraph{Measurements}~\\
To understand why skewing the tree is no improvement we need to figure out what causes the increase in query time. 
First we look at branch mispredictions to see if they actually decrease when we increase the skew factor.%
\begin{figure}
\caption{Rank Branch Mispredictions rate}
\label{fig:NaiveVsControlledNodeMemorySkewRankQueryBMrate}
\input{NaiveVsControlledNodeMemorySkewRankQuery_BMrate}
\end{figure}%
\begin{figure}
\caption{Select Branch Mispredictions rate}
\label{fig:NaiveVsControlledNodeMemorySkewSelectQueryBMrate}
\input{NaiveVsControlledNodeMemorySkewSelectQuery_BMrate}
\end{figure}
 
Figure ~\ref{fig:NaiveVsControlledNodeMemorySkewRankQueryBMrate} and \ref{fig:NaiveVsControlledNodeMemorySkewSelectQueryBMrate} shows the branch misprediction rate for rank and select queries on a wavelet tree with increasing skew factor for both the controlled and uncontrolled memory approach. 
The branch misprediction rate is calculated by dividing the number of branch mispredictions with the total number of conditional branches.

We observe that the branch misprediction rate is decreasing as we expected. 
This means that it is not branch mispredictions that causes the increase in query time. 

We now look at cache misses to see if they can explain the increase in query time. 
The values are shown in Figure ~\ref{fig:NaiveControlledNodeMemoryRankSkewCacheMisses} for Rank and in Figure ~\ref{fig:NaiveControlledNodeMemorySelectSkewCacheMisses} for Select.
The same tendency as in Figure ~\ref{fig:NaiveRankSelectSkewRunningTime} is observed: 
The cache misses increase linearly with the skew factor.

In Figure~\ref{fig:L2L3NaiveControlledNodeMemoryRankSkewCacheMisses} and \ref{fig:L2L3NaiveControlledNodeMemorySelectSkewCacheMisses} we do generally see a difference in level 2 and 3 cache misses especially for higher skew values between the controlled and uncontrolled memory layouts. For Select queries, controlled memory layout has fewer cache misses then Naive, but for Rank the opposite is true.

The cache misses for level 1 for Naive and Controlled Memory are almost exactly equal which might be because even the smallest bitmaps fills out the cache line resulting in cache misses even when those bitmaps are accessed.

Looking at the level 2 cache miss rates for rank and select, as plotted in Figure~\ref{fig:NaiveVsControlledNodeMemorySkewRankQuery_L2_DCMrate} and \ref{fig:NaiveVsControlledNodeMemorySkewSelectQuery_L2_DCMrate} we can see that the Data Cache Miss Rate for the level 2 cache increases for higher values of skew, meaning that it is not only a matter of increased amount of cycles causing the higher amount of cache misses, but that more cache accesses miss as skew increase meaning that the percentage of cache misses are increasing.
This means that it is not an improvement with regards to cache misses to skew the tree when querying.

\begin{figure}
\caption{Rank Cache Misses on Wavelet Tree with increasing skew}
\label{fig:NaiveControlledNodeMemoryRankSkewCacheMisses}
\centering
\begin{subfigure}{\textwidth}
	\caption{Naive L1 vs. ControlledNodeMemory L1}
	\label{fig:L1NaiveControlledNodeMemoryRankSkewCacheMisses}
	\input{L1NaiveVsControlledNodeMemorySkewRankQueryCacheMisses}
	\vspace*{5 mm}
\end{subfigure}
\begin{subfigure}{\textwidth}
	\caption{Naive L2, L3 vs. ControlledNodeMemory L2, L3}
	\label{fig:L2L3NaiveControlledNodeMemoryRankSkewCacheMisses}
 	\input{L2L3NaiveVsControlledNodeMemorySkewRankQueryCacheMisses}
\end{subfigure}
\end{figure}

\begin{figure}
\caption{Select Cache Misses on Wavelet Tree with increasing skew}
\label{fig:NaiveControlledNodeMemorySelectSkewCacheMisses}
\centering
\begin{subfigure}{\textwidth}
	\caption{Naive L1 vs. ControlledNodeMemory L1}
	\label{fig:L1NaiveControlledNodeMemorySelectSkewCacheMisses}
	\input{L1NaiveVsControlledNodeMemorySkewSelectQueryCacheMisses}
	\vspace*{5 mm}
\end{subfigure}
\begin{subfigure}{\textwidth}
	\caption{Naive L2, L3 vs. ControlledNodeMemory L2, L3}
	\label{fig:L2L3NaiveControlledNodeMemorySelectSkewCacheMisses}
 	\input{L2L3NaiveVsControlledNodeMemorySkewSelectQueryCacheMisses}
\end{subfigure}
\end{figure}


In Figure~\ref{fig:NaiveVsControlledNodeMemorySkewRankQueryTLB} and \ref{fig:NaiveVsControlledNodeMemorySkewSelectQueryTLB} we can see that it is no improvement with regards to TLB misses to skew the tree or to control the memory layout like we have. On the other hand, it's not a clear disadvantage either as certain skews produce a similar amount of TLB misses as the unskewed case. Our controlled memory layout seem to produce little difference for select queries, but is a clear disadvantage for rank queries when skewed with a skew value higher than 3.

\begin{figure}
\caption{Rank TLB}
\label{fig:NaiveVsControlledNodeMemorySkewRankQueryTLB}
\input{NaiveVsControlledNodeMemorySkewRankQueryTLB}
\end{figure}

\begin{figure}
\caption{Select TLB}
\label{fig:NaiveVsControlledNodeMemorySkewSelectQueryTLB}
\input{NaiveVsControlledNodeMemorySkewSelectQueryTLB}
\end{figure}


\begin{figure}
\caption{Rank L2 cache miss rate}
\label{fig:NaiveVsControlledNodeMemorySkewRankQuery_L2_DCMrate}
\input{NaiveVsControlledNodeMemorySkewRankQuery_L2_DCMrate}
\end{figure}

\begin{figure}
\caption{Select L2 cache miss rate}
\label{fig:NaiveVsControlledNodeMemorySkewSelectQuery_L2_DCMrate}
\input{NaiveVsControlledNodeMemorySkewSelectQuery_L2_DCMrate}
\end{figure}

\paragraph{Why skewing is not an improvement}~\\
When we skew the tree we do not split the alphabet into equal sizes and therefore add more data to the right side of the tree than to the left. 
This increases the hight of the tree and we need to look at more nodes to find the result than if the tree was balanced and as a result this we do more work because we have to access more nodes and bitmaps.

We had hoped to reduce the amount of branch mispredictions and cache misses enough that they would reduce the running time more than the increase from having a deeper tree structure.
Sadly, this was not the case. We believe the reason it worked so well for Brodal et. al.\cite{gerthSkewedBinarySearchTrees} but not for us, is that the binary search trees they worked on have much less work involved in traversing each node and so most of the time is spent traversing from node to node.
In rank and select queries on a wavelet tree, most of the time is spent calculating the binary ranks of the bitmaps of each node as well as other calculations performed for each node.

Skewing the tree can help reduce the time spent between nodes by reducing the performance hits from branch mispredictions and cache misses that arise because of the unpredictable nature of a balanced tree traversal, by making the path of the traversal much more predictable and pre-loadable.
It can not, however, help reduce the work performed when reaching each node, and when the total time spent on that work is proportional to the height of the tree, as it is with a wavelet tree, it is a great reduction in speed to skew the tree and thereby increase the height of the tree.
The total intra-node work is proportional to the height of the tree because that work is proportional to the bitmap size and the bitmap sizes are inversely proportional to the number of nodes in the same layer, as the size of all the bitmaps in one layer sum up to $n$. A skewed tree does not have each layer filled with nodes and so the rest of the bitmaps on those levels become larger, as well as any extra layer produced by skewing the tree means another number of bitmaps must appear with combined size $n$.


