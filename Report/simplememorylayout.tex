\section{Simple Algorithm with controlled Memory Layout and Skew}
\label{sec:memorylayout}
Mostly the same algorithm as the Simple, Na√Øve approach, but with a controlled memory layout and a skew of the tree.

We want to reduce the number of cache misses in our rank and select queries. To do this we to try and make it so that the next piece of memory the algorithm accesses is already loaded into a cacheline. One way of doing this is to have the data ordered in memory such that it will be accessed consecutively in memory, as this will mean full utilization of each cacheline as well as enabling prefetching of the next piece of memory to be effective.

\subsection{Prefetching}
Prefetching is a feature of the CPU whereby it can fetch other parts of the memory into cachelines even though it was not requested yet, if it believes it will be requested soon, to avoid having the program waiting for this fetching.
In more advanced versions, it can even look at the access into memory of the running program and try to determine a pattern and prefetch memory according to this pattern.
Looking at the Intel Optimization Manual~\cite{intel-optimization-manual} for our architecture~\footnote{Our architecture is Ivy Bridge, but the optimization manual sections for Sandy Bridge holds true for Ivy Bridge as well, as stated in section 2.2.7 in \cite{intel-optimization-manual}.} we find that it has a streaming prefetchers loading into L1, L2, and last level cache. The streaming prefetchers detect accesses to ascending or descending addresses and can prefetch up to 20 lines ahead or behind. It also has a prefetcher that can detect strides in memory access, as well as a "Next Page Prefetcher" when detecting memory accesses near the page boundary~\footnote{See section 2.2.7 of ~\cite{intel-optimization-manual}}.


\subsection{Skewing The Tree}
\label{sec:SkewingTheTree}
A way of achieving the sequential memory access in our case would be to skew the entire tree to one side, so that it is most likely that the queries will go to one side for most of the traversal and then control the memory layout so we can put that side of the tree next in memory.
Skewing the tree is done by changing the way we find which character in the alphabet to split on in each node's construction.
The split character is the last character in the alphabet of the left child node and to be able to skew the calculation we calculate it as
\[ \frac{alphabetSize-1}{skew} + alphabetMin \]
where $alphabetSize$ is the size of the alphabet at this node, $alphabetMin$ is the first character in the alphabet at this node, and $skew$ is the skew parameter which is 2 for a balanced tree and higher for skewed trees. E.g. a $skew$ value of 4 will mean we skew the tree by, in each node, putting 25\% of the alphabet into the left child node and 75\% into the right.
Notice also that the division is integer division, meaning anything past the decimal point is thrown away, or in other words, like calling a \texttt{floor()} function on the result of the division.

\subsection{Controlled Memory Layout}
We still want to support dynamic input and alphabet sizes without recompilation, so the nodes must be dynamically allocated on the heap.
It is impossible to know the size of the bitmap of each node before its input string is found by its parent, because the bitmap length in each node is the size of its input string.
Because of this, we cannot store the bitmaps as part of the nodes and neither can we simply use an array of bitmaps to ensure they are in consecutive order in memory.

The size of a node (not including its bitmap) is known at compile time as it contains simply pointers to parent node and left and right child nodes, as well as a boolean to flag it as a leaf node.
As such, we can and do allocate the memory for the nodes by allocating an array, then instantiating nodes into that array.
We pass a reference to a pointer into the array from parent to child nodes during construction, so they know where to allocate their child nodes.
The pointer points to the position of the last node in the array, and so before each instantiation of a new node, we increment the pointer so it points to free space, then place the new node there.

\subsection{Preallocating the Bitmaps}
Because the size of each bitmap is unknown at compile time, we cannot use an array, and so we must do it in another way. We still want the bitmaps stored consecutively in memory and because the bitmaps take up the most space, we would like to ensure that the bits are tightly packed.

We allocate the bitmaps as one giant bitmap the size of the maximum possible size required to store all the bitmaps for all the nodes. The sum of the size of all bitmaps on one layer of the tree can at most be $n$ and we can at most have $h$ layers, so the maximum size becomes
\[n \cdot h\]
where $n$ is the number of characters in the string and $h$ is the max height of a skewed binary tree and is defined in~\cite{Nievergelt:1972:BST:800152.804906} to be:
\[ h = \frac{log(2\sigma+1)}{ log(\frac{1}{1-\frac{1}{skew}})}\]
where $skew$ is the number we divide by to skew our tree.  We then store an offset and a size for the bitmap in each node, so we can index into the giant bitmap and access the bits corresponding to the node.
After having constructed the entire tree, we then shrink the giant bitmap to fit its actual size, to not waste the memory when the tree is in use for querying. Shrinking the bitmap takes less than a microsecond so it does not impact the construction time in any significant way. We use the \texttt{resize()} method to shrink the bitmap to the size of \texttt{bitmapOffset}, the counter that has been incremented to always point at the next free space in the bitmap during the construction and so is also the actual filled size of the bitmap.
If we had allocated a bitmap for each node individually, they would have been word-aligned, and the bits between the end of one bitmap and the start of another would have gone unused and so, wasted.

The nodes now contain, in addition to the previously mentioned pointers, a pointer to the large bitmap, an offset and a size.



\subsection{Experiments}

\subsubsection{Query time when Skewing uncontrolled vs. controlled memory-layout}

\begin{figure}
\caption{Rank and Select Running times on Wavelet Tree with increasing skew for naive construction and preallocated construction}
\label{fig:NaiveRankSkewRunningTime}
\input{naiveRankSkewRunningTime}
\end{figure}

\begin{figure}
\caption{Rank Cache Misses on Wavelet Tree with increasing skew for naive construction}
\label{fig:NaiveRankSkewCacheMisses}
\input{NaiveVsPreallocatedSkewQueryCacheMisses}
\end{figure}



In this experiment we want to test whether querying our controlled memory version of the Wavelet Tree would be faster than querying the uncontrolled memory version for increasingly skewed trees. 

We have run 100 \texttt{rank} queries and 100 \texttt{select} queries for each skew and registered the total running time for those 100 queries. 
We did 5 sample runs for each skew and then used the median running time of these. 
This was done to minimize the effect of outliers in the test. The 100 runs was simply done to increase the general size of the running time to better see the differences. The results can be seen in Figure ~\ref{fig:NaiveRankSkewRunningTime}.

We skew the Wavelet Tree (see section~\ref{sec:SkewingTheTree}) because we hoped it would have better performance than not skewing it since data could then be stored sequentially in memory and reduce cache-misses and would then be faster to access when querying. 
This was not the case. 
The query time increases linearly with increasing skew for both controlled and uncontrolled memory layout.

We also wanted to see if controlling the memory ourselves was better than not controlling it. 
Looking at the results in Figure ~\ref{fig:NaiveRankSkewRunningTime} again we can see that the two approaches are quite equal for both rank and select. 

Controlling the memory is in essence not better than letting the OS control it and skewing the tree is not an improvement. 
The hight of the Wavelet Tree increases so much that query overhead is too high to gain anything from the possible faster data access.

\subsubsection{Query vs. Build Time when skewing controlled memory-layout}
In this experiment we aim to show that while skewing the tree for a controlled memory-layout algorithm greatly increases the speed of the queries, it also increases the building time of the tree. But we expect to find the increase in building time to be somewhat slight, and almost insignificant when compared to the increase in query speed, making it well worth it for most use cases.




