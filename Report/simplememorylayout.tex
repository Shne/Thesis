\section{Simple Algorithm with controlled Memory Layout and Skew}
\label{sec:memorylayout}
In this section, we describe our attempt to improve the query times for the wavelet tree by controlling the memory layout and skewing the tree.
Brodal et al.~\cite{gerthSkewedBinarySearchTrees} showed that skewing a binary search tree could reduce the amount of cache misses and branch mispredictions considerately. Enough, in fact, to increase the speed of searching the tree manyfold, even though the skewing increased the depth of the tree structure.

We want to reduce the number of cache misses in our rank and select queries.
To do this we try and make it so that the next piece of memory the algorithm accesses is already loaded into a cacheline.
One way of doing this is to have the data ordered in memory such that the queries will access them consecutively in memory, as this will mean full utilization of each cacheline as well as enabling prefetching of the next piece of memory to be effective.

We will do this by placing nodes consecutively in memory that are consecutively accessed in a Depth First Search that traverses the right subnode first.

The algorithms remain mostly the same algorithm as the Simple, Naïve approach, but with modifications to handle a controlled memory layout and a skew of the tree.
We have also altered the Naïve approach to support skewing, so we can compare whether the controlled memory layout has the desired effect.

\subsection{Prefetching}
Prefetching is a feature of the CPU whereby it can fetch other parts of the memory into cachelines even though it was not requested yet, if it believes it will be requested soon, to avoid having the program waiting for this fetching.
In more advanced versions, it can even look at the access into memory of the running program and try to determine a pattern and prefetch memory according to this pattern.
Looking at the Intel Optimization Manual~\cite{intel-optimization-manual} for our architecture~\footnote{Our architecture is Ivy Bridge, but the optimization manual sections for Sandy Bridge holds true for Ivy Bridge as well, as stated in section 2.2.7 in \cite{intel-optimization-manual}.} we find that it has a streaming prefetchers loading into L1, L2, and last level cache. The streaming prefetchers detect accesses to ascending or descending addresses and can prefetch up to 20 lines ahead or behind. It also has a prefetcher that can detect strides in memory access, as well as a "Next Page Prefetcher" when detecting memory accesses near the page boundary~\footnote{See section 2.2.7 of ~\cite{intel-optimization-manual}}.


\subsection{Skewing The Tree}
\label{sec:SkewingTheTree}
A way of achieving the sequential memory access in our case would be to skew the entire tree to one side, so that it is most likely that the queries will go to one side for most of the traversal and then control the memory layout so we can put that side of the tree next in memory.
Skewing the tree is done by changing the way we find which character in the alphabet to split on in each node's construction.
The split character is the last character in the alphabet of the left child node and to be able to skew the calculation we calculate it as
\[SplitCharacter = \lfloor \frac{alphabetSize-1}{skew} + alphabetMin \rfloor \]
where $alphabetSize$ is the size of the alphabet at this node, $alphabetMin$ is the first character in the alphabet at this node, and $skew$ is the skew parameter which is 2 for a balanced tree and higher for skewed trees. E.g. a $skew$ value of 4 will mean we skew the tree by, in each node, putting 25\% of the alphabet into the left child node and 75\% into the right.
We then round the value down so it is an actual integer character instead of a decimal.


\subsection{Controlled Memory Layout}
We still want to support dynamic input and alphabet sizes without recompilation, so the nodes must be dynamically allocated on the heap.

The size of a node is known at compile time as it contains simply pointers to parent node and left and right child nodes, as well as a boolean to flag it as a leaf node and its bitmap as a vector, which internally stores a pointer to its backing array.
As such, we can and do allocate the memory for the nodes by allocating an array, then instantiating nodes into that array.
We pass a reference to a pointer into the array from parent to child nodes during construction, so they know where to allocate their child nodes.
The pointer points to the position of the last node in the array, and so before each instantiation of a new node, we increment the pointer so it points to free space, then place the new node there.

We don't control the memory layout of the bitmaps, because skewing the tree will not help the prefetcher with regards to the bitmaps, except in a few specific cases, because of the way the bitmaps are used and the resulting access patterns.
The algorithms for rank and select stop querying each bitmap at some position inside the bitmap and then continue to the next bitmap in the next node.
Rank stops when it reaches the position the query was searching up to, given as a parameter, and Select stops when it has found the sought number of occurrences in the bitmap.
In both these cases the rest of the bitmap is not used and any such data the prefetcher has fetched in vain.
The prefetcher cannot tell from the algorithms access pattern when it will jump ahead to the next bitmap, and every such jump will therefore give rise to a cache miss.
The problem is shown in figure ~\ref{fig:QueryPrefetchFigure}. The bitmap is stored sequentially and the prefetcher prefetches the next cache line (colored green), but the algorithm stops at some position and skips ahead to the next bitmap. This makes it not utilize the prefetched data (green) and try to access memory not yet in the cache; a cache miss.
[TODO: perhaps reword the following]
So almost no matter which bitmap is the next, a cache miss will occur.
The exceptions are in the cases where either the entire bitmap is used for the query, that is, when the rank query is for the entire string, or the bitmap is small enough that the beginning of the next bitmap can fit within the same word, which is rare when the input string is much bigger than the alphabet.

\begin{figure}
\caption{Why concatenating bitmaps doesn't enable cache prefetching between bitmaps}
\label{fig:QueryPrefetchFigure}
\includegraphics[width=\textwidth]{QueryPrefetchFigure.pdf}
\end{figure}

A cache miss happens almost every time the select or rank query looks at a new node, until we reach a level of the tree where the bitmap has become small enough to not fill up the cache, which might not happen at all since there can be so many occurrences of even a single character that it allows the bitmap to still be too large to fit in the cache even in the lower levels of the tree.


\subsection{Experiments}

\subsubsection{Query time when skewing the Wavelet Tree using uncontrolled and controlled memory-layout}

\begin{figure}
\caption{Rank and Select Running times on Wavelet Tree with increasing skew for naive construction and preallocated construction}
\label{fig:NaiveRankSelectSkewRunningTime}
\input{naiveRankSelectSkewRunningTime}
\end{figure}


In this experiment we want to test whether querying our controlled memory version of the Wavelet Tree is faster than querying the uncontrolled memory version for increasingly skewed trees. 

We have run 1000 \texttt{rank} queries and 1000 \texttt{select} queries for each skew and registered the total running time for each set of 1000 queries. 
We ran did 5 such sets for each skew and then used the median running time of these. 
This was done to minimize the effect of outliers in the test. The 1000 queries was done for characters even spread across the alphabet for both rank and select. The results can be seen in Figure ~\ref{fig:NaiveRankSelectSkewRunningTime}.

We skew the Wavelet Tree (see section~\ref{sec:SkewingTheTree}) because we believe it has better performance than not skewing it since data can then be stored sequentially in memory and reduce cache-misses because of prefetching and should then be faster to access when querying. 
Skewing should also reduce branch mispredictions since branching to the right become more probable than branching to the left. 

If we look at figure ~\ref{fig:NaiveRankSelectSkewRunningTime} we can see that the query time increases linearly with increasing skew for both controlled and uncontrolled memory layout. We also observe that the two memory layout approaches are almost equal for both rank and select queries in terms of running time.

This means we can conclude that skewing the tree is not an improvement on performance. It is also not an improvement to control the memory ourselves.

\subsubsection{Why skewing is not an improvement}
To understand why skewing the tree is no improvement we need to figure out what causes the increase in query time. 
First we look at branch mispredictions to see if they actually decrease when we increase the skew factor.%
\begin{figure}
\caption{Rank Branch Mispredictions rate}
\label{fig:NaiveVsPreallocatedSkewRankQueryBMrate}
\input{NaiveVsPreallocatedSkewRankQuery_BR_CN}
\end{figure}%
\begin{figure}
\caption{Select Branch Mispredictions rate}
\label{fig:NaiveVsPreallocatedSkewSelectQueryBMrate}
\input{NaiveVsPreallocatedSkewSelectQuery_BR_CN}
\end{figure}
 
Figure ~\ref{fig:NaiveVsPreallocatedSkewRankQueryBMrate} and \ref{fig:NaiveVsPreallocatedSkewSelectQueryBMrate} shows the branch misprediction rate for rank and select queries on a wavelet tree with increasing skew factor for both the controlled and uncontrolled memory approach. 
The branch misprediction rate is calculated by dividing the number of branch mispredictions with the total number of conditional branches.

We observe that the branch misprediction rate is decreasing as we expected. 
This means that it is not branch mispredictions that causes the increase in query time. 

We now look at cache misses to see if they can explain the increase in query time. 
The values are shown in Figure ~\ref{fig:NaivePreallocatedRankSkewCacheMisses} for rank and in Figure ~\ref{fig:NaivePreallocatedSelectSkewCacheMisses} for select.
The same tendency as in Figure ~\ref{fig:NaiveRankSelectSkewRunningTime} is observed: 
The cache misses increase linearly with the skew factor which could explain the linear increase in query time when skewing the tree.

When we skew the tree we do not split the alphabet into equal sizes and therefore add more data to the right side of the tree than to the left. 
This increases the hight of the tree and we need to look at more nodes to find the result than if the tree was balanced and as a result this we introduce more cache misses because we have to access more bitmaps.
Increasing the height of the tree also generally increases the query time because there are more levels to search through than in a balanced tree.

This is why the query time increases when we skew the tree: we get more cache misses and we have to search through more levels of the tree. 
This suggests that skewing the tree can be an improvement when most of the searching time is spent traversing the tree, which is not the case for our wavelet tree.

\begin{figure}
\caption{Rank Cache Misses on Wavelet Tree with increasing skew}
\label{fig:NaivePreallocatedRankSkewCacheMisses}
\centering
\begin{subfigure}{\textwidth}
	\caption{Naive L1 vs. Preallocated L1}
	\label{fig:L1NaivePreallocatedRankSkewCacheMisses}
	\input{L1NaiveVsPreallocatedSkewRankQueryCacheMisses}
	\vspace*{5 mm}
\end{subfigure}
\begin{subfigure}{\textwidth}
	\caption{Naive L2, L3 vs. Preallocated L2, L3}
	\label{fig:L2L3NaivePreallocatedRankSkewCacheMisses}
 	\input{L2L3NaiveVsPreallocatedSkewRankQueryCacheMisses}
\end{subfigure}
\end{figure}

\begin{figure}
\caption{Select Cache Misses on Wavelet Tree with increasing skew}
\label{fig:NaivePreallocatedSelectSkewCacheMisses}
\centering
\begin{subfigure}{\textwidth}
	\caption{Naive L1 vs. Preallocated L1}
	\label{fig:L1NaivePreallocatedSelectSkewCacheMisses}
	\input{L1NaiveVsPreallocatedSkewSelectQueryCacheMisses}
	\vspace*{5 mm}
\end{subfigure}
\begin{subfigure}{\textwidth}
	\caption{Naive L2, L3 vs. Preallocated L2, L3}
	\label{fig:L2L3NaivePreallocatedSelectSkewCacheMisses}
 	\input{L2L3NaiveVsPreallocatedSkewSelectQueryCacheMisses}
\end{subfigure}
\end{figure}


\paragraph{TLB}

\begin{figure}
\caption{Rank TLB}
\label{fig:NaiveVsPreallocatedSkewRankQueryTLB}
\input{NaiveVsPreallocatedSkewRankQueryTLB}
\end{figure}

\begin{figure}
\caption{Select TLB}
\label{fig:NaiveVsPreallocatedSkewSelectQueryTLB}
\input{NaiveVsPreallocatedSkewSelectQueryTLB}
\end{figure}



\subsubsection{Why controlled memory is not better than uncontrolled memory}
When we look at the graphs for cache misses and branch mispredictions we can see that the difference between controlled and uncontrolled memory management is very small which fits very well with the fact that the query times are very close.
The cache misses for \textit{Level 1} (when comparing the two construction methods) are almost exactly equal while \textit{Level 2} and \textit{Level 3} cache misses varies a bit as shown in figure ~\ref{fig:NaivePreallocatedRankSkewCacheMisses} and \ref{fig:NaivePreallocatedSelectSkewCacheMisses}
This is because the size of the caches are larger in higher levels than in lower levels and \textit{Level 1} gets filled up much faster than \textit{Level 2} and \textit{Level 3} resulting in more \textit{Level 1} cache misses.

It can also explain why the difference in \textit{Level 1} cache misses is so small. 
The \textit{Level 1} cache simply gets filled up quickly and is then often missed. \textit{Level 2} and \textit{Level 3}are larger, which means that they do not reach their limit as quickly, resulting in fewer and more varied cache miss values.



Controlling the memory is in essence not better than letting the OS control it, which indicates that the effect of how we control memory is equal to the effect of how OS does it.







