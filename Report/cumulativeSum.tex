\section{Precomputed Cumulative Sum of Binary Ranks}
We have found that using precomputed rank values is a great improvement to the running time of both rank and select queries, though with a higher gain for rank queries.
It works so well, because it allows the algorithms to skip most of the bitmaps, only directly accessing them near the position that was queried for in case of rank queries and near the sought-after occurrence in the case of select queries, and relying on the precomputed values for the rest of the bitmap.

It is still however necessary to iterate through the precomputed values.
Most of the time the algorithms are interested in the rank value at some position inside a bitmap and it is the rank from the beginning of the bitmap to the position and rarely just the rank of that particular block.
Therefore it might be possible to save a number of instructions by not iterating through the precomputed values if the precomputed values were already this cumulative sum of rank values through the bitmap.

We implement this based on UnalignedNaive, again testing the performance for various block sizes to fin the optimal block size and then compare that performance to the UnalignedNaive wavelet tree from Section~\ref{sec:queryRunTimePrecomputedBlockSizes}.

\subsection{Advantages of Cumulative Sum}
\label{sec:AdvantagesOfCumulativeSum}
As previously mentioned, the rank and select query algorithms do not actually need the rank values of individual blocks, but rather the cumulative sum rank value from the beginning of the bitmap to some position.
If we instead implement the precomputed values as being the cumulative sum of rank values of each block from the beginning of the bitmap up to and including the block corresponding to the precomputed value, we can save a lot of precomputed value lookups in the rank and select queries.

Calculating the cumulative rank sums during the construction does not require much more computation.
It can e.g. be done by a single sweep through the precomputed values vector after having computed the entire bitmap, adding each precomputed value to the next in the vector.

Rank queries will benefit from the precomputed values being cumulative sums because they can do a single lookup of the precomputed value corresponding to the block covering the queried-for position.
The need to calculate rank by using \texttt{popcount} within a single block remains unchanged.
This means that the required work per level of the tree changes from $O(\frac{n}{b}+b)$ to $O(b)$ because binary rank becomes an $O(b)$ operation, making the total work required for a rank query $O(b \log \sigma)$.

Select queries should also see some benefit.
Previously, the select query would iterate through the precomputed values and sum them up, looking for when it surpasses the sought-after occurrence, and then calculate the position within a single block using \texttt{popcount} and manual counting of bits within a single word.
Using cumulative precomputed rank values, the select query is able to use binary search on the precomputed value vector to find the word wherein the occurrence is.
Using \texttt{popcount} within a block and manual counting within a word still remains unchanged.
This makes the previously required work per level change from $O(\frac{n}{b} + b)$ to $O(\log \frac{n}{b} + b)$, making the total work required for a select query $O((\log \frac{n}{b} + b) \log \sigma)$.

In Section~\ref{sec:OptimalBlockSizeForRankAndSelect} we test what block size achieves the best running time for rank, select and branchless select.
We test rank for low block sizes since $O(b \log \sigma)$ indicates that the lower \textit{b} is, the faster the running time is.
The running time for select can also be written as $O(b \log \sigma + \log \frac{n}{b} \log \sigma)$ and comparing it to the running time of rank we can see that select has an extra term: $\log \frac{n}{b} \log \sigma$.
The effect of this extra term is that our expected optimal block size is higher for select than for rank, because this term decreases as $b$ increases.

\subsection{Disadvantages of Cumulative Sum}
The O-notation memory analysis remains the same as before, at $O(n \log \sigma + \sigma + \frac{n}{b} \log \sigma)$, because it is still one number stored per block.
However, in practical terms, the precomputed values are no longer limited in value size by the block size but rather the bitmap size, as the last value in the precomputed rank value vector could potentially become as large as the bitmap is long.
Storing the cumulative sums will then require more bytes per value and thus use more space in the end.

The bitmap size is limited by the input string length and so, for our choice of input string with length $10^8$ characters, each precomputed value must be able to store a value up to $10^8$.
It takes at least 28 bits to store the value $10^8$, because $2^{27} < 10^8 < 2^{28}$.
Because the value types supported by x86 and C++ must be byte (8-bit) aligned and use a number of bytes that is a power of 2, the smallest type we can use is the 4-byte type \texttt{unsigned int} capable of storing values up to $2^{32}$.
This means the vector, instead of holding 2-byte \texttt{unsigned short int}s, must hold 4-byte \texttt{unsigned int}s, doubling the space required to store the precomputed values.
We expect this increase in memory usage to be tiny, as it is another 2 bytes per block, of which there are $\frac{n}{b}$ per layer of the tree, of which there are $\log n$.
So with our input string of length $n = 10^8$ and a blockSize of $2^{10} = 1024$ bits, we expect an extra memory usage of about 634 kB:
\[\text{Memory usage} = 2 \cdot \frac{10^8}{1024} \cdot \log 10^8 \approx 648,814 \text{ bytes} \approx 634 \text{ kB} \]

We have already seen that the UnalignedNaive wavelet tree for this input size and block size and an alphabet size of $2^{16}$ uses about 721 MB of memory, so another few hundred kilobytes is barely worth mentioning.
We will see in our experiments how much actual memory is used and whether the difference in running time can make up for the increase in storage space required.

\subsection{Optimal Block Size}
\label{sec:OptimalBlockSize}
Like when using non-cumulative precomputed rank values, the block size can affect the performance.
But, unlike using non-cumulative precomputed rank values, when using cumulative precomputed rank values the optimal block size $b$ for rank queries is not affected by the input size $n$.
This is because the rank algorithm no longer has to linearly scan through the precomputed rank values, but can perform a single lookup before using popcount.
This is also reflected in the running times of $O(\frac{n}{b} + b)$ for non-cumulative and $O(b)$ for cumulative, as there is no $n$ term in the cumulative running time.

From the theoretical running time of $O(b)$, we expect the optimal block size to be small.
However, any block size below the size of word \texttt{popcount} operates on, 64 bit for our machine, will likely not be any improvement, as using \texttt{popcount} to calculate the rank within that word takes constant time.
The only exception, we expect, is if a block size of 1 bit was used and the algorithm modified to just use that precomputed value and not use \texttt{popcount}, which corresponds to precomputing the answer to every possible rank query, and using a lot of memory in the process.
We will not be testing this, though we will do experiments for block sizes smaller than 64 bit.

For select queries, there is still a dependence on $n$, as it has to perform a binary search over the precomputed rank values, and that is reflected in the theoretical running time of $O((\log \frac{n}{b} + b) \log \sigma)$.

\subsection{Select Queries with less branching code}
When implementing the select query for the cumulativeSum wavelet tree, we realized it included a lot of \texttt{if/else} branches that could be difficult to predict by the branch prediction unit.
We anticipated that we might improve upon the query by eliminating as much branching code as possible.
That is, reduce the number of \texttt{if/else} statements, \texttt{while}-, and \texttt{for}-loops in the code and instead replace them with “clever” arithmetic operations achieving much of the same.

One large disadvantage of this approach was that it resulted in a binary search that did not terminate early if the correct block was reached, but would instead always jump and do a lookup $\log (\mathit{blocksInNode})$ times for each node.
Many of the later jumps that would be skipped by terminating early lie close in memory and with high probability exist in the same cacheline and thus be fast to lookup.
This fact, combined with a reduction in branch mispredictions could make this “branchless” version faster.

Based on experiments we found that \textproc(Select) was slower when using the “branchless” approach than just using the simple approach (see Section~\ref{sec:cumulativeSumExperimentSelectQueries}).
When we realized this we attempted to combine the two approaches to get the best from both: early termination from the simple approach and less branching code meaning fewer branch mispredictions from the “branchless” approach.
However, whatever we tried, it always seemed to be slower than the simple approach, and so we stopped trying to combine the two and there a no experiments for a combined approach.

It makes sense that the branchless select is slower than the branching version of select with mispredictions.
According to Agner Fog\footnote{Section 3.7 in \url{http://www.agner.org/optimize/microarchitecture.pdf}} the branch misprediction penalty in the Ivy Bridge Architecture is at least 15 clock cycles.
If a branch is very hard to predict by going one way half the time and the other way the other half, it would be mispredicted about 50\,\% of the time, making it cost on average $1+\frac{15}{2}=8.5$ clock cycles if we assume that a correctly predicted branch costs 1 clock cycle.
This means that our alternative method of computing something without using branches must cost less than 8.5 clock cycles to be an improvement.
Looking at our code, the alternative method where we have attempted to reduce branches could easily cost much more than 8.5 clock cycles.
This also fits with our experimental data (see Section~\ref{sec:cumulativeSumExperimentSelectQueries} and Figure~\ref{fig:CumulativeSumSelect}) because both cache misses, branch mispredictions and TLB misses are smaller for “branchless” select than for the branching version of select, yet the wall time is higher.
An increased amount of cycles can explain why this method results in a slowdown while reducing hardware based penalties.

\subsection{Experiments}
With the following experiments we want to test whether the changes described in the previous section achieves any improvements in practice.
We want to know what effect the changes have on the amount of hardware penalties incurred and try to explain why.
We show the trade-off between build time, memory usage and running time of queries.
We test tree construction and rank and select queries for different block sizes of cumulative sums of precomputed rank values to find the one achieving the best running time.
We compare rank and select for UnalignedNaive vs. using cumulative sum and show how their running time and hardware penalties differ.


\subsubsection{Build Time And Memory Usage For Various Block Sizes}
In Figure~\ref{fig:CumulativeSumBuild} we have plotted the wall time and memory usage of building the UnalignedNaive and CumulativeSum Wavelet Trees.
In Figure~\ref{fig:CumulativeSumBuildWalltime} we can see that it takes slightly longer to build the tree when we have to calculate the cumulative sum across the precomputed values we store.
The difference at $2^{10}$ is 0.49 seconds, which is a $3.33\,\%$ increase from UnalignedNaive to CumulativeSum, and for other block sizes similar differences are found.

In Figure~\ref{fig:CumulativeSumBuildMemoryUsage} we can see that, as expected, CumulativeSum takes more memory, but only significantly so when using block sizes less than $2^8$ bits (8 bytes).
At the lowest block size we tested for, $2^3$ bits, we see a massive increase in memory usage.
For CumulativeSum, the memory usage at a block size of $2^3$ bits is about double that at a block size of $2^8$ bits.

If we look closer at the raw data at block size $2^{10}$ for which we calculated an expected extra memory usage of 634\,KB., there is an increase of 350\,KB in memory usage when storing the cumulative sum, which constitutes an increase of about 0.38\,\%, and is even less than what we expected and is negligible compared to the expected increase in running time.



\subsubsection{Optimal Block Size For Rank And Select}
\label{sec:OptimalBlockSizeForRankAndSelect}
We have made tests of the running time of Rank, Select, and SelectBranchless queries on wavelet trees of varying block sizes from $2^2$ to $2^{16}$ bits.
The test results are shown in Figure~\ref{fig:CumulativeSumBlockSize}.

From Figure~\ref{fig:CumulativeSumBlockSizeWallTimeRank} we observe that the best running time of rank queries is achieved using a block size of $2^6=64$ bits.
The blue line indicates the theoretically best block size of 64 bit as explained in Section~\ref{sec:OptimalBlockSize} and now confirmed by this test.

Select achieves the best running time with a block size of $2^{11}=2048$ bits which can be observed in Figure~\ref{fig:CumulativeSumBlockSizeWallTimeSelect} and the branchless version achieves the best running time using a block size of $2^{10}=1024$ bits as seen in Figure~\ref{fig:CumulativeSumBlockSizeWallTimeSelectBranchless}
The found block sizes fits with the theoretical Big-O analysis.
Rank is best with a small block size and select is also better with a relatively small block size that is larger than for rank.

In a realistic use case one would want to build a single tree using one block size and do rank and select on that tree and not have two trees with different block size, one for rank and one for select.
From our experiments, a block size of $10^{10}=1024$ bits seem to be the best choice when using an input string of $10^8$ characters.
It has close to optimal query running time for both rank and select, and only uses about 0.38\,\% more memory.

\newgeometry{left=2cm,right=2cm, top=2cm, bottom=3cm}
\begin{figure}\tiny
\begin{subfigure}{0.48\textwidth}
	\input{CumulativeSumBlockSizeWallTimeBuild}
	\caption{Wall Time}
	\label{fig:CumulativeSumBuildWalltime}
\end{subfigure}
\hfill
\begin{subfigure}{0.48\textwidth}
	\input{CumulativeSumVsUnalignedNaiveBlockSizeBuildMemory}
	\caption{Memory Usage. Note that the y-axis does not start at 0.}
	\label{fig:CumulativeSumBuildMemoryUsage}
\end{subfigure}
\caption{Measurements on Building the UnalignedNaive and CumulativeSum wavelet trees. The x-axis (block size) is logarithmic.}
\label{fig:CumulativeSumBuild}
\vspace{5mm}
\begin{subfigure}{0.48\textwidth}
	\input{CumulativeSumBlockSizeWallTimeRank}
	\caption{Rank. Blue line marks expected best block size.}
	\label{fig:CumulativeSumBlockSizeWallTimeRank}
\end{subfigure}
\hfill
\begin{subfigure}{0.48\textwidth}
	\input{CumulativeSumBlockSizeWallTimeSelect}
	\caption{Select.}
	\label{fig:CumulativeSumBlockSizeWallTimeSelect}
\end{subfigure}

\begin{subfigure}{0.48\textwidth}
	\input{CumulativeSumBlockSizeWallTimeSelectBranchless}
	\caption{Branchless Select.}
	\label{fig:CumulativeSumBlockSizeWallTimeSelectBranchless}
\end{subfigure}

\caption{Running times of CumulativeSum rank and select for varying block sizes with $n=10^8$ characters. The x-axis (block size) is logarithmic.}
\label{fig:CumulativeSumBlockSize}
\end{figure}

\restoregeometry

\subsubsection{Rank Queries}
In Figure~\ref{fig:CumulativeSumRank} we have plotted various measurements for Rank queries on the UnalignedNaive and CumulativeSum trees.
In Figure~\ref{fig:CumulativeSumRankWalltime} we can see that storing and using the cumulative sum of rank values instead of the rank values for each block improves the running time of rank queries.
UnalignedNaive spends 4.05 milliseconds on 1000 queries, where CumulativeSum spends 1.43 milliseconds, a reduction in running time of 64.7\,\%.

Looking at Figure~\ref{fig:CumulativeSumRankBranchMiss}, Figure~\ref{fig:CumulativeSumRankBranchMissRate} we can see that the tree using cumulative sums has much fewer branch mispredictions but a higher misprediction rate, which can be explained by the fact that fewer conditional branches are executed overall during rank queries as seen in Figure~\ref{fig:CumulativeSumRankBranchExe}.
The decreased amount of branch mispredictions can be explained by the removal of a for-loop in CumulativeSum that iterated over the precomputed values, summing them up to calculate the rank, instead replacing it with a single lookup of a precomputed value.

In Figure~\ref{fig:CumulativeSumRankTLBMiss} we see that the CumulativeSum tree has slightly more Translation Lookaside Buffer Misses than UnalignedNaive but not lot, so the amount of TLB misses are not reduced when using a cumulative sum.

In Figure~\ref{fig:CumulativeSumRankL1CM}, Figure~\ref{fig:CumulativeSumRankL2CM}, and Figure~\ref{fig:CumulativeSumRankL3CM} we can see that rank queries on the CumulativeSum wavelet tree has a much better level 1 cache, level 2 and 3 cache performance because of the decreased amount of cache misses.
Cache misses are reduced and this helps to improve the CumulativeSum rank running time because fewer cache lookups are needed.

In Figure~\ref{fig:CumulativeSumRankL2CHits} we can see that the amount of level 2 cache hits decrease significantly when using cumulative sums of the precomputed values.
The explanation for the decrease in level 2 cache hits might lie in the reduction of level 1 cache misses as seen in Figure~\ref{fig:CumulativeSumRankL1CM}, like results from previous experiments.
The reduction in level 2 cache hits is mainly the amount of cache lookups that the level 1 cache instead was able to handle.
The reduction of level 1 cache misses is on average $\num{319996}$ and the reduction in level 2 cache hits is on average $\num{213818}$ which seems to support this.
The level 2 cache miss rate (not shown) is therefore somewhat misleading as it would suggest a worse cache performance where the truth is that CumulativeSum has a much better cache performance, having much fewer level 1 cache misses, which helps to explain why the rank queries are faster.



\subsubsection{Select Queries}
\label{sec:cumulativeSumExperimentSelectQueries}
In Figure~\ref{fig:CumulativeSumSelect} we have plotted the same measurements as in Figure~\ref{fig:CumulativeSumRank}, but for Select queries, including our “branchless” variant of the cumulativeSum select query.

In Figure~\ref{fig:CumulativeSumSelectWalltime} we can see that storing and using the cumulative sum of precomputed rank values is also an improvement for select queries, with a reduction in wall time of 40\,\%.
Our “branchless” approach is also faster than not using the cumulative sum, but much slower than the simpler approach only achieving a wall time reduction of 18\,\%.

Looking at Figure~\ref{fig:CumulativeSumSelectBranchExe} we can see that both approaches using the cumulative sum executes much fewer conditional branches, which could be caused by using the binary search instead of having to iterate through every precomputed value from the beginning of the bitmap to the position where the sought-after occurrence lies.
The “branchless” select also executes fever branches than the branching version of select.
The difference is not large though, which could mean that most of the branches are from traversing the tree rather than computing the binary select on the bitmap.


In Figure~\ref{fig:CumulativeSumSelectBranchMiss} and Figure~\ref{fig:CumulativeSumSelectBranchMissRate} we can see that the branching approach using cumulative sum has, as expected, more branch mispredictions and a higher branch misprediction rate than both of the others.
The additional number of branch mispredictions contribute about $\num{936255}$ extra clock cycles compared to UnalignedNaive which, assuming 15 clock cycles per misprediction, is only 4.5\,\% of the total number of clock cycles, which is $\num{20954197}$, used in the cumulative sum branching select query.

In Figure~\ref{fig:CumulativeSumRankTLBMiss} we can see that the branching approach also has more TLB misses than the others, yet this still has not made it slower than the others.

In Figure~\ref{fig:CumulativeSumSelectL1CM}, Figure~\ref{fig:CumulativeSumSelectL2CM}, and Figure~\ref{fig:CumulativeSumSelectL3CM} we can see that using a tree with cumulative sum again has better level 1, level 2 and 3 cache performances than UnalignedNaive.
The “branchless” approach has the best level 1 cache, level 2 and 3 cache performances as was also the case for rank.
We see a reduction in level 2 cache hits as for rank in Figure~\ref{fig:CumulativeSumSelectL2CHits} from UnalignedNaive to the two CumulativeSum approaches.
This rediction can again be explained mainly by the reduction in level 1 cache misses as seen in Figure~\ref{fig:CumulativeSumSelectL1CM}.
Level 2 cache misses are a little higher for branching CumulativeSum than for UnalignedNaive.
The “branchless” select reduces the hardware penalties more that branching select but it is still slower because requires more cycles to achieve “branchless” select and the hardware penalty reduction does not make up for this increase. 

In the end we can confirm based on our measurements that it can be explained why Rank and Select is faster for CumulativeSum than for UnalignedNaive.
We feel the performance gain for queries by far make up for the small increase in time and memory usage when building the tree.

CumulativeSum is already theoretically better than UnalignedNaive (see Section~\ref{sec:AdvantagesOfCumulativeSum}) because of the cumulative sums allowing binary rank to be computed in $O(b)$ time.
The tests show the practical effect of this theoretical improvement and confirms that the improvement also works in practice.



\newgeometry{left=2cm,right=2cm, top=2cm, bottom=3cm}
\begin{figure}\tiny

\begin{subfigure}{0.30\textwidth}
	\input{CumulativeSumRankWalltime}
	\caption{Wall Time}
	\label{fig:CumulativeSumRankWalltime}
\end{subfigure}
\hfill
\begin{subfigure}{0.30\textwidth}
	\input{CumulativeSumRankBranchMiss}
	\caption{Branch Mispredictions}
	\label{fig:CumulativeSumRankBranchMiss}
\end{subfigure}
\hfill
\begin{subfigure}{0.30\textwidth}
	\input{CumulativeSumRankBranchExe}
	\caption{Branches Executed}
	\label{fig:CumulativeSumRankBranchExe}
\end{subfigure}


\begin{subfigure}{0.30\textwidth}
	\input{CumulativeSumRankBranchMissRate}
	\caption{Branch Misprediction Rate}
	\label{fig:CumulativeSumRankBranchMissRate}
\end{subfigure}
\hfill
\begin{subfigure}{0.30\textwidth}
	\input{CumulativeSumRankTLBMiss}
	\caption{TLB Misses}
	\label{fig:CumulativeSumRankTLBMiss}
\end{subfigure}
\hfill
\begin{subfigure}{0.30\textwidth}
	\input{CumulativeSumRankL1CM}
	\caption{Level 1 Cache Misses}
	\label{fig:CumulativeSumRankL1CM}
\end{subfigure}

\begin{subfigure}{0.30\textwidth}
	\input{CumulativeSumRankL2CM}
	\caption{Level 2 Cache Misses}
	\label{fig:CumulativeSumRankL2CM}
\end{subfigure}
\hfill
\begin{subfigure}{0.30\textwidth}
	\input{CumulativeSumRankL2CHits}
\caption{Level 2 Cache Hits}
\label{fig:CumulativeSumRankL2CHits}
\end{subfigure}
\hfill
%\begin{subfigure}{0.30\textwidth}
%	\input{CumulativeSumRankL2CMRate}
%	\caption{Level 2 Cache Miss Rate}
%	\label{fig:CumulativeSumRankL2CMRate}
%\end{subfigure}
%\hfill
\begin{subfigure}{0.30\textwidth}
	\input{CumulativeSumRankL3CM}
	\caption{Level 3 Cache Misses}
	\label{fig:CumulativeSumRankL3CM}
\end{subfigure}

\caption{Measurements on Rank Queries on the UnalignedNaive and CumulativeSum Wavelet Trees. Part 1.}
\label{fig:CumulativeSumRank}
\end{figure}





\clearpage




\begin{figure}\tiny

\begin{subfigure}{0.30\textwidth}
	\input{CumulativeSumSelectWalltime}
	\caption{Wall Time}
	\label{fig:CumulativeSumSelectWalltime}
\end{subfigure}
\hfill
\begin{subfigure}{0.30\textwidth}
	\input{CumulativeSumSelectBranchMiss}
	\caption{Branch Mispredictions}
	\label{fig:CumulativeSumSelectBranchMiss}
\end{subfigure}
\hfill
\begin{subfigure}{0.30\textwidth}
	\input{CumulativeSumSelectBranchExe}
	\caption{Branches Executed}
	\label{fig:CumulativeSumSelectBranchExe}
\end{subfigure}


\begin{subfigure}{0.30\textwidth}
	\input{CumulativeSumSelectBranchMissRate}
	\caption{Branch Misprediction Rate}
	\label{fig:CumulativeSumSelectBranchMissRate}
\end{subfigure}
\hfill
\begin{subfigure}{0.30\textwidth}
	\input{CumulativeSumSelectTLBMiss}
	\caption{TLB Misses}
	\label{fig:CumulativeSumSelectTLBMiss}
\end{subfigure}
\hfill
\begin{subfigure}{0.30\textwidth}
	\input{CumulativeSumSelectL1CM}
	\caption{Level 1 Cache Misses}
	\label{fig:CumulativeSumSelectL1CM}
\end{subfigure}


\begin{subfigure}{0.30\textwidth}
	\input{CumulativeSumSelectL2CM}
	\caption{Level 2 Cache Misses}
	\label{fig:CumulativeSumSelectL2CM}
\end{subfigure}
\hfill
\begin{subfigure}{0.30\textwidth}
	\input{CumulativeSumSelectL2CHits}
	\caption{Level 2 Cache Hits}
	\label{fig:CumulativeSumSelectL2CHits}
\end{subfigure}
\hfill
%\begin{subfigure}{0.30\textwidth}
%	\input{CumulativeSumSelectL2CMRate}
%	\caption{Level 2 Cache Miss Rate}
%	\label{fig:CumulativeSumSelectL2CMRate}
%\end{subfigure}
%\hfill
\begin{subfigure}{0.30\textwidth}
	\input{CumulativeSumSelectL3CM}
	\caption{Level 3 Cache Misses}
	\label{fig:CumulativeSumSelectL3CM}
\end{subfigure}

\caption{Measurements on Select Queries on the UnalignedNaive and CumulativeSum and CumulativeSumBranchless Wavelet Trees. Part 1.}
\label{fig:CumulativeSumSelect}
\end{figure}





\restoregeometry













