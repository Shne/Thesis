\section{Precomputed Cumulative Sum of Binary Ranks}
We have found that using precomputed rank values is a great improvement to the running time of both rank and select queries, though with a higher gain for rank queries.
It works so well, because it allows the algorithms to skip most of the bitmaps, only directly accessing them near the position that was queried for in case of rank queries and near the sought-after occurrence in the case of select queries, and relying on the precomputed values for the rest of the bitmap.

It is still however necessary to iterate through the precomputed values.
Most of the time the algorithms are interested in the rank value at some position inside a bitmap and it is the rank from the beginning of the bitmap to the position and rarely just the rank of that particular block.
Therefore we might be able to save a number of instructions by not iterating through the precomputed values if the precomputed values were already this cumulative sum of rank values through the bitmap.

We will implement this based on UnalignedNaive and test the performance compared to the UnalignedNaive wavelet tree from Section~\ref{sec:queryRunTimePrecomputedBlockSizes}, using half a page size as the block size, as we found that to be the fastest configuration

\subsection{Advantages of Cumulative Sum}
As previously mentioned, the rank and select query algorithms do not actually need the rank values of individual blocks, but rather the cumulative sum rank value from the beginning of the bitmap to some position.
If we instead implemented the precomputed values as being the cumulative sum of rank values of each block from the beginning of the bitmap up to and including the block corresponding to the precomputed value, we could save a lot of precomputed value lookups in the rank and select queries.

Calculating the cumulative rank sums during the construction will not require much more computation.
It could e.g. be done by a single sweep through the precomputed values vector after having computed the entire bitmap, adding each precomputed value to the next in the vector.

Rank queries will benefit from the precomputed values being cumulative sums because they can do a single lookup of the precomputed value corresponding to the block covering the queried-for position.
The need to calculate rank by \texttt{popcount}ing within a single block and manually counting the bits within a single word remains unchanged.

Select queries should also see some benefit.
Previously, the select query would iterate through the precomputed values and sum them up, looking for when it surpasses the sought-after occurrence, and then calculate the position within a single block using \texttt{popcount} and manual counting of bits within a single word.
Using cumulative precomputed rank values, the select query is be able to use binary search on the precomputed value vector to find the word wherein the occurrence is.
Using \texttt{popcount} within a block and manual counting within a word still remains unchanged.

\subsection{Disadvantages of Cumulative Sum}
The precomputed values are no longer limited in value size by the block size but rather the bitmap size, as the last value in the precomputed rank value vector could potentially become as large as the bitmap is long.
Storing the cumulative sums will then require more bytes per value and thus use more space in the end.
The bitmap size is limited by the input string length and so for our choice of input string with length $10^8$ each precomputed value must be able to store a value up to $10^8$.
It takes at least 28 bits to store $10^8$, because $2^{27} < 10^8 < 2^{28}$.
Because the value types supported by x86 and C++ must be byte (8-bit) aligned and use a number of bytes that is a power of 2, the smallest type we can use is the 4-byte type \texttt{unsigned int} capable of storing values up to $2^{32}$.
This means the vector, instead of holding 2-byte \texttt{unsigned short int}s, must hold 4-byte \texttt{unsigned int}s, doubling the space required to store the precomputed values.
We expect this increase in memory usage to be tiny, as it is another 2 bytes per block, of which there are $\frac{n}{blockSize}$ per layer of the tree, of which there are $\log(n)$.
So with our input string of length $n = 10^8$ and a blockSize of half a page, or 16384 bits, we expect an extra memory usage of about 317 kB:
\[\text{Memory usage} = 2 \times \frac{10^8}{16384} \times \log(10^8) \approx 324407 \text{ bytes} \approx 317 \text{ kB} \]

We have already seen that the UnalignedNaive wavelet tree for this input size and alphabet size of $2^{16}$ uses about 721 MB of memory, so another few hundred kilobytes is barely worth mentioning.
We will see in our experiments how much actual memory is used and whether the difference in running time can make up for the increase in storage space required.

\subsection{Select Queries with less branching code}
When implementing the select query for the cumulativeSum wavelet tree, we realized it included a lot of \texttt{if/else} branches that could be difficult to predict by the branch prediction unit.
We anticipated that we might improve upon the query by eliminating as much branching code as possible.
That is, reduce the number of \texttt{if/else} statements, \texttt{while}-, and \texttt{for}-loops in the code and instead replace them with "clever" arithmetic operations achieving much of the same.

One large disadvantage of this approach was that it resulted in a binary search that did not terminate early if the correct block was reached, but would instead always jump and do a lookup $\log(\#blocksInNode)$ times for each node.
We believed that the fewer branch mispredictions and the fact that many of the later jumps that would be skipped by terminating early should with high probability exist in the same cacheline and thus fast be to lookup.

Based on experiments we found that \textproc(Select) was slower when using the "branchless" approach than just using the simple approach (see Section~\ref{sec:cumulativeSumExperimentSelectQueries}).
When we realized this we attempted to combine the two approaches to get the best from both: early termination from the simple approach and less branching code meaning fewer branch mispredictions from the "branchless" approach.
However, whatever we tried, it always seemed to be slower than the simple approach, and so we stopped trying to combine the two.

It makes sense that our alternatives to branches is slower than branching with mispredictions.
According to Agner Fogh~\citep{agner} the branch misprediction penalty in the Ivy Bridge Architecture is at least 15 clock cycles.
If a branch is very hard to predict by going one way half the time and the other way the other half, it would be mispredicted about 50\% of the time, making it cost on average $1+\frac{15}{2}=8$ clock cycles.
This means that our alternative method of computing something without using branches must cost less than 8 clock cycles to be an improvement.
Looking at our code, the alternative methods where we have attempted to reduce branches could easily cost much more than 8 clock cycles, which explains why these methods resulted in a slowdown.
The amount of branch mispredictions was reduced. Just not enough to make up for the increased amount of work required.

\subsection{Experiments}

\subsubsection{Build Time and Memory Usage}
In Figure~\ref{fig:CumulativeSumBuild} we have plotted the wall time and memory usage of building the UnalignedNaive and CumulativeSum Wavelet Trees.
In Figure~\ref{fig:CumulativeSumBuildWalltime} we can see that it takes slightly longer to build the tree when we have to calculate the cumulative sum across the precomputed values we store.
It is only a difference of 151 milliseconds or an increase from UnalignedNaive to CumulativeSum of 1.00\%.
In Figure~\ref{fig:CumulativeSumBuildMemoryUsage} we can see that there is, as expected, no significant difference in memory usage between the two trees.
If we look closer at the raw data, there is a slight (124 KB) increase in memory usage when storing the cumulative sum, which constitutes an increase of about 0.0168\%, and is even less than what we expected.
When taking the performance gain on queries into account then the small increased time and memory usage when building the tree is negligible.

\subsubsection{Rank Queries}
In Figure~\ref{fig:CumulativeSumRank} we have plotted various measurements for Rank queries on the UnalignedNaive and CumulativeSum trees.
In Figure~\ref{fig:CumulativeSumRankWalltime} we can see that storing and using the cumulative sum of rank values instead of the rank values for each block improves the running time of rank queries.
UnalignedNaive spends 3.57 milliseconds on 1000 queries, where CumulativeSum spends 3.04 milliseconds, an improvement of 14.77\%.

Looking at Figure~\ref{fig:CumulativeSumRankBranchMiss}, Figure~\ref{fig:CumulativeSumRankBranchMissRate}, and Figure~\ref{fig:CumulativeSumRankBranchExe} we can see that the tree using cumulative sums has much fewer branch mispredictions and a lower misprediction rate, as well as executing fewer conditional branches overall during rank queries.
This can be explained by the removal of a for-loop in CumulativeSum that iterated over the precomputed values, summing them up to calculate the rank, instead replacing it with a single lookup of a precomputed value.

In Figure~\ref{fig:CumulativeSumRankTLBMiss} we see that the CumulativeSum tree has slightly fewer Translation Lookaside Buffer Misses, although we can see from the errorbars that they their standard deviations overlap, so perhaps there is less of an improvement here than it might look like at first glance, if any at all.

In Figure~\ref{fig:CumulativeSumRankL1CM}, Figure~\ref{fig:CumulativeSumRankL2CM}, and Figure~\ref{fig:CumulativeSumRankL3CM} we can see that rank queries on the CumulativeSum wavelet tree has a much better level 1 cache performance, and only slightly worse level 2 and 3 cache performance.

In Figure~\ref{fig:CumulativeSumRankL2CHits} we can see that the amount of level 2 cache hits decrease significantly when using cumulative sums of the precomputed values.
We believe the explanation for the decrease in level 2 cache hits lies in the reduction of level 1 cache misses (Figure~\ref{fig:CumulativeSumRankL1CM}), like results from previous experiments.
The reduction in level 2 cache hits is simply the amount of cache lookups that the level 1 cache instead was able to handle.
The reduction of level 1 cache misses is on average $\num{135394.4}$ and the reduction in level 2 cache hits is on average $\num{133885.2}$ making them near-identical.
The level 2 cache miss rate (not shown) is therefore somewhat misleading as it would suggest a worse cache performance where the truth is that CumulativeSum has a much better cache performance, having much fewer level 1 cache misses, which helps to explain why the rank queries are faster.



\subsubsection{Select Queries}
\label{sec:cumulativeSumExperimentSelectQueries}
In Figure~\ref{fig:CumulativeSumSelect} we have plotted the same measurements as in Figure~\ref{fig:CumulativeSumRank}, but for Select queries, including our "branchless" variant of the cumulativeSum select query.

In Figure~\ref{fig:CumulativeSumSelectWalltime} we can see that storing and using the cumulative sum of precomputed rank values is also an improvement for select queries, with a reduction in wall time of 22.07\%.
Our "branchless" approach is also faster than not using the cumulative sum, but much slower than the simpler approach.

Looking at Figure!\ref{fig:CumulativeSumSelectBranchExe} we can see that both approaches using the cumulative sum executes much fewer conditional branches, which we believe is caused by using the binary search instead of having to iterate through every precomputed value from the beginning of the bitmap to the position where the sought-after occurrence lies.
Ironically, the "branchless" approach executes more conditional branches than the simple approach, which we believe is because it can not terminate the binary search early, leading to more branching code executed in the extra jumps in the binary search.

In Figure~\ref{fig:CumulativeSumSelectBranchMiss} and Figure!\ref{fig:CumulativeSumSelectBranchMissRate} we can see that the simple approach using cumulative sum has, as expected, more branch mispredictions and a higher branch misprediction rate than both the others.
The additional number of branch mispredictions contribute about $\num{508704}$ extra clock cycles, assuming 15 clock cycles per misprediction, which is only 1.87\% of the total number of clock cycles used in the simple approach using cumulative sum.

In Figure~\ref{fig:CumulativeSumRankTLBMiss} we can see that the simple approach also has more TLB misses than the others, yet this still has not made it slower than the others.

In Figure~\ref{fig:CumulativeSumSelectL1CM}, Figure~\ref{fig:CumulativeSumSelectL2CM}, and Figure~\ref{fig:CumulativeSumSelectL3CM} we can see that the fastest algorithm, the simple approach using cumulative sum, again has a better level 1 cache performance and worse level 2 and 3 cache performances.
The "branchless" approach has the worst level 1 cache performance and level 2 and 3 cache performances similar to the algorithm not using cumulative sum.
Again we see a reduction in level 2 cache hits in Figure~\ref{fig:CumulativeSumSelectL2CHits} from not using cumulative sum to using it, that can be, at least partially, explained by the reduction in level 1 cache misses as seen in Figure~\ref{fig:CumulativeSumSelectL1CM}.
The other part is then the increase in level 2 cache misses.

In the end we can confirm based on the our measurements that it makes sense why Rank and Select is faster for CumulativeSum than for UnalignedNaive.







\begin{figure}\tiny
\begin{subfigure}{0.48\textwidth}
	\input{CumulativeSumBuildWalltime}
	\caption{Wall Time}
	\label{fig:CumulativeSumBuildWalltime}
\end{subfigure}
\hfill
\begin{subfigure}{0.48\textwidth}
	\input{CumulativeSumBuildMemory}
	\caption{Memory Usage}
	\label{fig:CumulativeSumBuildMemoryUsage}
\end{subfigure}
\caption{Measurements on Building the UnalignedNaive and CumulativeSum wavelet trees}
\label{fig:CumulativeSumBuild}
\end{figure}


\newgeometry{left=2cm,right=2cm, top=2cm, bottom=3cm}
\begin{figure}\tiny

\begin{subfigure}{0.30\textwidth}
	\input{CumulativeSumRankWalltime}
	\caption{Wall Time}
	\label{fig:CumulativeSumRankWalltime}
\end{subfigure}
\hfill
\begin{subfigure}{0.30\textwidth}
	\input{CumulativeSumRankBranchMiss}
	\caption{Branch Mispredictions}
	\label{fig:CumulativeSumRankBranchMiss}
\end{subfigure}
\hfill
\begin{subfigure}{0.30\textwidth}
	\input{CumulativeSumRankBranchExe}
	\caption{Branches Executed}
	\label{fig:CumulativeSumRankBranchExe}
\end{subfigure}


\begin{subfigure}{0.30\textwidth}
	\input{CumulativeSumRankBranchMissRate}
	\caption{Branch Misprediction Rate}
	\label{fig:CumulativeSumRankBranchMissRate}
\end{subfigure}
\hfill
\begin{subfigure}{0.30\textwidth}
	\input{CumulativeSumRankTLBMiss}
	\caption{TLB Misses}
	\label{fig:CumulativeSumRankTLBMiss}
\end{subfigure}
\hfill
\begin{subfigure}{0.30\textwidth}
	\input{CumulativeSumRankL1CM}
	\caption{Level 1 Cache Misses}
	\label{fig:CumulativeSumRankL1CM}
\end{subfigure}

\begin{subfigure}{0.30\textwidth}
	\input{CumulativeSumRankL2CM}
	\caption{Level 2 Cache Misses}
	\label{fig:CumulativeSumRankL2CM}
\end{subfigure}
\hfill
\begin{subfigure}{0.30\textwidth}
	\input{CumulativeSumRankL2CHits}
\caption{Level 2 Cache Hits}
\label{fig:CumulativeSumRankL2CHits}
\end{subfigure}
\hfill
%\begin{subfigure}{0.30\textwidth}
%	\input{CumulativeSumRankL2CMRate}
%	\caption{Level 2 Cache Miss Rate}
%	\label{fig:CumulativeSumRankL2CMRate}
%\end{subfigure}
%\hfill
\begin{subfigure}{0.30\textwidth}
	\input{CumulativeSumRankL3CM}
	\caption{Level 3 Cache Misses}
	\label{fig:CumulativeSumRankL3CM}
\end{subfigure}

\caption{Measurements on Rank Queries on the UnalignedNaive and CumulativeSum Wavelet Trees. Part 1.}
\label{fig:CumulativeSumRank}
\end{figure}





\clearpage




\begin{figure}\tiny

\begin{subfigure}{0.30\textwidth}
	\input{CumulativeSumSelectWalltime}
	\caption{Wall Time}
	\label{fig:CumulativeSumSelectWalltime}
\end{subfigure}
\hfill
\begin{subfigure}{0.30\textwidth}
	\input{CumulativeSumSelectBranchMiss}
	\caption{Branch Mispredictions}
	\label{fig:CumulativeSumSelectBranchMiss}
\end{subfigure}
\hfill
\begin{subfigure}{0.30\textwidth}
	\input{CumulativeSumSelectBranchExe}
	\caption{Branches Executed}
	\label{fig:CumulativeSumSelectBranchExe}
\end{subfigure}


\begin{subfigure}{0.30\textwidth}
	\input{CumulativeSumSelectBranchMissRate}
	\caption{Branch Misprediction Rate}
	\label{fig:CumulativeSumSelectBranchMissRate}
\end{subfigure}
\hfill
\begin{subfigure}{0.30\textwidth}
	\input{CumulativeSumSelectTLBMiss}
	\caption{TLB Misses}
	\label{fig:CumulativeSumSelectTLBMiss}
\end{subfigure}
\hfill
\begin{subfigure}{0.30\textwidth}
	\input{CumulativeSumSelectL1CM}
	\caption{Level 1 Cache Misses}
	\label{fig:CumulativeSumSelectL1CM}
\end{subfigure}


\begin{subfigure}{0.30\textwidth}
	\input{CumulativeSumSelectL2CM}
	\caption{Level 2 Cache Misses}
	\label{fig:CumulativeSumSelectL2CM}
\end{subfigure}
\hfill
\begin{subfigure}{0.30\textwidth}
	\input{CumulativeSumSelectL2CHits}
	\caption{Level 2 Cache Hits}
	\label{fig:CumulativeSumSelectL2CHits}
\end{subfigure}
\hfill
%\begin{subfigure}{0.30\textwidth}
%	\input{CumulativeSumSelectL2CMRate}
%	\caption{Level 2 Cache Miss Rate}
%	\label{fig:CumulativeSumSelectL2CMRate}
%\end{subfigure}
%\hfill
\begin{subfigure}{0.30\textwidth}
	\input{CumulativeSumSelectL3CM}
	\caption{Level 3 Cache Misses Rate}
	\label{fig:CumulativeSumSelectL3CM}
\end{subfigure}

\caption{Measurements on Select Queries on the UnalignedNaive and CumulativeSum and CumulativeSumBranchless Wavelet Trees. Part 1.}
\label{fig:CumulativeSumSelect}
\end{figure}





\restoregeometry













