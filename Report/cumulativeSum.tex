\section{Precomputed Cumulative Sum of Binary Ranks}
We have found that using precomputed rank values is a great improvement to the running time of both rank and select queries, though with a higher gain for rank queries.
It works so well, because it allows the algorithms to skip most of the bitmaps, only directly using at them near the position that was queried for in case of rank queries and near the sought-after occurrence in the case of select queries, and relying on the precomputed values for the rest of the bitmap.

It does still, however, need to iterate through the precomputed values.
Most of the time the algorithms are interested in the rank value at some position inside a bitmap, it is the rank from the beginning of the bitmap to the position, rarely just the rank of that particular block.
Therefore we might be able to save a number of instructions by not iterating through the precomputed values if the precomputed values were already this cumulative sum of rank values through the bitmap.

We will implement this based on UnalignedNaive and test the performance compared to the UnalignedNaive wavelet tree from Section~\ref{sec:queryRunTimePrecomputedBlockSizes}, using half a page size as the block size, as we found that to be the fastest.

\subsection{Advantages of Cumulative Sum}
As previously mentioned, the rank and select query algorithms don't actually need the rank values of individual blocks, but rather the cumulative sum rank value from the beginning of the bitmap to some position.
If we instead implemented the precomputed values as being the cumulative sum of rank values of each block from the beginning of the bitmap up to and including the block corresponding to the precomputed value, we could save a lot of precomputed value lookups in the rank and select queries.

Calculating the cumulative rank sums during the construction will not require much more computation.
It could e.g. be done by a single sweep through the precomputed values vector after having computed the entire bitmap, adding each precomputed value to the next in the vector.

Rank queries will benefit from the precomputed values being cumulative sums because they can do a single lookup of the precomputed value corresponding to the block covering the queried-for position.
The need to calculate a rank using \texttt{popcount}ing within a single block and manual counting of bits within a single word remains unchanged.

Select queries will also likely be quicker.
Previously, the select query would iterate through the precomputed values and sum them up, looking for when it surpasses the sought-after occurrence, and then calculate the position within a single block using \texttt{popcount} and manual counting of bits within a single word.
Using cumulative precomputed rank values, the select query will be able to use binary search on the precomputed value vector to find the word wherein the occurrence is.
Using \texttt{popcount} within a block and manual counting within a word still remains unchanged.

\subsection{Disadvantages of Cumulative Sum}
The precomputed values are no longer limited in value size by the block size but rather the bitmap size, as the last value in the precomputed rank value vector could potentially become as large as the bitmap is long.
Storing the cumulative sums will then require more bytes per value and thus use more space in the end.
The bitmap size is limited by the input string length and so for our choice of input string with length $10^8$ each precomputed value must be able to store a value up to $10^8$.
It takes at least 28 bits to store $10^8$, because $2^{27} < 10^8 < 2^{28}$.
Because the value types supported by x86 and C++ must be byte (8-bit) aligned and use a number of bytes that is a power of 2, the smallest type we can use is the 4-byte type \texttt{unsigned int} capable of storing values up to $2^{32}$.
This means the vector, instead of holding 2-byte \texttt{unsigned short int}s, must hold 4-byte \texttt{unsigned int}s, doubling the space required to store the precomputed values.
We expect this increase in memory usage to be very small, as it is another 2 bytes per block, of which there are $\frac{n}{blockSize}$ per layer of the tree, of which there are $\log(n)$.
So with our input string of length $n = 10^8$ and a blockSize of half a page, or 16384 bits, we expect an extra memory usage of about
\[ 2 \times \frac{10^8}{16384} \times \log(10^8) \approx 324407\]
bytes, or 317 KB.
We have already seen that the UnalignedNaive wavelet tree for this input size and alphabet size of $2^{16}$ uses about 721 MB of memory, so another few hundred kilobytes is barely worth mentioning.
We will see in our experiments how much actual memory is used and whether the difference in running time can make up for the increase in storage space required.


\subsection{Experiments}

\subsubsection{Build Time and Memory Usage}
In Figure~\ref{fig:CumulativeSumBuild} we have plotted the wall time and memory usage of building the UnalignedNaive and CumulativeSum Wavelet Trees.
In Figure~\ref{fig:CumulativeSumBuildWalltime} we can see that it takes slightly longer to build the tree when we have to calculate the cumulative sum across the precomputed values we store.
In Figure~\ref{fig:CumulativeSumBuildMemoryUsage} we can see that there is, as expected, no significant difference in memory usage between the two trees.
If we look closer at the raw data, there is only a slight (124 KB) increase in memory usage when storing the cumulative sum, which constitutes an increase of about 0.0168\%, and is even less than what we expected.

\subsubsection{Rank Queries}


\subsubsection{Select Queries}



\begin{figure}\tiny
\begin{subfigure}{0.48\textwidth}
	\input{CumulativeSumBuildWalltime}
	\caption{Wall Time of Building the Wavelet tree}
	\label{fig:CumulativeSumBuildWalltime}
\end{subfigure}
\hfill
\begin{subfigure}{0.48\textwidth}
	\input{CumulativeSumBuildMemory}
	\caption{Memory Usage of Building the Wavelet Tree.}
	\label{fig:CumulativeSumBuildMemoryUsage}
\end{subfigure}
\caption{Measurements on Building the UnalignedNaive and CumulativeSum wavelet trees}
\label{fig:CumulativeSumBuild}
\end{figure}


\newgeometry{left=2cm,right=2cm, top=2cm, bottom=3cm}
\begin{figure}\tiny

\begin{subfigure}{0.30\textwidth}
	\input{CumulativeSumRankWalltime}
	\caption{Wall Time}
	\label{fig:CumulativeSumRankWalltime}
\end{subfigure}
\hfill
\begin{subfigure}{0.30\textwidth}
	\input{CumulativeSumRankBranchMiss}
	\caption{Branch Misses}
	\label{fig:CumulativeSumRankBranchMiss}
\end{subfigure}
\hfill
\begin{subfigure}{0.30\textwidth}
	\input{CumulativeSumRankBranchExe}
	\caption{Branches Executed}
	\label{fig:CumulativeSumRankBranchExe}
\end{subfigure}


\begin{subfigure}{0.30\textwidth}
	\input{CumulativeSumRankBranchMissRate}
	\caption{Branch Misprediction Rate}
	\label{fig:CumulativeSumRankBranchMissRate}
\end{subfigure}
\hfill
\begin{subfigure}{0.30\textwidth}
	\input{CumulativeSumRankTLBMiss}
	\caption{TLB Misses}
	\label{fig:CumulativeSumRankTLBMiss}
\end{subfigure}
\hfill
\begin{subfigure}{0.30\textwidth}
	\input{CumulativeSumRankL1CM}
	\caption{Level 1 Cache Misses}
	\label{fig:CumulativeSumRankL1CM}
\end{subfigure}

\begin{subfigure}{0.30\textwidth}
	\input{CumulativeSumRankL2CM}
	\caption{Level 2 Cache Misses}
	\label{fig:CumulativeSumRankL2CM}
\end{subfigure}
%\hfill
%\begin{subfigure}{0.30\textwidth}
%	\input{CumulativeSumRankL2CHits}
%	\caption{Level 2 Cache Hits}
%	\label{fig:CumulativeSumRankL2CHits}
%\end{subfigure}
\hfill
\begin{subfigure}{0.30\textwidth}
	\input{CumulativeSumRankL2CMRate}
	\caption{Level 2 Cache Miss Rate}
	\label{fig:CumulativeSumRankL2CMRate}
\end{subfigure}
\hfill
\begin{subfigure}{0.30\textwidth}
	\input{CumulativeSumRankL3CM}
	\caption{Level 3 Cache Misses}
	\label{fig:CumulativeSumRankL3CM}
\end{subfigure}

\caption{Measurements on Rank Queries on the UnalignedNaive and CumulativeSum Wavelet Trees. Part 1.}
\label{fig:CumulativeSumRank}
\end{figure}





\clearpage




\begin{figure}\tiny

\begin{subfigure}{0.30\textwidth}
	\input{CumulativeSumSelectWalltime}
	\caption{Wall Time}
	\label{fig:CumulativeSumSelectWalltime}
\end{subfigure}
\hfill
\begin{subfigure}{0.30\textwidth}
	\input{CumulativeSumSelectBranchMiss}
	\caption{Branch Misses}
	\label{fig:CumulativeSumSelectBranchMiss}
\end{subfigure}
\hfill
\begin{subfigure}{0.30\textwidth}
	\input{CumulativeSumSelectBranchExe}
	\caption{Branches Executed}
	\label{fig:CumulativeSumSelectBranchExe}
\end{subfigure}


\begin{subfigure}{0.30\textwidth}
	\input{CumulativeSumSelectBranchMissRate}
	\caption{Branch Misprediction Rate}
	\label{fig:CumulativeSumSelectBranchMissRate}
\end{subfigure}
\hfill
\begin{subfigure}{0.30\textwidth}
	\input{CumulativeSumSelectTLBMiss}
	\caption{TLB Misses}
	\label{fig:CumulativeSumSelectTLBMiss}
\end{subfigure}
\hfill
\begin{subfigure}{0.30\textwidth}
	\input{CumulativeSumSelectL1CM}
	\caption{Level 1 Cache Misses}
	\label{fig:CumulativeSumSelectL1CM}
\end{subfigure}


\begin{subfigure}{0.30\textwidth}
	\input{CumulativeSumSelectL2CM}
	\caption{Level 2 Cache Misses}
	\label{fig:CumulativeSumSelectL2CM}
\end{subfigure}
\hfill
%\begin{subfigure}{0.30\textwidth}
%	\input{CumulativeSumSelectL2CHits}
%	\caption{Level 2 Cache Hits}
%	\label{fig:CumulativeSumSelectL2CHits}
%\end{subfigure}
%\hfill
\begin{subfigure}{0.30\textwidth}
	\input{CumulativeSumSelectL2CMRate}
	\caption{Level 2 Cache Miss Rate}
	\label{fig:CumulativeSumSelectL2CMRate}
\end{subfigure}
\hfill
\begin{subfigure}{0.30\textwidth}
	\input{CumulativeSumSelectL3CM}
	\caption{Level 3 Cache Misses Rate}
	\label{fig:CumulativeSumSelectL3CM}
\end{subfigure}

\caption{Measurements on Select Queries on the UnalignedNaive and CumulativeSum and CumulativeSumBranchless Wavelet Trees. Part 1.}
\label{fig:CumulativeSumSelect}
\end{figure}





\restoregeometry













