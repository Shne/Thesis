\section{Notes on The Experiments}
Here we discuss some things general for all our experiments, or all those where applicable.

\subsection{General Setup}
\label{sec:ExpNotesGeneralSetup}
We run 1000 queries 5 times for each variable parameter and register the total running time for each set of 1000 queries and then use the average of those 5 as the result.
Examples of variable parameters used in our tests are the alphabet size, the skew of the tree or the block size.

We calculate the standard deviation of the 5 runs and include that in the graphs as errorbars.
All our graphs with lines (as opposed to bar-graphs) include the standard deviation as errorbars.
If one appears not to have any errorbars that means the standard deviation is just very small.

\subsection{Choice of Input String}
We have chosen to construct the input strings used in our experiments so that each character occurs with the same probability at each position.
This means the string has a uniform distribution of characters from the alphabet.
We have chosen to do so for several reasons, among them being that we believe it to be a realistic use case \textbf{[TODO: why?]}, as well as making the choice of character to query for in our experiments make less difference.
The even amount of occurrences of each character also means there will be little difference in size of bitmap among the nodes in a single layer of the tree.

\subsubsection{Uniform vs. Non-Uniform data}
If the data is non-uniform it means that some symbols from the alphabet will appear with a higher frequency than others.
If one knows the frequencies of all symbols then one can build a Huffman shaped Wavelet Tree, which places symbols with higher frequencies closer to the root than those that have a smaller frequency, in such a way that the path from root to symbol leaf corresponds to the binary Huffman code of the symbol. 
The Huffman code of a symbol is a small binary value for the symbol with the highest frequency and a high binary value for the symbol with the smallest frequency.
This would make the tree skewed which increases the height of the tree but decreases query time for symbols with high frequency.
If one then queries the Huffman shaped Wavelet Tree for a symbol that has a high frequency then one would find it faster than if one was looking for a symbol with a small frequency.

We have not implemented a Huffman shaped Wavelet Tree but non-uniform data still introduces a bias in the normal balanced Wavelet tree.
This is because the sizes of the bitmaps in each node in a given level of the tree would not be equal, as they would for uniform data.
When we query the tree and take a path with many large bitmaps it will take longer than a path with many small bitmaps
Therefore having nun-uniform data would introduce a bias in our query tests based on the symbol we are querying for. 
If our data on the other hand is uniform that bias would not be present.

There is also the problem of choosing how the data must be non-uniformly distributed as there are several non-uniform distribution methods to choose from.

\subsubsection{Non-uniform distribution choice}
The wavelet tree has applications within full text indexing which suggests that having a distribution based on how words are distributed with in a normal English text could be a good choice, because it would be a realistic use case. 
Zipf's Law describes such a distribution \citep[abstract]{ZipfsLawOnText} but it requires a distribution parameter $s$ that describes the frequency relation between each symbol, e.g. if the most frequent word has double the frequency of the 2nd most frequent word and this relation continues down the list of most frequent words, it is a Zipf's Law distribution with an $s$ parameter of 2.
We have not been able to find anyone describing which parameter value would produce a distribution most closely resembling real world English. 
We have searched through various articles to find an \textit{s} value representative of the English language, but it does not seem like there is a single good value as it depends a lot on the type of text, e.g. scientific journals vs. newspapers vs. books.
This is also a conclusion that \citep[abstract]{ZipfsLawOnText} arrives at.

It is possible to estimate \textit{s} for a given text but doing so then creates the problem of choosing a representative text to estimate \textit{s} from.
We tried using the word frequencies in the NGSL \citep{NGSL} wordlist which contain the 31,241 most used words in the English language and the frequency with which they appear, to estimate \textit{s}.
NGSL is based on data from the Cambridge English Corpus which is a multi-billion word collection of written, spoken and learner texts and is the largest of its kind.
This fact combined with the fact that NGSL is fairly new (2013) makes us believe that the frequencies in the NGSL wordlist are accurate enough for our purpose.

Estimating \textit{s} using NGSL gave us a value very close to 1 which only grows closer to 1 the more words we used from the list.
This is a problem because with an $s$ parameter of 1, a Zipf's Law distribution is uniform.
It also tells us that the English language, or at least the part that has been aggregated in NGSL, might not, in fact, be following the Zipf's Law very closely, as we would expect our calculations to converge towards some constant value $>1$.
Instead we use the data from the NGSL word list and generate our own non-uniform dataset from it based directly on the word frequencies found in the NGSL word list.
This way we end up with a more realistic non-uniform dataset than if we had used the Zipf's Law model since the data is based on real empirical data and we avoid the problem of choosing a good \textit{s} value.


\subsection{Choice of Query Parameters}
\label{sec:choiceOfQueryParameters}
It is important to ensure that we don't introduce a bias in our experiments on the rank and select query performances by our choice of query parameters.
As we have chosen to use a randomly generated input string with uniform distribution of characters for most of our tests, there should be little difference in the frequency of characters and little difference in query performance based on the exact choice of character.
There is, however a difference of where in the tree the node each character corresponds to, and we should make sure to use characters from various positions in the alphabet, to have the queries together traverse as much of the tree as possible.

For the rank queries there is also the position parameter, determining how far into the string the query should look and therefore how far into each bitmap the query should look.
A high value (close to the length of the string) might seem like a good idea to make the query go through most of the bitmaps, but we don't want to introduce a bias by using some constant high value, nor do we want to risk introducing a bias by only looking at high values for the position parameter.
Again we choose to use values from all parts of the range of valid values for the parameter.

We are also interested in avoiding introducing any bias by using only one type of combination of parameters.
If we had e.g. let both parameter values depend on the index of a single for-loop around the call to the query, we would have only tested low character values together with low position values as well as high character values together with high position values.

Instead we let one parameter ascend from valid low values to valid high values with even spacing to reach the highest valid value in the lastly performed query. Meanwhile, the other parameter increases more rapidly with wider spacing, and then wraps around before passing highest valid value to then start again at low values, with an offset to not repeat parameter values, doing so many times before the end.
This ensures we perform the queries for all combinations of high, medium and low parameter values in our experiments.
