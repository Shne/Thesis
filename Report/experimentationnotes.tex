\section{Notes on The Experiments}
Here we discuss some things general for all our experiments, or all those where applicable.

\subsection{General Setup}
\label{sec:ExpNotesGeneralSetup}
We run 1000 queries 5 times for each variable parameter and register the total running time for each set of 1000 queries and then use the average of those 5 as the result.
Examples of variable parameters used in our tests are the alphabet size, the skew of the tree or the block size.

We calculate the standard deviation of the 5 runs and include that in the graphs as errorbars.
All our graphs with lines (as opposed to bar-graphs) include the standard deviation as errorbars.
If one appears not to have any errorbars that means the standard deviation is just very small.

\subsection{Choice of Input String}
We have chosen to construct the input strings used in our experiments so that each character occurs with the same probability at each position.
This means the string has a uniform distribution of characters from the alphabet.
We have chosen to do so for several reasons, among them being that we believe it to be a realistic use case \textbf{[TODO: why?]}, as well as making the choice of character to query for in our experiments make less difference.
The even amount of occurrences of each character also means there will be little difference in size of bitmap among the nodes in a single layer of the tree.

\subsubsection{Uniform vs. Non-Uniform data}
Uniform data for a wavelet tree is a string with each character occurring with the same probability at each position and therefore with similar frequencies.
If the data is non-uniform it means that some symbols from the alphabet will appear with a significantly higher frequency than others.
If one knows the frequencies of all symbols in the alphabet, without needing it to be exact, then one can build a Huffman shaped Wavelet Tree and we believe it will beat out any balanced wavelet tree in terms of performance.
The frequencies can be found by a simple linear scan of the input string before building the tree.

We are more interested in the general case of being able to perform well on any input string and so we don't want to implement optimizations that require a specific character distribution in the input string.
Using non-uniform data for our testing with a general wavelet tree will also introduce bias into our results.
This is because the sizes of the bitmaps in each node in a given level of the tree would not be equal, as they would for uniform data.
When we query the tree and take a path with many large bitmaps it will take longer than a path with many small bitmaps.
Depending on which non-uniform distribution is used, some characters might not even appear at all in the input string, and querying for them would terminate early.
This is especially true for Select queries that find the leaf node corresponding to the character that was queried for and then spends most of the its time traversing up the tree looking for the position in the complete input string.
If the queried-for character didn't exist in the input string, a select query would terminate before even having gone down the full length of the tree, because the leaf node corresponding to the character doesn't exist.
Rank queries on the other hand would still take much the same time on non-occurring characters as on occurring characters, because they spend most of their time in the single downward traversal of the tree they perform, and still will perform for non-occurring characters as it is only near the end of this traversal that the character will be found to not occur.

Therefore having nun-uniform data would introduce a bias in our query tests based on the symbol we are querying for, one that would be difficult to avoid without introducing more bias by choosing exactly which characters to query for.

There is also the problem of choosing which occurrence to query for in the case of select, as the character should occur at least that many times.
When using uniform data we know that it is extremely unlikely for any character to occur less than some minimum number of times, because of their equal occurrence probability.

If we used non-uniform data we would also have the problem of choosing which non-uniform distribution we should use, and there are many to choose from.

\subsubsection{Non-uniform distribution choice}
The wavelet tree has applications within full text indexing which suggests that having a distribution based on how words are distributed with in a normal English text could be a good choice, because it would be a realistic use case. 
Zipf's Law describes such a distribution \citep[abstract]{ZipfsLawOnText} but it requires a distribution parameter $s$ that describes the frequency relation between each symbol, e.g. if the most frequent word has double the frequency of the 2nd most frequent word and this relation continues down the list of most frequent words, it is a Zipf's Law distribution with an $s$ parameter of 2.
We have not been able to find anyone describing which parameter value would produce a distribution most closely resembling real world English. 
We have searched through various articles to find an \textit{s} value representative of the English language, but it does not seem like there is a single good value as it depends a lot on the type of text, e.g. scientific journals vs. newspapers vs. books.
This is also a conclusion that \citep[abstract]{ZipfsLawOnText} arrives at.

It is possible to estimate \textit{s} for a given text but doing so then creates the problem of choosing a representative text to estimate \textit{s} from.
We tried using the word frequencies in the NGSL \citep{NGSL} wordlist which contain the 31,241 most used words in the English language and the frequency with which they appear, to estimate \textit{s}.
NGSL is based on data from the Cambridge English Corpus which is a multi-billion word collection of written, spoken and learner texts and is the largest of its kind.
This fact combined with the fact that NGSL is fairly new (2013) makes us believe that the frequencies in the NGSL wordlist are accurate enough for our purpose.

Estimating \textit{s} using NGSL gave us a value very close to 1 which only grows closer to 1 the more words we used from the list.
This is a problem because with an $s$ parameter of 1, a Zipf's Law distribution is uniform.
It also tells us that the English language, or at least the part that has been aggregated in NGSL, might not, in fact, be following the Zipf's Law very closely, as we would expect our calculations to converge towards some constant value $>1$.
Instead we use the data from the NGSL word list and generate our own non-uniform dataset from it based directly on the word frequencies found in the NGSL word list.
This way we end up with a more realistic non-uniform dataset than if we had used the Zipf's Law model since the data is based on real empirical data and we avoid the problem of choosing a good \textit{s} value.


\subsection{Choice of Query Parameters}
\label{sec:choiceOfQueryParameters}
It is important to ensure that we don't introduce a bias in our experiments on the rank and select query performances by our choice of query parameters.
As we have chosen to use a randomly generated input string with uniform distribution of characters for most of our tests, there should be little difference in the frequency of characters and little difference in query performance based on the exact choice of character.
There is, however a difference of where in the tree the node each character corresponds to, and we should make sure to use characters from various positions in the alphabet, to have the queries together traverse as much of the tree as possible.

For the rank queries there is also the position parameter, determining how far into the string the query should look and therefore how far into each bitmap the query should look.
A high value (close to the length of the string) might seem like a good idea to make the query go through most of the bitmaps, but we don't want to introduce a bias by using some constant high value, nor do we want to risk introducing a bias by only looking at high values for the position parameter.
Again we choose to use values from all parts of the range of valid values for the parameter.

We are also interested in avoiding introducing any bias by using only one type of combination of parameters.
If we had e.g. let both parameter values depend on the index of a single for-loop around the call to the query, we would have only tested low character values together with low position values as well as high character values together with high position values.

Instead we let one parameter ascend from valid low values to valid high values with even spacing to reach the highest valid value in the lastly performed query. Meanwhile, the other parameter increases more rapidly with wider spacing, and then wraps around before passing highest valid value to then start again at low values, with an offset to not repeat parameter values, doing so many times before the end.
This ensures we perform the queries for all combinations of high, medium and low parameter values in our experiments.
