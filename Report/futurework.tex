\section{Future Work}
We have many ideas for future work on practical implementation and optimization of wavelet trees.

\subsection{Interleaving Bitmap and Precomputed Cumulative Sum Values}
Calculating the binary rank using a precomputed cumulative sum value and a bitmap requires a lookup in two separate vectors both of which can introduce a cache miss.
If the precomputed cumulative sum values were to be interleaved with the bitmap, so that the precomputed value for a block would lie right next to that block from the bitmap, we expect the second of these cache misses could be avoided.

More precisely, in our implementation two vectors containing different data are stored: one containing the bitmap values and one containing the precomputed values.
Instead of this, a new data type could be defined that contained both a block of the bitmap and its precomputed value, and then a single vector containing this data type could then be stored.

We expect this would avoid a cache miss, because the access pattern of a rank query is to access the precomputed value for a block, and then the corresponding block in the bitmap, and if the block from the bitmap is in the same cacheline as the precomputed value, accessing it right after will not lead to a cache miss.

\subsection{vEB Memory Layout}
\label{sec:futurework_vebmemorylayout}
We tried a right-side depth-first memory layout in Section~\ref{sec:memorylayout} when we tried to skew the tree.
Without trying to skew the tree, other memory layouts might still be able to improve the performance of the wavelet tree.
Brodal et al.~\citeA{gerthSkewedBinarySearchTrees} tested several memory layouts for their skewed binary search tree and found that the blocked memory layout based on van Emde Boas Trees performed best for all skew values.
It could be interesting to try a van Embe Boas memory layout for a balanced wavelet tree to see if it could improve the query performance.

\subsection{$d$-ary}
Alex Bowe~\citeA{MultiaryWaveletTreesInPractice} has shown that multiary wavelet trees can work in practise.
In our implementations we have used a binary wavelet tree which means its height is the base-2 logarithm of the alphabet size.
With a $d$-ary tree the height would be reduced to base $d$ logarithm of the alphabet size.
This could improve access, rank, and select query performance significantly as their traversal down or up the tree would be significantly shortened.

A disadvantage of a $d$-ary wavelet tree is that each bitmap must encode $\log_2(d)$ bits of information for each character in the string, to signify which of the subtrees each character belongs to.
This makes using the native \texttt{popcount} cpu instruction impossible, perhaps unless some clever bitshifting and \texttt{XOR}ring could be applied to avoid manually counting sets of bits.
On the other hand, using the stored precomputed values means only few sets of bits would have to be counted and perhaps the benefit from a lower tree will outweigh the loss from not using \texttt{popcount}.

\subsubsection{SIMD}
When constructing or traversing a $d$-ary wavelet tree, finding which of 4 or more subtrees to either pass a character too or traverse into requires comparing the character with more than just one split character.
To improve the performance of this multi-way comparison, SIMD instructions might be employed with success.


\subsection{Parallelization}
To expand on the potential improvement from using SIMD instructions when constructing and traversing $d$-ary wavelet trees, some amount of parallelization of the algorithms might improve the performance even further.
\subsubsection{On GPU}
If parallelization proves to be an improvement, implementing them on the GPU e.g. using CUDA could be a massive improvement as modern GPUs have several hundred cores and if well-utilized can surpass the power of a modern CPU.

\subsection{RRR structure}
The RRR structure allows computation of binary rank in $O(1)$ time. 
It also implicitly achieves zero-order compression of the data.
RRR uses some of the same concepts as we do in our CumulativeSum implementation: Precomputed ranks, cumulative sum of those and concatenation of bitmaps.
It could be interesting to measure and analyse the hardware penalties in this structure, and perhaps improve its running time.

