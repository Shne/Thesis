\section{Simple, Naïve Algorithm}
This section deals with the simple, straightforward, naïve implementation based on ~the description by Navarro~\citeA[Section 2]{Navjda13}, before any smart ideas and optimizations were introduced.
We will call this version of the wavelet tree SimpleNaive.

The construction of the wavelet tree is implemented similarly to the pseudo-code in Section~\ref{sec:nodeconstruction}.
In our implementation $Alphabet$, $LeftAlphabet$, and $RightAlphabet$ are stored as two integer values each: a minimum and a maximum.
It is explained in Section~\ref{sec:UsingIntAsChar} how this is equivalent to storing the full alphabet and passing pointers into it around.
$Bitmap$ is stored as a \texttt{vector<bool>} which is a tightly packed data structure, only using 1 bit per bool\footnote{\url{http://www.cplusplus.com/reference/vector/vector-bool/}}, plus a little bookkeeping data and at most 8 bytes minus 1 bit of superfluous stored data when the amount of bits stored does not align with 8 bytes.

Rank queries were implemented as described in Section~\ref{sec:rankDescription} and
binary rank was implemented as described in Section~\ref{sec:TheoryBinaryRank}.
Select queries were implemented as described in Section~\ref{sec:selectDescription} with binary select implemented as described in Section~\ref{sec:binarySelectDescription}.

\subsection{Optimizations}
\label{sec:simpleoptimizations}
\subsubsection{Binary Rank using Popcount}
\label{sec:popcountBinaryRank}
 
To improve \textproc{BinaryRank} the intrinsic cpu instruction \texttt{popcount}, which will count the number of 1s in the binary representation of the number that is passed to it, is used.
Our use of \texttt{popcount} to improve binary rank and select queries was inspired by~ González et al.~\citeB{Gonzalez05practicalimplementation} who used it to improving binary rank and binary select for bit arrays.
Unlike González et al., we do not use a popcount function implemented in software, but rather the built in \texttt{popcount} instruction in the CPU instruction set.
An \texttt{unsigned long} pointer can be retrieved from the \texttt{vector<bool>} bitmap into the backing array. The implementation will therefore be working on \texttt{unsigned long}s and we will call their size (64 bits on machine 1) our wordsize.
When using \texttt{popcount}, \textproc{BinaryRank} remains in theory an $O(N/wordsize) = O(N)$ operation, as \textit{wordsize} is a constant factor, but it has a large practical effect on performance as can be seen in Section~\ref{sec:experimentPopcountRankSelect}.

To use \texttt{popcount} we call \texttt{\_\_builtin\_popcountl} which is a function built into the GCC compiler~\citeB{Popcount-GCC-Builtin}. 
It takes an \texttt{unsigned long} as a parameter and returns the number of 1s in it. \texttt{\_\_builtin\_popcountl} will automatically figure out how to do popcount based on what CPU you are using. Popcount as an intrinsic cpu instruction is supported on both AMD~\citeA{AMD-Popcount} and Intel arhitectures~\citeA{Intel-Popcount}.
We have verified, by looking at the produced assembly code, that \texttt{popcount} is calculated using a cpu instruction \texttt{popcnt} on our computers which we assume to be the fastest way to calculate \texttt{popcount} since it is only a single instruction.
	
The binary rank can then be found by summing the result of calling \texttt{popcount} on each word of the bitmap up to a given \textit{position}.
When the position argument of the rank query is not a multiple of the word size, it is necessary to constrain what part of the last word is counted using \texttt{popcount}. This can be done by constructing a bitmask by bitshifting the number 1 \textit{position} times towards the most significant bit and then subtracting one, as that will create a word where the \textit{position} least significant bits are set to 1 and the rest to 0.
Then we do a bitwise \texttt{AND} operation of this bitmask and the word containing the bit corresponding to \textit{position}, and call \texttt{popcount} on the result. 

As also noted in~\citeA{Navjda13}, we do not need to count the number of 0s, although required by the algorithm, as we can simply take the number of bits in the bitmap and subtract the number of 1s to calculate the number of 0s.

\subsubsection{Binary Select using Popcount}
\label{sec:ImplBinarySelect}
We improved \textproc{Binary Select} by again using the \texttt{popcount} instruction. 
We iterate through the words of the bitmap and call \texttt{popcount} for each word and sum up the results along the way. 
When the sum after the next word would be greater than the sought number of occurrences we discard the \texttt{popcount} result for the next word and fall back to the simple binary select for that next word to find the position within that word.

We define the input occurrence parameter as $io$, the number of words iterated through so far as $w$, the sum so far as $sum$, and the wordsize as $ws$. 
The occurrence argument for that last simple binary select is then the $io - sum$ and the output position is $w \times ws + \textproc{BinaryRank}(bitmapwords[ws+1], io - sum)$ \footnote{Using simple binary rank without \texttt{popcount}}.

Again, when popcount for 0s instead of 1s is needed, we simply subtract the result of \texttt{popcount} from 1 to obtain the count of 0s.

\subsection{Experiments}

\subsubsection{Uniform vs. Non-Uniform data}
\label{sec:uniformVsNonUniform}
We have tested and graphed the build wall time as well as the rank and select query wall times in Figure~\ref{fig:UniformVsNonUniform}.
The non-uniform data has been generated by extracting word frequencies from the NGSL~\citeA{NGSL} word list, then generating a $10^8$ characters long string using an alphabet of integer characters the size of the NGSL word list, each character with the corresponding frequency from the word list, but randomly permuted so each frequency of character occurs at a random place in the alphabet.
The permutation was done to avoid the bias of having all the most frequent characters in the beginning of the alphabet and thus in the leftmost side of the wavelet tree.

In Section~\ref{sec:expnotesUniformVsNonuniform} we theorized that building the tree on non-uniform data would be slightly faster as some of the characters would not occur in the string and therefore some of the nodes in the tree would not have to be created.
We also theorized that much the same would happen for Select queries as it could terminate much faster when a character could not be found in the tree.
We did not expect it would make as much difference for rank queries as they cannot terminate as early as select queries for non-occurring characters.

Looking at Figure~\ref{fig:UniformVsNonUniform} we see that our theories have been confirmed. 
The build time in Figure~\ref{fig:UniformVsNonUniform_Build_WallTime} is noticeably lower for the non-uniform data.
The select query time in Figure~\ref{fig:UniformVsNonUniform_Select_WallTime} is almost half for non-uniform data as that of uniform data.
The rank wall times are much closer and this time it is on uniform data that it is slightly faster, by about 1.6\%.
We expect this is because the non-uniform data just so happens to be distributed so that some of the query parameters used in our test result in a slightly slower execution compared to the uniform data.
When looking at the numbers of branch mispredictions and cache misses and so forth, we find that the rank queries on the non-uniform data have about 4\% more level 3 cache misses, but little difference in the other measurements.


%\newgeometry{left=2cm,right=2cm, top=2cm, bottom=3cm}
\begin{figure}\tiny
	\begin{subfigure}{0.31\textwidth}
		\input{UniformVsNonUniform_Build_WallTime}
		\caption{Build Wall Time}
		\label{fig:UniformVsNonUniform_Build_WallTime}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.31\textwidth}
		\input{UniformVsNonUniform_Rank_WallTime}
		\caption{Rank Wall Time}
		\label{fig:UniformVsNonUniform_Rank_WallTime}
	\end{subfigure}	
	\hfill
	\begin{subfigure}{0.31\textwidth}
		\input{UniformVsNonUniform_Select_WallTime}
		\caption{Select Wall Time}
		\label{fig:UniformVsNonUniform_Select_WallTime}
	\end{subfigure}
\caption{Build time and Rank and Select query time for uniform and non-uniform data based on the NGSL word list.}
\label{fig:UniformVsNonUniform}
\end{figure}
%\restoregeometry

\subsubsection{Running time of Tree Construction vs Alphabet Size}

We would like to find out whether our implementation of the construction of the tree conforms to the theoretical running time of $O(n \log \sigma)$ and how much of an improvement using the \texttt{popcount} cpu instruction was for the queries.

The general test setup is as described in Section~\ref{sec:ExpNotesGeneralSetup}.
The query parameters were chosen as described in Section~\ref{sec:choiceOfQueryParameters}.

To test what Big-O notation running time our construction algorithm was running at, we tested the running time of building the tree relative to the alphabet size by running the program 5 times for each size of the alphabet, and took the average value of the resulting measurements for each measurement type we used.
We tested for alphabet sizes $2^p$ with $p = [8...23]$ and used a constant input string of length $n = 10^8$, except in a single test (Figure~\ref{fig:naiveIntegerAlphabetSize_WallTime_plusSigma}) where we used $n = 10^2$.

A theoretical running time of $O(n \cdot \log(\sigma))$ is equivalent to $a \cdot n \cdot \log(\sigma)$ where $a$ is some constant factor.
Assuming our construction algorithm has this running time, a plot of the wall time divided by $n \cdot \log(\sigma)$ should converge on the constant factor $a$ as $\sigma \rightarrow \infty$
In Figure~\ref{fig:naiveIntegerAlphabetSize_WallTime} we have plotted this, and find that it could be said to be converging on a constant value until reaching an alphabet size of about $2^{16}$ whereafter it increases as $\sigma$ increases, with what looks like exponential growth.
This means our implementation of the construction of a Wavelet Tree is not conforming to the theoretical running time for higher alphabet sizes.



To attempt to understand why our algorithm performs so, we turn to the many other measurements available to us through PAPI: Branch Mispredictions, Cache Misses, etc.

Looking at the raw Wall Time and Branch Misprediction numbers in Figure~\ref{fig:naiveIntegerAlphabetSize_WallTime_BM} it might seem natural to conclude that the Branch Mispredictions are to blame.

But if we instead plot the rate of Branch Mispredictions, as we have done in Figure~\ref{fig:naiveIntegerAlphabetSize_BMRate}, we can see that the rate of Branch Mispredictions stay constant for most of the tested alphabet sizes, and even decrease for large alphabet sizes.

We next turn to look at Cache Misses, plotting all three levels in Figure~\ref{fig:naiveIntegerAlphabetSize_CM} and see that cache misses increase for larger alphabet sizes up to an alphabet size of around $2^{18}$ after which they seem to remain constant for even larger alphabet sizes.
This is in contrast to the wall time over theoretical running time plot in Figure~\ref{fig:naiveIntegerAlphabetSize_WallTime} that seem to remain somewhat constant until about $2^{18}$ after which it increases.
We conclude the Cache Misses are not the problem.

We then considered that the difference in theoretical and practical running time might be explained by the fact that our algorithm spends a constant amount of time per node, constructing it, independent on the size of the input, $n$, and scaling linearly with alphabet size, $\sigma$.
The actual running time should then be $a \cdot n \cdot \log(\sigma) + b \cdot \sigma$.
Since $n$ in our previous experiment is somewhat large ($10^8$), it might be the dominating factor in the running time.
So to show whether the added $b \cdot \sigma$ term can explain the running time, we redid the experiment with a reduced length input string $n = 10^2$ and plotted it in Figure~\ref{fig:naiveIntegerAlphabetSize_WallTime_plusSigma} divided by $\log(\sigma) + \sigma$ to see whether it would converge on some constant as $\sigma \rightarrow \infty$.

We can see in Figure~\ref{fig:naiveIntegerAlphabetSize_WallTime_plusSigma} that it does not converge on any constant value other than 0, meaning the constant factor from each node cannot explain our implementation's running time.

Having not found an explanation we pull data for Translation Lookaside Buffer Misses from the experiment and plot it together with wall time, both divided by $log(\sigma)$, in Figure~\ref{fig:naiveIntegerAlphabetSize_WallTime_TLB}




We can see that the TLB Misses increase drastically from alphabet size about $2^{20}$ and up.
Having found no other reasonable explanation for the discrepancy between the theoretical and our implementation's actual running time, we believe that TLB Misses is the culprit.

In our further experimentation of the further improvements we do we will be using an alphabet size of $2^{16}$.
It is a realistic use cases to use types such as \texttt{char}, \texttt{wchar16\_t} and \texttt{wchar32\_t} which are stored in 8, 16 and 32 bits respectively.
\texttt{char}'s size of 8 bits corresponds only to the ASCII table with 256 entries and we believe that many real-world scenarios require a larger alphabet.
\texttt{wchar16\_t} enables an alphabet up to $2^{16} = 65,536$, which should be enough for many use cases, such as full text indexing.
Zachery B. Simpson~\citeB{bookWordStatistics} has found that no book occurring in the Gutenberg Project uses more than 43113 distinct words.
According to one website~\footnote{http://testyourvocab.com/blog/2013-05-10-Summary-of-results}, testing suggests an average adult has a vocabulary of 20,000 - 35,000 words. 
Others~\footnote{http://www.worldwidewords.org/articles/howmany.htm}~\footnote{http://english.stackexchange.com/questions/93289} cite researchers saying about 60,000 words is the actual limit when including names.
Whichever is the actual number, they all suggest that an alphabet size of $2^{16}$ is sufficient to index all the occurring words in a realistic use case text.

Looking at the graphs from this experiment we can see that building the tree using an alphabet size of $2^{16}$ is still fairly quick, not running into much trouble with TLB misses, and not exceeding the expected asymptotically bound running time.

We also attempted using an alphabet size of 32, but our machine did not have enough memory for that to be possible.

\subsubsection{Rank and Select using Popcount}
\label{sec:experimentPopcountRankSelect}
We wanted to see how much of an improvement using the native cpu instruction \texttt{popcount} was, and how it affected the cache misses, branch mispredictions and TLB misses.


In Figure~\ref{fig:rankPopcountDiff}, Figure~\ref{fig:selectPopcountDiff}, and Figure~\ref{fig:valuesForPopcountDiff} we see the resulting relative cpu cycles, wall time, branch mispredictions, translation lookaside buffer misses, and cache misses for our rank and select queries, respectively.
We have chosen the measured values of the queries not using popcount to be index 100 and calculated and plotted the relative values of the queries using popcount, to show which values increase or decrease by what relative amount, all within the same graph.
In Figure~\ref{fig:valuesForPopcountDiff} we list the actual raw values as well as the percentages graphed in Figure~\ref{fig:rankPopcountDiff} and Figure~\ref{fig:selectPopcountDiff}.

In all three figures we see that the algorithm using \texttt{popcount} is much faster, using only a fraction of the time of the other algorithm, about $0.011\%$ for rank and $0.0051\%$ for select.
We see a massive decrease in branch mispredictions for both rank and select queries. For the select queries we see a great reduction in Translation Lookaside Buffer Misses as well as Cache Misses, especially Level 2 and 3.
For the rank queries, we see little improvement in TLBM and L1 CM, as well as a slightly larger improvement in L3 CM, but we also see a high increase in L2 CM Rate to more than double.
The higher L2 Cache Miss Rate comes from both having fewer L2 Cache Hits and many more L2 Cache Misses.
We have no good explanation to offer as to the L2 Cache Miss Rate increases so much, other than the algorithm being different and having a different access pattern.
It is not a problem at any rate, as we can see in the massive decrease in running time.

We believe the massive reduction in branch mispredictions accounts for some of the saved cpu cycles.
Given that the branch misprediction penalty on the Ivy Bridge architecture (on which this experiment was run) is about "15 cycles or more"~\citeA[Section 3.7]{agner}, we can calculate an estimate of how many cpu cycles the branch misprediction reduction has saved us.
The number of saved branch mispredictions for \textproc{rank} is then 1,396,757,616.4 - 22,682.2 = 1,396,734,934.2 mispredictions. Assuming a penalty of 15 cycles this becomes 1,396,734,934.2 $\cdot$ 15 = 20,951,024,013 cpu cycles saved, and given that the total number of cycles saved is 397,345,711,110.4 - 4,423,412,635.0 = 392,922,298,475, it is 20,951,024,013 / 392,922,298,475 = 0.05332103597 = 5.33\% of the total amount of cpu cycles saved.
This means that the branch mispredictions do have an effect, but it is only a small part of this increase in speed. The main improvement, we expect, comes from using only a few cpu cycles per word of the bitmap to calculate the binary rank, as well as possibly the slight decrease in L1 and L3 cache misses.


By similar calculations the saved cpu cycles from branch mispredictions for \textproc{select} is at least 48,84\% of the total saved. We expect this is because of the much higher number of branch mispredictions and the lower number of cycles for the original select algorithm, as well as the fact that we cannot use \texttt{popcount} for every word of the bitmap but must go back to doing manual counting of the bits when we find the word the sought-after occurrence is in.


\subsubsection{Further improvements}
Looking at the values in Figure~\ref{fig:valuesForPopcountDiff} we find that, of the measurements we collect, cache misses are among the highest and were not reduced significantly by using the \texttt{popcount} instruction.
Cache misses are expensive and reducing them could greatly increase the speed of queries on the wavelet tree.

Inspired by Brodal et al.~\citeA[Introduction]{gerthSkewedBinarySearchTrees} we will in Section~\ref{sec:memorylayout} look at skewing the tree as a means to reduce cache misses and branch mispredictions.



\newgeometry{left=2cm,right=2cm, top=2cm, bottom=3cm}
\begin{figure}\tiny
	\begin{subfigure}{0.48\textwidth}
		\input{naiveIntegerAlphabetSize_WallTime}
		\caption{Wall Time divided by theoretical running time}
		\label{fig:naiveIntegerAlphabetSize_WallTime}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.48\textwidth}
		\input{naiveIntegerAlphabetSize_WallTime_BM}
		\caption{Wall Time and Branch Mispredictions}
		\label{fig:naiveIntegerAlphabetSize_WallTime_BM}
	\end{subfigure}	
	
	\begin{subfigure}{0.48\textwidth}
		\input{naiveIntegerAlphabetSize_BMRate}
		\caption{Branch Misprediction Rate}
		\label{fig:naiveIntegerAlphabetSize_BMRate}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.48\textwidth}
		\input{naiveIntegerAlphabetSize_CM}
		\caption{Level 1-3 Cache Misses}
		\label{fig:naiveIntegerAlphabetSize_CM}
	\end{subfigure}
	
	\begin{subfigure}{0.48\textwidth}
		\input{naiveIntegerAlphabetSize_WallTime_plusSigma}
		\caption{Wall Time divided by $log(\sigma) + \sigma$}
		\label{fig:naiveIntegerAlphabetSize_WallTime_plusSigma}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.48\textwidth}
		\input{naiveIntegerAlphabetSize_WallTime_TLB}
		\caption{Wall Time and Translation Lookaside Buffer Misses divided by theoretical running time}
		\label{fig:naiveIntegerAlphabetSize_WallTime_TLB}
	\end{subfigure}
	\caption{Various Measurements for the construction of the SimpleNaive Wavelet Tree over various alphabet sizes.}
	\label{fig:naiveIntegerAlphabetSize}
\end{figure}
\restoregeometry
\clearpage

\newgeometry{left=2cm,right=2cm, top=2cm, bottom=3cm}
\begin{figure}\tiny
	\begin{subfigure}{0.48\textwidth}
		\input{popcountRankNew}
		\caption{Rank}
		\label{fig:rankPopcountDiff}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.48\textwidth}
		\input{popcountSelectNew}
		\caption{Select}
		\label{fig:selectPopcountDiff}
	\end{subfigure}
	\caption{Simple binary rank and select queries vs. rank and select queries using the popcount instruction.}
\end{figure}
\begin{figure}
\center
	\caption{Values for Figure~\ref{fig:rankPopcountDiff} and Figure~\ref{fig:selectPopcountDiff}}
	\label{fig:valuesForPopcountDiff}
	\input{valuesForPopcount}
\end{figure}

\restoregeometry



