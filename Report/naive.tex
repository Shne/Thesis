\section{Simple, Naïve Algorithm}
This is the simple, straightforward, naïve implementation we did before any smart ideas and optimizations.
\subsection{Algorithm Description}
The Naïve Wavelet Tree construction algorithm is recursively defined, calling itself to construct the left and right sub-tree from the root node and down. At each recursion the algorithm splits the given alphabet in two halves and traverses the given string putting each character into a left or right partition based on whether the character was in the left or right half of the alphabet.

\begin{algorithm}
\caption{Construction of nodes in the Wavelet Tree}
\label{alg:ConstructNode}
\begin{algorithmic}
\Function {ConstructNode} {$String, Alphabet$}
\If{$Alphabet.Size() = 1$ or $String.Length() = 0$}
	\State \Return
\EndIf
\State Split $Alphabet$ into $LeftAlphabet$ and $RightAlphabet$
\State $Split \gets$ middle character in $Alphabet$
\ForAll {$Character$ in $String$}
	\If {$Character < Split$}
		\State $LeftString.Append(Character)$
		\State $Bitmap.Append(0)$
	\Else
		\State $RightString.Append(Character)$
		\State $Bitmap.Append(1)$
	\EndIf
\EndFor
\State $LeftNode \gets$ \Call {ConstructNode} {$LeftAlphabet, LeftString$}
\State $RightNode \gets$ \Call {ConstructNode} {$RightAlphabet, RightString$}
\EndFunction

\State \Call {ConstructNode} {InputString, InputAlphabet}
\end{algorithmic}
\end{algorithm}

\noindent In our implementation, $Alphabet$, $LeftAlphabet$, and $RightAlphabet$ are stored as two integer values each: a minimum and a maximum. It is explained in~\ref{sec:UsingIntAsChar} how this is equivalent to storing the full alphabet and passing pointers into it around. $Bitmap$ is stored as a \texttt{vector<bool>} which is tightly packed, only using 1 bit per bool\footnote{\url{http://www.cplusplus.com/reference/vector/vector-bool/}}.

\subsection{Optimizations}
\subsubsection{Binary Rank using Popcount}
\label{sec:popcountBinaryRank}
Binary Rank was first implemented using the algorithm described in \ref{sec:TheoryBinaryRank}. 
To improve on this, we found the intrinsic cpu instruction \texttt{popcount} which will count the number of 1s in the binary representation of the number we pass to it.
We can get an \texttt{unsigned long} pointer from the \texttt{vector<bool>} bitmap into the backing array. We will therefore be working on \texttt{unsigned long}s and call their size (64 bits on machine 1) our wordsize.
When using \texttt{popcount}, \textproc{BinaryRank} remains in theory an $O(N/wordsize) = O(N)$ operation, as \textit{wordsize} is a constant factor, but it has a large practical effect on performance as can be seen in section~\ref{sec:experimentPopcountRankSelect}.

To use \texttt{popcount} we call \texttt{\_\_builtin\_popcountl} which is a function built into the GCC compiler \citep{Popcount-GCC-Builtin}. 
It takes an \texttt{unsigned long} as a parameter and returns the number of 1s in it. \texttt{\_\_builtin\_popcountl} will automatically figure out how to do popcount based on what CPU you are using. Popcount as an intrinsic cpu instruction is supported on both AMD \citep{AMD-Popcount} and Intel arhitectures \citep{Intel-Popcount}.
We have verified, by looking at the produced assembly code, that \texttt{popcount} is calculated using a cpu instruction \texttt{popcnt} on our computers which we assume to be the fastest way to calculate \texttt{popcount} since it is only a single instruction.
	
The binary rank can then be found by summing the result of calling \texttt{popcount} on each word of the bitmap up to a given \textit{position}.
When the position argument of the rank query is not a multiple of the word size, we need to constrain what part of the last word is counted using \texttt{popcount}. This can be done by constructing a bitmask by bitshifting the number 1 \textit{position} times towards the most significant bit and then subtracting one, as that will create a word where the \textit{position} least significant bits are set to 1 and the rest to 0.
Then we do a bitwise \texttt{AND} operation of this bitmask and the word containing the bit corresponding to \textit{position}, and call \texttt{popcount} on the result. 

As also noted in~\citep{Navjda13}, we don't need to count the number of 0s, although required by the algorithm, as we can simply take the number of bits in the string and subtract the number of 1s to calculate the number of 0s.

\subsubsection{Binary Select using Popcount}
\label{sec:ImplBinarySelect}
We improved \textproc{Binary Select} by again using the \texttt{popcount} instruction. 
We iterate through the words of the bitmap and call \texttt{popcount} for each word and sum up the results along the way. 
When the sum after the next word would be greater than the sought number of occurrences we discard the \texttt{popcount} result for the next word and fall back to the simple binary select for that next word to find the position within that word.

Defining the input occurrence parameter as $io$, the number of words iterated through so far $w$, the sum so far $sum$, and the wordsize $ws$, the occurrence argument for that last simple binary select is then the $io - sum$ and the output position is $w \times ws + \textproc{SimpleBinaryRank}(bitmapwords[ws+1], io - sum)$.

Again, when popcount for 0s instead of 1s is needed, we simply subtract the result of \texttt{popcount} from 1 to obtain the count of 0s.

\subsection{Experiments}
We would like to find out whether our implementation of the construction of the tree conforms to the theoretical running time of $O(n \log \sigma)$ and how much of an improvement using the \texttt{popcount} cpu instruction was for the queries.


\subsubsection{Running time of Tree Construction vs Alphabet Size}
To test what Big-O notation running time our construction algorithm was running at, we implemented the following test setup.
We tested the running time of building the tree relative to the alphabet size by running the program 5 times for each size of the alphabet we tested, and took the average value of the resulting measurements for each measurement type we used.
We tested for alphabet sizes $2^p$ with $p = [8..23]$ and a constant input string length of $n = 10^8$, except in a single test (Figure~\ref{fig:naiveIntegerAlphabetSize_WallTime_plusSigma}) where we used $n = 10^2$.

A theoretical running time of $O(n \cdot log(\sigma))$ is equivalent to $a \cdot n \cdot log(\sigma)$ where $a$ is some constant factor.
Assuming our construction algorithm has this running time, a plot of the wall time divided by $n \cdot log(\sigma)$ should converge on the constant factor $a$ as $\sigma \rightarrow \infty$
In Figure~\ref{fig:naiveIntegerAlphabetSize_WallTime} we have plotted this, and find that it does not appear to be converging on any constant value, but rather increases as $\sigma$ increases, especially for alphabet sizes larger than about $2^{18}$
This means our implementation of the construction of a Wavelet Tree is not keeping within its theoretical running time.

\figureBegin	
	\caption{Construction: Wall Time divided by theoretical running time over Alphabet Size}
	\label{fig:naiveIntegerAlphabetSize_WallTime}
	\input{naiveIntegerAlphabetSize_WallTime}
\figureEnd

To try and understand why our algorithm performs so, we turn to the many other measurements available to us through PAPI: Branch Mispredictions, Cache Misses, etc.

Looking at the raw Wall Time and Branch Misprediction numbers in Figure~\ref{fig:naiveIntegerAlphabetSize_WallTime_BM} it might seem natural to conclude that the Branch Mispredictions are to blame.
\figureBegin
	\caption{Construction: Wall Time and Branch Mispredictions over Alphabet Size}
	\label{fig:naiveIntegerAlphabetSize_WallTime_BM}
	\input{naiveIntegerAlphabetSize_WallTime_BM}
\figureEnd
\figureBegin
\caption{Construction: Branch Misprediction Rate over Alphabet Size}
\label{fig:naiveIntegerAlphabetSize_BMRate}
\input{naiveIntegerAlphabetSize_BMRate}
\figureEnd
But if we instead plot the rate of Branch Mispredictions, as we have done in Figure~\ref{fig:naiveIntegerAlphabetSize_BMRate}, we can see that the rate of Branch Mispredictions stay constant for most of the tested alphabet sizes, and even decrease for large alphabet sizes.

We next turn to look at Cache Misses, plotting all three levels in Figure~\ref{fig:naiveIntegerAlphabetSize_CM}.
\figureBegin
\caption{Construction: Level 1-3 Cache Misses over Alphabet Size}
\label{fig:naiveIntegerAlphabetSize_CM}
\input{naiveIntegerAlphabetSize_CM}
\figureEnd
We see that cache misses increase for larger alphabet sizes up to an alphabet size of around $2^{18}$ after which they seem to remain constant for even larger alphabet sizes.
This is in contrast to the wall time over theoretical running time plot in Figure~\ref{fig:naiveIntegerAlphabetSize_WallTime} that seem to remain somewhat constant until about $2^{18}$ after which it increases.
We conclude the Cache Misses are not the problem.

We then considered that the difference in theoretical and practical running time might be explained by the fact that our algorithm spends a constant amount of time per node, constructing it, independent on the size of the input, $n$, and scaling linearly with alphabet size, $\sigma$.
The actual running time should then be $a \cdot n \cdot log(\sigma) + b \cdot \sigma$.
Since $n$ in our previous experiment is somewhat large ($10^8$), it might be the dominating factor in the running time.
So to show whether the added $b \cdot \sigma$ term can explain the running time, we redid the experiment with a reduced length input string $n = 10^2$ and plotted it in Figure~\ref{fig:naiveIntegerAlphabetSize_WallTime_plusSigma} divided by $log(\sigma) + \sigma$ to see whether it would converge on some constant as $\sigma \rightarrow \infty$.
\figureBegin
\caption{Construction: Wall Time divided by $log(\sigma) + \sigma$ over Alphabet Size}
\label{fig:naiveIntegerAlphabetSize_WallTime_plusSigma}
\input{naiveIntegerAlphabetSize_WallTime_plusSigma}
\figureEnd
We can see in Figure~\ref{fig:naiveIntegerAlphabetSize_WallTime_plusSigma} that it does not converge on any constant value other than 0, meaning the constant factor from each node cannot explain our implementation's running time.

Having not found an explanation we pull data for Translation Lookaside Buffer Misses from the experiment and plot it together with wall time, both divided by $log(\sigma)$, in Figure~\ref{fig:naiveIntegerAlphabetSize_WallTime_TLB}

\figureBegin
	\caption{Construction: Wall Time and Translation Lookaside Buffer Misses divided by theoretical running time  over Alphabet Size}
	\label{fig:naiveIntegerAlphabetSize_WallTime_TLB}
	\input{naiveIntegerAlphabetSize_WallTime_TLB}
\figureEnd


We can also see that the TLB Misses increase drastically from alphabet size about $2^{20}$ and up.
Having found no other reasonable explanation for the discrepancy between the theoretical and our implementation's actual running time, we find it likely that TLB Misses is the culprit.

In our further experimentation of the further improvements we do we will be using an alphabet size of $2^{16}$.
It is a realistic use cases to use types such as \texttt{char}, \texttt{wchar16\_t} and \texttt{wchar32\_t} which are stored in 8, 16 and 32 bits respectively.
\texttt{char}'s size of 8 bits corresponds only to the ASCII table with 256 entries and we believe that many real-world scenarios require a larger alphabet.
\texttt{wchar16\_t} enables an alphabet up to $2^{16} = 65,536$, which should be enough for many use cases, such as full text indexing.
Zachery B. Simpson~\citep{bookWordStatistics} has found that no book found in the Gutenberg Project uses more than 43113 distinct words.
According to one website~\footnote{http://testyourvocab.com/blog/2013-05-10-Summary-of-results}, their testing suggests an average adult has a vocabulary of 20,000 - 35,000 words. 
Others~\footnote{http://www.worldwidewords.org/articles/howmany.htm}~\footnote{http://english.stackexchange.com/questions/93289/are-there-acknowledged-studies-about-the-relationship-of-vocabulary-and-comprehe} cite researchers saying about 60,000 words is the actual limit when including names.
Whichever is the actual number, they all suggest an alphabet size of $2^{16}$ is sufficient to index all the occurring words in a realistic use case text.

Looking at the graphs from this experiment we can see that building the tree using an alphabet size of $2^{16}$ is still fairly quick, not running into much trouble with TLB misses, and not exceeding the expected asymptotically bound running time.

We also attempted using an alphabet size of 32, but our machines didn't have enough memory.

\subsubsection{Rank and Select using Popcount}
\label{sec:experimentPopcountRankSelect}
We wanted to see how much of an improvement using the native cpu instruction \texttt{popcount} was, and how it affected the cache misses, branch mispredictions and TLB misses.

We wrote our program to build the tree, then run 100 rank or select queries for characters evenly distributed across the alphabet.
For the rank queries we used the size of the input string as the positional argument so that the entire tree would be used in the query.
We ran each test 5 times and used the average value for each measurement.
For the select queries we queried for the 2000th occurrence because previously run rank queries showed us that each character we queried for occurred about 2400-2600 times in the input string we used, and we could then be reasonably sure that each character in the alphabet would occur at least 2000 times.

By querying for characters evenly distributed along the alphabet, we ensure that the queries together traverse most of the tree, and thus try to avoid having everything cached for every query. Another option would have been to use randomly selected characters, but that would have introduced some uncertainty as to whether it was the access pattern or the change in algorithm that caused a difference in running time.



\figureBegin
\caption{Simple Binary Rank vs. Binary Rank using the Popcount instruction.}
\label{fig:rankPopcountDiff}
\input{popcountRankNew}
\figureEnd

\figureBegin
\caption{Simple Binary Select vs. Binary Select using the Popcount instruction.}
\label{fig:selectPopcountDiff}
\input{popcountSelectNew}
\figureEnd

\figureBegin
\center
\caption{Values for Figure~\ref{fig:rankPopcountDiff} and \ref{fig:selectPopcountDiff}}
\label{fig:valuesForPopcountDiff}
\input{valuesForPopcount}
\figureEnd

In Figure~\ref{fig:rankPopcountDiff}, \ref{fig:selectPopcountDiff}, and \ref{fig:valuesForPopcountDiff} we see the resulting relative cpu cycles, wall time, branch mispredictions, translation lookaside buffer misses, and cache misses for our rank and select queries, respectively.
We have chosen the values for the queries not using popcount to be index 100 and calculated and plotted the relative values of the queries using popcount, to show which values increase, decrease and by what relative amount, all within the same graph.
In Figure~\ref{fig:valuesForPopcountDiff} we list the actual raw values as well as the percentages graphed in Figure~\ref{fig:rankPopcountDiff} and \ref{fig:selectPopcountDiff}.

In all three figures we see that the algorithm using \texttt{popcount} is much faster, using only a fraction of the time of the other algorithm, about $0.011\%$ for rank and $0.0051\%$ for select.
We see a massive decrease in branch mispredictions both for both rank and select queries. For the select queries we see a great reduction in Translation Lookaside Buffer Misses as well as Cache Misses, especially Level 2 and 3.
For the rank queries, we see little improvement in TLBM and L1 CM, as well as a slightly larger improvement in L3 CM, but we also see a great increase in L2 CM to more than double the amount.
[TODO: speculate as to why]


The massive reduction in branch mispredictions likely accounts for some of the saved cpu cycles.
Given that the branch misprediction penalty on the Ivy Bridge architecture (on which this experiment was run) is about "15 cycles or more"~\cite{agner}, we can calculate an estimate of how many cpu cycles the branch misprediction reduction has saved us.
The number of saved branch mispredictions for \textproc{rank} is then 309,342,424.4 - 1,882.6 = 309,340,541.8 mispredictions. Assuming a penalty of 15 cycles this becomes 309,340,541.8 $\cdot$ 15 = 4,640,108,127 cpu cycles saved, and given that the total number of cycles saved is 88,918,629,772.2 - 994,895,919.6 = 87,923,733,852.6, it is 4,640,108,127 / 87,923,733,852.6 = 0.0527742388 = 5.28\% of the total amount of cpu cycles saved.
This means that the branch mispredictions do have an effect, but it is only a small part of this increase in speed. The main improvement, we expect, comes from using only a few cpu cycles per word of the bitmap to calculate the binary rank, as well as possibly the slight decrease in L1 and L3 cache misses.


By similar calculations the saved cpu cycles from branch mispredictions for \textproc{select} is at least 48,38\% of the total saved. We expect this is because of the much higher number of branch mispredictions and the lower number of cycles for the original select algorithm, as well as the fact that we can't use \texttt{popcount} for every word of the bitmap but must go back to doing manual counting of the bits when we find the word the sought-after occurrence is in.


\subsubsection{Further improvements}
Looking at the values in Figure~\ref{fig:valuesForPopcountDiff}, we find that, of the measurements we collect, cache misses are among the highest and were not reduced significantly by using the \texttt{popcountl} instruction.
Cache misses are expensive and reducing them would likely greatly increase the speed of queries on the wavelet tree.

Inspired by Brodal et al.~\cite{gerthSkewedBinarySearchTrees} we will in section~\ref{sec:memorylayout} look at skewing the tree as a means to reduce cache misses and branch mispredictions.

