\section{Precomputing Binary Rank in Blocks}
During our experiment of skewing the tree, we concluded that most of the work during queries is performed inside each node, calculating the binary rank of each bitmap.
It is simply a summing up of popcounts of each word, and we considered whether precomputing these sums for blocks of several words of the bitmaps could improve the query times.

The size of these precomputed blocks is a new variable that could have influence on the running time and memory usage.
Some advantages of larger blocks is less memory usage and fewer precomputed value lookups for the same part of the bitmap.
Some advantages of smaller blocks are that they are more often useful as exemplified by extreme case of a block size equal to the bitmap size which is of little use compared to a block size of half the bitmap size when we only use part of the bitmap for a single query.

All the bitmaps will likely not be page-aligned and some might be smaller than the block size, yet we would still like to utilize the precomputed values in these cases.
A way of achieving this would be to concatenate all bitmaps into one giant bitmap for the entire tree and precompute rank values for each block, keeping them in a separate vector and then, when needing the binary rank of a part smaller than the block size, use the precomputed value and subtract the binary rank of the extra part covered by the block.
Note, this is only worth doing when the part we want to compute the binary rank for is larger than half the block size.
See Section~\ref{sec:rankQueriesWithPrecomputedRanksEdgeCases} for more explanation of this.




\subsection{Concatenating the Bitmaps}
We allocate the bitmaps as one giant bitmap the size of the maximum possible size required to store all the bitmaps for all the nodes. The sum of the size of all bitmaps on one layer of the tree can at most be $n$ and we can at most have $h$ layers, so the maximum size becomes
\[n \cdot h\]
where $n$ is the number of characters in the string and $h$ is the max height of the binary wavelet tree which is
\[ h = log(\sigma) \]
We then store an offset and a size for the bitmap in each node, so we can index into the giant bitmap and access the bits corresponding to the node. 
If we had allocated a bitmap for each node individually, they would have been word-aligned, and the bits between the end of the last used bit in the last used word at the end of one bitmap and the start of another would have gone unused and so, wasted.

Similarly, we allocate a vector for holding the precomputed block values of size
\[ \frac{BitmapSize}{BlockSize} \]

Instead of storing a pointer to the bitmap and precomputed values vector in each node, we store them once for the whole tree and then pass them to the methods that need them.
The nodes now additionally contain, in addition to these pointers, an offset and a size.


\subsection{Rank Queries with Precomputed Ranks}
The rank of a string can be expressed as the sum of the rank of any number and various sizes of subparts as long as they together perfectly cover the string and don't overlap.
we can use this property for performance gain if we split the bitmap into many smaller parts, which we will call blocks henceforth, and precompute the rank values of those blocks.
We can compute the rank values easily and cheaply by doing so as we build the tree where we already need to compute and store each individual bit of the bitmap.
We simply increment a counter for the corresponding block each time we set a bit to 1 in the bitmap.
Using a separate vector to store these counters and integer division of the bitmap offset with the block size to index into said vector, it is an efficient and simple way to precompute the rank values of fixed sized and uniformly distributed parts of the bitmap.


\subsubsection{Edge Cases}
\label{sec:rankQueriesWithPrecomputedRanksEdgeCases}
Because the parts must perfectly cover the string and not overlap, and the bitmaps of each node are not of same size, nor multiples of some single value, we have a problem if we want to use uniformly sized and distributed parts (blocks).
The problem exists at the boundary between bitmaps, where the precomputed rank value will be the sum of the rank of the end of the first bitmaps and the rank of the beginning of the second bitmap.

Looking at a single bitmap for a node, there is an edge case for the first and last part of the bitmap, because they don't fill an entire block, so we can't simply use the corresponding precomputed value as-is.
Instead, we can compute the rank of the part of the block that our part doesn't fill and subtract that from the precomputed rank value.
This is only worth doing when our part fills more than half a block, because then the other part is smaller than half a block and therefore quicker to compute.
Figure ~\ref{fig:PrecomputePopcountBlock} illustrates this.

\figureBegin
\caption{Rank value of a part of a bitmap is equal to the precomputed value for the block minus the rank of the other remaining part.}
\label{fig:PrecomputePopcountBlock}
\includegraphics[width=\textwidth]{PrecomputePopcountBlock.pdf}
\figureEnd



\subsubsection{Page-aligning the Blocks}
Translation Lookaside Buffer misses are expensive and to avoid those, we can try to reduce the number of pages we load.
Using the precomputed vector of ranks, we only need to load pages of the bitmap at the beginning and end of each node's bitmap, to compute the popcount binary rank directly on the bitmap, and only within one block at each end.
If the blocks are not aligned with the memory pages, then, even if the block size is less than a page, it might span more than one page and thus more than one page of memory must be loaded into the TLB.
More precisely, we might at most load $\lceil\frac{blockSize}{pageSize}\rceil +1$ pages to do the popcount binary rank computation at the beginning and end of each node.

If we page-align the blocks, and use block sizes divisible by the page size or that page is divisible by, the extra $+1$ will disappear, because a block can no longer span more pages than its size is a multiple of the page size.
This means we can ensure that at most $2 \cdot \lceil\frac{blockSize}{pageSize}\rceil$ memory pages of the bitmap are loaded for each node by page-aligning the blocks.



\subsection{Select Queries with Precomputed Ranks}
Select queries, although they don't return a rank value, can still utilize the precomputed rank values to skip much computation directly on the bitmap by iterating though them.
Partway in a select query, if the sum of the occurrences found so far and the rank value of the current block of the bitmap is more than the queried-for occurrence, we can add that rank value to our occurrences seen so far and skip ahead to the next block and perform the same test.
If the sum is less than the queried-for occurrence, we know we will find the occurrence in the current block and switch back to the previous method of select querying using popcount starting at this block.


\subsubsection{Edge Cases}
As with rank queries, there are edge cases at the beginning and end of each bitmap.
However, in this case, the edge case at the end is easily handled as the test of sum of rank and occurrence-so-far should fail, sending the algorithm into the block with the previous select query method, finding the occurrence with no problem and no specific handling of the edge case.
This is assuming the input occurrence parameter is valid, that is, at least that many occurrences of that character is in the original input string.

For the other case, at the beginning of the bitmap, we do almost exactly the same as in the case for the rank query.
In fact, we use a rank query to calculate the rank of the first part of the bitmap, using the trick of subtracting from the precomputed value if larger than half a block size, to figure out whether the occurrence is in the first part of the bitmap, and therefore whether we should skip it or not.

\paragraph{Using Rank Queries in Select Queries}~\\
We would like to analyse whether it is worth using a rank query to find out whether we should do a select query on the first part of the bitmap, as the rank query is purely extra work in the cases where the occurrence is in that part of the bitmap.

We will assume an equal number of occurrences in the string of each character in the alphabet, a uniform distribution of each character in the string, and an equal probability of each valid parameter for the select query.
A valid character parameter is one that exists in the input string and a valid occurrence parameter is an integer above 0 and below or equal to the number occurrences appearing in the input string.

The rank query is a computation of worst-case cost $\frac{blockSize}{2}$ because it will at most popcount half of the block, because we utilize the precomputed rank value when advantageous.
The popcount select query into the first partial block is an operation of word-case cost $blockSize$ because it can at most popcount the entire block.
So, in the worst case, when the sought-after occurrence is in the first partial block of the bitmap, $\frac{blockSize}{2}$ work is wasted, yet when the occurrence is elsewhere, $blockSize$ work is saved.
This means that the boundary between where using the rank query becomes a gain or a loss in performance is where the ratio between times the occurrence is to be found in and outside the first partial block of the bitmap is $\nicefrac{2}{3}$.
That is, when the occurrence is to be found outside the first partial block of the bitmap less than two-thirds of the time, using the rank query first to see if that partial block should be select queried is a gain in performance, when considering the worst-case query time for both queries.

Though, this is giving the select query a disadvantage in the analysis, as, even when the partial block is close to a block in size, it might terminate early if it found the sought-after occurrence of the character early in the partial block.

It is not entirely clear whether it is an advantage to use the binary rank call, or it would be better to use popcount BinarySelect on the first partial block at all times.
We will test this in section~\ref{sec:experimentPrecomputedBlocksRankInSelect}



\subsection{Extra Space Used}
Since each precomputed value cannot exceed the block size in value, assuming we don't use block sizes exceeding $2^{16} = 65536$ bits, or $\frac{2^{16}}{8} = 8192$ bytes, we can store them in 16-bit unsigned integers, called \texttt{unsigned short int} on our machines. Since the page size on our machines are 4096 bits, that we should not use a block size larger than $\frac{65536}{4096} = 16$ pages if we want to use 16-bit unsigned integers.

In Section[TODO:reference] we find that the optimal block size is 16384 bits, or 4 pages.

Assuming we then store the precomputed values as 16-bit unsigned short integers it will only consume an extra 16 bits per block of which there are $\frac{BitmapSize}{BlockSize}$.
This means, assuming 4 pages per block, an extra space consumption of
\[ \frac{16}{16384} = 0.0009765625 = 0.098\% \]
of the bitmaps.
Which is even less when considering the total space used including the nodes, and would be less for larger block sizes.

Using one giant bitmap instead of one for each node might also cause a reduction in memory usage from no longer having unused, yet stored, bits at the end of each bitmap.


\subsection{Experiments}
\subsubsection{Query Running Time for Bitmap with Precomputed Blocks for different Block Sizes}

\subsubsection{Using Rank Queries in Select Queries}
\label{sec:experimentPrecomputedBlocksRankInSelect}

\subsubsection{Memory Usage of Concatenated vs. Individual Bitmaps}