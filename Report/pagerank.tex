\section{Precomputing Binary Rank in Blocks}
Using the profiling tool callgrind, we concluded that most of the work during queries is performed inside each node, calculating the binary rank of each bitmap.
Rank queries are simply summing up results of popcounting each word, and we considered whether precomputing these sums for blocks spanning several words, covering the bitmaps could improve the query times.
When a block does not line up with the position of a rank query is for, the algorithm can simply fall back to doing popcounting of either the remaining uncovered words or the extraneously covered words, whichever has fewer words.
See Section~\ref{sec:rankQueriesWithPrecomputedRanksEdgeCases} for more explanation of this.

The rank values can be precomputed easily and cheaply by doing so as the tree is built where each individual bit of the bitmap already needs to be computed and stored.
The algorithm increments a counter for the corresponding block each time it sets a bit to 1 in the bitmap.

The size of the precomputed blocks, $b$, is a new variable that could have influence on the running time and memory usage.
Some advantages of larger blocks is less memory usage and fewer precomputed value lookups for the same part of the bitmap.
Some advantages of smaller blocks are that they can cover more precisely the part of the bitmap that is relevant to the query, leading to fewer calls to popcount.
Later, in Section~\ref{sec:bDependOnN}, we will analyse how the optimal block size $b$ depends on the input size $n$, and later again we will experiment with varying block sizes to see how it works in practise for a wavelet tree and to see how it corresponds to the theoretical analysis.

To further reduce the space used by the precomputed values, we considered concatenating all the bitmaps into one big bitmap and keeping a single vector of precomputed rank values for blocks for the entire bitmap. 
That would eliminate the many cases where the length of a bitmap does not align with the block size and the last precomputed value for that bitmap will therefore not cover an entire block, leading to more precomputed values than minimally needed to cover all the bitmaps.

We also considered the cost of TLB misses and how ensuring that entire pages are skipped as often as possible might increase the query performance. We try to achieve this by page-aligning the blocks.

We will test and compare the Rank and Select running times and memory usage of four wavelet trees using precomputed rank values for blocks: one using concatenated bitmaps and aligned blocks, one using concatenated bitmaps but not aligned blocks, one only using aligned blocks and one using neither.





\subsection{Concatenating the Bitmaps}
The bitmaps are allocated as one giant bitmap the size of the maximum possible size required to store all the bitmaps for all the nodes.
They are stored in a Depth-First-Search-right (DFSr) manner, that is, the bitmap of the right child of a node comes right after the bitmap of the node.

There are many other alternative memory layouts that might provide better performance.
The one we expect to have the greatest potential for improving the wavelet tree performance, is the van Emde Boas memory layout~\citeA[Abstract]{Brodal02cacheoblivious} because of its cache-oblivious nature.
It would be a challenge to implement, because the size of a bitmap of a node is unknown before the bitmap of the parent has been calculated, and so the position of each bitmap in the giant bitmap cannot be known before the parent of each has been calculated, meaning construct the tree must be constructed in vEB layout order.
To support construction of the wavelet tree in vEB layout order our construction algorithm would have to be dramatically changed, possibly utilizing concurrency constructs such as a job queue and message passing, and so we skip this work for now and save it for future work in Section~\ref{sec:futurework_vebmemorylayout}.

The sum of the size of all bitmaps on one layer of the tree can at most be $n$ and we can at most have $\log \sigma$ layers, so the maximum size becomes
\[n \log \sigma\;,\]
where $n$ is the number of characters in the string and $\sigma$ is the alphabet size.
Luckily for us, memory allocation in Linux does not actually take up space because Linux uses optimistic memory allocation which means that only when the memory is accessed will the actual physical memory be used \footnote{\url{http://man7.org/linux/man-pages/man3/malloc.3.html}}.
This fact enables overcomitting which allows allocation of more memory than what is available which can be a problem for long running processes.
As a result we can conclude that allocated memory is not present in physical memory before it is actually initialized.
The effect of an optimistic memory allocation scheme and overcomitting is tested by Andries Brouwer \footnote{\url{http://www.win.tue.nl/~aeb/linux/lk/lk-9.html}} who confirms that it is possible to allocated more memory using \texttt{malloc()} than what is physically available.
So over-allocating the bitmap should not take up any more space than what will actually be needed.

An offset and a size for the bitmap is the stored in each node, so it is possible to index into the giant bitmap and access the bits corresponding to the node.
This should also cause a decrease in memory usage as the offset and size are stored in an unsigned long and unsigned int respectively, taking a total of $64+32=96$ bits per node, where each individual bitmap requires storage of at least a pointer to it, a point to where its internal array starts and a pointer to where it ends, taking up $3 \times 64=192$ bits per node.
Additionally, when using an individual bitmap for each node, they would have been word-aligned, and the bits between the last used bit and the end of the last used word would have gone unused and so, wasted.

A vector is also allocated to hold the precomputed block values of size
\[\mathit{VectorSize} = \frac{\mathit{BitmapSize}}{b}.\]
Instead of storing a pointer to the bitmap and precomputed values vector in each node, they are stored once for the whole tree and then passed down through the query methods when the tree is queried.

In order to be able to index into said vector, integer division of the bitmap offset and the block size is used. It is an efficient and simple way to precompute the rank values of blocks of fixed size of the bitmap, as we do not have to traverse the tree again.


\subsubsection{Edge Cases}
\label{sec:rankQueriesWithPrecomputedRanksEdgeCases}
The rank of a string can be expressed as the sum of the rank of any number and various sizes of subparts as long as they together perfectly cover the string and do not overlap.
Because the blocks must perfectly cover the string and not overlap, and the bitmaps of each node are not of same size, nor multiples of some single value, we have a problem if we want to use uniformly sized and distributed blocks.
The problem exists at the boundary between bitmaps, where the precomputed rank value will be the sum of the rank of the end of the first bitmaps and the rank of the beginning of the second bitmap.

Looking at a single bitmap for a node, there is an edge case for the first and last part of the bitmap, because they do not fill an entire block, so the corresponding precomputed value cannot simply be used as-is.
Instead, the rank of the part of the block that the bitmap does not fill can be computed and then subtracted from the precomputed rank value.
This is only worth doing when the bitmap fills more than half a block, because then the other part is smaller than half a block and therefore quicker to compute.
Figure~\ref{fig:PrecomputePopcountBlock} illustrates this.



\figureBegin
\caption{Rank value of a part of a bitmap is equal to the precomputed value for the block minus the rank of the other remaining part.}
\label{fig:PrecomputePopcountBlock}
\includegraphics[width=\textwidth]{PrecomputePopcountBlock.pdf}
\figureEnd




%\begin{algorithm}
%\caption{Rank Query for PreallocatedPrecomputed}
%\label{dsfdsfsd}
%\begin{algorithmic}
%\State \Comment{\textproc{PopcountRank} is a function that calculates the rank of a given part of a bitmap using the popcount cpu instruction}
%\Function{Rank}{Position, Bitmap, PrecomputedRanks}
%\If{Position $<$ BlockSize/2}
%\State \Return \Call{PopcountRank}{This bitmap}
%\EndIf
%\If{Position $<$ BlockSize \textit{and} Position $<$ LengthToNextBlock}
%\State PreRank $\gets$ \Call{PopcountRank}{Part of block before this bitmap}
%\State PostRank $\gets$ \Call{PopcountRank}{Part of block after this bitmap}
%\State BlockIndex $\gets$ \textit{Index of the block this bitmap is in}
%\State \Return PrecomputedRanks[BlockIndex] - PreRank - PostRank
%\EndIf
%\If{
%\EndFunction
%\end{algorithmic}
%\end{algorithm}




\subsubsection{Page-aligning the Blocks}
Translation Lookaside Buffer misses are expensive and to avoid those, we can try to reduce the number of pages that is loaded.
Using concatenated bitmaps and the precomputed vector of ranks, we only need to load pages of the bitmap at the beginning and end of each node's bitmap, to compute the popcounted version of binary rank directly on the bitmap, and only within one block at each end.
If the blocks are not aligned with the memory pages, then, even if the block size is less than a page, it might span more than one page and thus more than one page of memory must be loaded into the TLB.
More precisely, the algorithm might at most load \[2 \cdot \left( \left\lceil\frac{b}{\mathit{pageSize}}\right\rceil +1 \right)\] pages to do the popcount binary rank computation at the beginning and end of each node.

If the blocks are page-aligned, and has block sizes divisible by the page size or that the page is divisible by, the extra $+1$ will disappear, because a block can no longer span more pages than the number of pages its size is a multiple of.
This means we can ensure that at most \[2 \cdot \left\lceil\frac{b}{\mathit{pageSize}}\right\rceil\] memory pages of the bitmap are loaded for each node by page-aligning the blocks.
With an alphabet size of $2^{16}$ this amounts to saving up to 16 page loads per query.

While we expect that this will save some expensive TLB misses it also has some drawbacks, especially when not using concatenated bitmaps.
For a wavelet tree not using concatenated bitmaps, using page-aligned blocks will cause the first precomputed value of each non-page-aligned bitmap to not cover an entire block, increasing the number of precomputed values needed to cover the bitmap as well as additional computations to calculate exactly which part of the bitmap it covers.
For the wavelet trees using concatenated bitmaps, these computations are needed regardless of block alignment, as the blocks are already not aligned with the individual bitmaps, and so we expect that it is an improvement to page-align the blocks in this case.
We test this by implementing a variation of the wavelet tree using concatenated bitmaps that does not page-align the blocks.

We test whether it increases the performance of rank and select queries when not using concatenated bitmaps in Section~\ref{sec:queryRunTimePrecomputedBlockSizes}.


\subsection{Select Queries with Precomputed Ranks}
Select queries, although they do not return a rank value, can still utilize the precomputed rank values to skip much computation directly on the bitmap by iterating though them.
Partway in a select query, if the sum of the occurrences found so far and the rank value of the current block of the bitmap is more than the queried-for occurrence, we can add that rank value to our occurrences seen so far and skip ahead to the next block and perform the same test.
If the sum is less than the queried-for occurrence, we know the occurrence will be found in the current block and the previously implemented method of calculating select using popcount can then be used, starting at this block.


\subsubsection{Edge Cases}
As with rank queries, there are edge cases at the beginning and end of each bitmap.
However, in this case, the edge case at the end is easily handled as the test of sum of rank and occurrence-so-far should fail, sending the algorithm into the block with the previous select query method, finding the occurrence with no problem and no specific handling of the edge case.
This is assuming the input occurrence parameter is valid, meaning that at least that many occurrences of that character is in the original input string.

For the other case, at the beginning of the bitmap, almost exactly the same as in the case for the rank query is done.
In fact, a rank query is used to calculate the rank of the first part of the bitmap, using the trick of subtracting from the precomputed value if larger than half a block size, to figure out whether the occurrence is in the first part of the bitmap, and therefore whether a select query should be run on it or not.

\paragraph{Using Rank Queries in Select Queries}~\\
We would like to analyze whether it is worth using a rank query to find out whether we should do a select query on the first part of the bitmap when using concatenated bitmaps, as the rank query is purely extra work in the cases where the occurrence is in that part of the bitmap.

We will assume an equal number of occurrences in the string of each character in the alphabet, a uniform distribution of each character in the string, and an equal probability of each valid parameter for the select query.
A valid character parameter is one that exists in the input string and a valid occurrence parameter is an integer above 0 and below or equal to the number of occurrences appearing in the input string.

The rank query is a computation of worst-case cost $O(\frac{b}{2})$ because it will at most popcount half of the block, because the precomputed rank value is utilized when advantageous.
The select query using popcount in the first partial block is an operation of worst-case cost $O(b)$ because it can at most popcount the entire block.
So, in the worst case, when the sought-after occurrence is in the first partial block of the bitmap, $O(\frac{b}{2})$ work is wasted, yet when the occurrence is elsewhere, $O(b)$ work is saved.
This means that the boundary between where using the rank query becomes a gain or a loss in performance is where the ratio between the number of times the occurrence is to be found in and outside the first partial block of the bitmap is $\frac{2}{3}$.
That is, when considering the worst-case query time for both queries, if the occurrence is to be found outside the first partial block of the bitmap more than one third of the time, using the rank query first to see if that partial block should be select queried is a gain in performance.

This is giving the select query a disadvantage in the analysis, even when the partial block is close to a block in size and it might terminate early if the sought-after occurrence of the character if found early in the partial block.

We expect it is an improvement, though it is not 100\,\% certain, but we will use it going forward for the algorithms using concatenated bitmaps.
We have considered doing tests to determine whether it is an improvement for the running time or not, but as we are under time constraints and we feel we have other, far more interesting, things to implement and test we will not be testing this.



\subsection{Extra Space Used by Precomputed Values}
Storing the precomputed values requires more memory: one number per block.
There are $O(\frac{n}{b})$ blocks per level of the tree, and so an extra memory consumption of $O(\frac{n}{b} \log \sigma)$ words making the total memory consumption $O(n \log \sigma + (\sigma + \frac{n}{b} \log \sigma) \cdot \mathit{ws})$ bits.

Since each precomputed value cannot exceed the block size in bits, assuming we do not use block sizes exceeding $2^{16} = 65536$ bits, or $\frac{2^{16}}{8} = 8192$ bytes, we can store them in 16-bit unsigned integers, called \texttt{unsigned short int} in C++ on our machine.
Since the page size on our machine are 4096 bytes we should not use a block size larger than $\frac{8192}{4096} = 2$ pages if we want to use 16-bit unsigned integers.

In Section~\ref{sec:queryRunTimePrecomputedBlockSizes} we find that the optimal block size is 16384 bits = 2048 bytes = $\frac{1}{2}$ page.

Assuming the precomputed values are then stored as 16-bit unsigned short integers it will only consume an extra 16 bits or 2 bytes per block and there are $\frac{\mathit{BitmapSize}}{b}$ of these blocks when the bitmaps are concatenated.
This means, assuming a block size of 2048 bytes, a relative extra space consumption of
\[ \frac{ 2 \cdot \frac{\mathit{BitmapSize}}{b} }{\mathit{BitmapSize}} = \frac{2}{b} = \frac{2}{2048} = 0.0009765625 = 0.098\,\% \]
of the bitmaps, which is even less when considering the total space used including the nodes.

When the bitmaps are not concatenated there is a higher space consumption by the precomputed values, as each precomputed value do not cover an entire block and more precomputed values is therefore needed to cover all the bitmaps.
When the blocks are not page-aligned, each node has potentially one precomputed value not covering an entire block at the end of its bitmap.
When blocks are page-aligned, there is another precomputed value potentially not covering an entire block at the beginning of each bitmap.
The extra space consumption by the precomputed values when not concatenating the bitmaps is therefore limited proportionally to the number of nodes, which is at most $2 \sigma - 1$, making it limited proportionally by the alphabet size.

We expect to see a difference in memory usage between using concatenated bitmaps and non-concatenated bitmaps as well as between using page-aligned and non-page-aligned blocks.
However, we expect most of the difference to come from the space used by the bitmaps themselves, and therefore a noticeable difference between the data structure concatenating the bitmaps and the others, with a little difference between using page-aligned and non-page-aligned blocks, with the one using page-aligned blocks using most memory.


\subsection{Dependence of Optimal Block Size on Input Size}
\label{sec:bDependOnN}
Whether or not using precomputed values in blocks is an improvement in running time of rank queries or not, depends on which block size is used.
If the block size is only 1 bit, then there is nothing to be gained from looking up the value via the precomputed rank instead of looking at the bit in the bitmap.
If the block size is the same size as the entire bitmap, then it can only be useful when the positional parameter $p$ for the rank query is above half the size of the bitmap, as the rank of a smaller part of the bitmap beyond the halfway point can then be calculated and subtracted.

The work needed to compute the binary rank of a bitmap of size $n$ without using a precomputed value, but using popcount on pieces (machine words) of the bitmap of size \textit{ws} is $O(\frac{n}{\mathit{ws}})$ to scan the bitmap using popcount up to the word spanning position $p$ and $O(1)$ to calculate the rank up to position $p$ within that word using popcount, making it in total $O(\frac{n}{\mathit{ws}})$.

When using lookups of precomputed values, the analysis is similar.
It costs $O(\frac{n}{b} + b)$ to calculate the binary rank when using precomputed values, as it costs $O(\frac{n}{b})$ to scan the blocks, and $O(b)$ to calculate the rank within a single block using popcount.
The optimal block size should be one that minimizes this.
The derivative of $\frac{n}{b}+b$ is $1-\frac{n}{b^2}$ and its root is $n = b^2$ making the optimal block size $b = \sqrt{n}$.
This is only the optimal block size for a single bitmap, and a wavelet tree has many bitmaps of varying sizes $n$ that are lower near the leaves.
This means that the best block size in a wavelet tree is either one that varies for each bitmap or, if using a fixed block size, some value below the theoretically optimal block size for the root bitmap.

Later, in Section~\ref{sec:expOptimalBlockSizeDependsOnInputSize}, we show using a fixed block size $b$ for all bitmaps in a wavelet tree, whether the optimal $b$ does indeed depend on $n$ and whether the practically optimal value of $b$ is below the theoretically optimal size of $b$ for the root bitmap.



\subsection{Experiments}
We will test and compare the Rank and Select running times of three wavelet trees using precomputed rank values for varying block sizes: one using concatenated bitmaps and aligned blocks named Preallocated, one using concatenated bitmaps but not aligned blocks named UnalignedPreallocated, one using aligned blocks but not concatenated bitmaps, called Naive,
and one using unaligned blocks and non-concatenated bitmaps called UnalignedNaive.
In table-form:\\
\begin{tabular}{|lcc|}
\hline
Name						& Concatenated Bitmaps	& Page-aligned Blocks	\\ \hline
Preallocated				& yes					& yes					\\ \hline
UnalignedPreallocated	& yes					& no						\\ \hline
Naive					& no						& yes					\\ \hline
UnalignedNaive			& no						& no						\\ \hline
\end{tabular}\\

\noindent Later, we will compare the memory usage and query times with the non-precomputed version called SimpleNaive.

\paragraph{Test Setup}~\\
The general setup is as described in Section~\ref{sec:ExpNotesGeneralSetup}.
The query parameters were chosen as described in Section~\ref{sec:choiceOfQueryParameters}.

\subsubsection{Query Running Time for Bitmap with Precomputed Blocks for different Block Sizes}
\label{sec:queryRunTimePrecomputedBlockSizes}

\paragraph{Rank Queries}~\\\\
In Figure~\ref{fig:PrecomputedRankBlockSize_Rank_WallTime} we have plotted the rank query wall time in $\mu s$ for the wavelet trees using precomputed rank values.
See Figure~\ref{fig:PrecomputedRankBlockSize_Rank_WallTime} in Appendix A for a graph of the same, covering a wider range of block sizes, from $2^{6}$ to $2^{20}$ bits, showing that the wall time is worse for both smaller and larger block sizes than the ones in Figure~\ref{fig:PrecomputedRankBlockSize_Rank_WallTime}.

We see that both concatenating the bitmaps and page-aligning the blocks is consistently slower, which was expected for concatenating the bitmaps, but not entirely so for page-aligning the blocks.
Preallocated is on average about 6.22\,\% slower than UnalignedPreallocated, so page-aligning the blocks when using concatenated bitmaps is a 6.22\,\% performance hit.
Preallocated is on average about 13.85\,\% slower than Naive, meaning concatenating the bitmaps when using page-aligned blocks is a 13.85\,\% performance hit.

Naive has its fastest running time at 1 page per block, whereas both trees using concatenated bitmaps (Preallocated and UnalignedPreallocated) seem to have a slightly better performance with a slightly higher or slightly lower block size, at 0.75 and 1.25 page per block.

UnalignedNaive is the surprising outlier in this graph with a much lower wall time, especially for smaller block sizes.
At block size = 0.5 page size, where UnalignedNaive is fastest, it only takes 65.58\,\% of the time that Naive does at block size = 1 page size, where Naive is fastest.

Much of the increased running time of Rank queries on the two Preallocated wavelet trees can be explained by the increased amount of instructions needed to calculate the rank of the first part inside the first block of each bitmap because the precomputed value includes part of the preceding bitmap, as well as the ineffectiveness of utilizing the precomputed rank values for small bitmaps, for the same reason.
This is also corroborated by Figure~\ref{fig:PrecomputedRankBlockSize_Rank_BranchExe} that plots the number of branches executed.
It introduces more branches to the code to check for alignment and to find which part of the giant bitmap corresponds to the current node.
We can see that it is the Preallocated tree using both concatenated bitmaps and page-aligned blocks that execute the most branches, while UnalignedNaive executes much, much fewer than all the others.
This looks similar to the wall time plot in Figure~\ref{fig:PrecomputedRankBlockSize_Rank_WallTime}.

By examining Figure~\ref{fig:PrecomputedRankBlockSize_Rank_BranchMiss} and Figure~\ref{fig:PrecomputedRankBlockSize_Rank_BranchMissRate} we can conclude that part of the wall time difference between using and not using concatenated bitmaps is due to the increased number of branch misses from a higher branch miss rate.

In Figure~\ref{fig:PrecomputedRankBlockSize_Rank_BranchMissRate} we initially see a surprising increase in the branch misprediction rate of UnalignedNaive for smaller block sizes.
When looking at Figure~\ref{fig:PrecomputedRankBlockSize_Rank_BranchMiss} we see that it follows somewhat the same increase in branch misprediction amount for smaller block sizes as the others, making us conclude that UnalignedNaive has a higher branch misprediction rate alone because it has fewer easily predicted branches but the same amount of branches difficult to predict compared to the others for smaller block sizes.
We expect this is a result of UnalignedNaive where it is not necessary to do any calculations to figure out which part and how much of the bitmap the first precomputed value covers, as it always covers an entire block of the bitmap and is perfectly aligned with the start of the bitmap instead of a memory page.
This means that a number of if-statements comparing the size of the bitmap with the block size to figure out whether the precomputed value can be used is not present in UnalignedNaive, where they are present in the others.

Looking at Figure~\ref{fig:PrecomputedRankBlockSize_Rank_TLB} we can see that our expressed goal of reducing TLB misses when using page-aligned blocks is achieved when not using concatenated bitmaps, though only little but TLBs are not reduced when concatenated bitmaps are used which is in line with our expectation.
On the other hand, we see that TLB misses are reduced to about 29.80\,\% when using concatenated bitmaps in the page-aligned version and to about 26.39\,\% when not page-aligning the blocks.
This improvement was not enough to make up for the extra bookkeeping code, however, it seems.
We also notice that the two trees using non-concatenated bitmaps, Naive and UnalignedNaive has a noticeable drop in TLB misses at a block size of 1 page size.


Looking at the level 1 data cache misses we can see that there are fewest cache misses when a block size is equal to half a page size, with a sharp rise in cache misses again for smaller block sizes.
We expect this is the main reason that UnalignedNaive exhibits the best running time at that block size instead of at lower values.
The others also seems to have the best level 1 data cache performance for rank queries at half a page per block, while their wall times are best at a full page per block.
This might be explained by total branch execution, as we saw in Figure~\ref{fig:PrecomputedRankBlockSize_Rank_BranchExe}, they execute many more branches at lower block sizes, which we expect to be because of the extra bookkeeping code needed.

The level 2 data cache miss rate plotted in Figure~\ref{fig:PrecomputedRankBlockSize_Rank_L2CacheMissRate} is generally worst for block size = 0.5 page size, but looking at the raw cache misses in Figure~\ref{fig:PrecomputedRankBlockSize_Rank_L2CacheMiss} we generally see more cache misses for higher block sizes, meaning that the level 2 data cache miss rate cannot explain the better performance at block size = 1 page size for every tree other than UnalignedNaive.
Looking at Level 2 data cache misses in Figure~\ref{fig:PrecomputedRankBlockSize_Rank_L2CacheMiss} and the level 2 data cache miss rate in Figure~\ref{fig:PrecomputedRankBlockSize_Rank_L2CacheMissRate}, we do not see much difference between the different tree implementations, except that the Naive tree has a noticeable drop in both the raw amount and the rate at block size = 1 page size, just like it had for TLB misses.
It is interesting, though, that they all have a somewhat high, $0.35-0.4$, level 2 data cache miss rate around 0.5-0.75 page per block, where they all have good wall time performance.
This could be explained by the good level 1 data cache performance at those block sizes.
To explain what we mean, let us assume there is a fixed amount of operations accessing memory that is hard for the cache to have prefetched or otherwise loaded beforehand, independent of the block size.
E.g. when the queried-for position is reached in a bitmap and the rank algorithm jumps to a different node and a different bitmap.
When the first cache level can handle more and more of the 'easy' memory operations, fewer of those are left to be handled by the second cache level, yet the same amount of 'hard' memory accesses are hitting the second cache level, and so the miss rate of the second cache level will increase for lower block sizes, but not because of more misses, but because of fewer hits, as can be seen in Figure~\ref{fig:PrecomputedRankBlockSize_Rank_L2CacheHits}.
In fact, the level 1 cache misses and level 2 cache hits, in Figure~\ref{fig:PrecomputedRankBlockSize_Rank_L2CacheHits} and Figure~\ref{fig:PrecomputedRankBlockSize_Rank_L1CacheMiss} respectively, look near identical.

In Figure~\ref{fig:PrecomputedRankBlockSize_Select_L3CacheMiss} we have plotted the level 3 total cache misses.
We notice that all four trees have fairly low level 3 cache misses at the lowest tested block size of 0.25 page size.
The trees using concatenated bitmaps then rise to have the most level 3 cache misses at around 1 page per block then decreasing again at higher block sizes.
What is perhaps most interesting in this graph is that the Naive tree again has a large dip at 1 page per block.
We expect this dip, combined with the others, is what causes Naive to have its fastest rank query wall time at 1 page per block.


\paragraph{Select Queries}~\\\\
In Figure~\ref{fig:PrecomputedRankBlockSize_Select_WallTime} we see the wall time of 1000 Select queries for the different wavelet trees using precomputed rank values.
Again, see Figure~\ref{fig:PrecomputedRankBlockSize_Select_WallTime} in Appendix A for a graph of the same, covering a wider range of block sizes, from $2^{6}$ to $2^{20}$ bits, showing that the wall time is worse for both smaller and larger block sizes than the ones in Figure~\ref{fig:PrecomputedRankBlockSize_Select_WallTime}.
We can see that all have the best running time at half a page per block, though some of them have about the same speed at 0.75 page per block.
We expect the main reason for this is to be found in the level 1 data cache performance data, which is shown in Figure~\ref{fig:PrecomputedRankBlockSize_Select_L1CacheMiss} as we can see that they have better level 1 data cache performance at lower values, except for Naive, which has its best performance at 0.5 page per blockand for decreasing block sizes the cache misses increase again.

In much the same way as for rank queries, the branch mispredictions, as plotted in Figure~\ref{fig:PrecomputedRankBlockSize_Select_BranchMiss}, decrease as block size increases, but looking at Figure~\ref{fig:PrecomputedRankBlockSize_Select_BranchMissRate} we can see that the branch misprediction rate is highest at about 0.75 page per block and much smaller at smaller block sizes, this is because much more branching code, correctly predicted, is executed at block sizes below 0.75 pages per block, as can be seen in Figure~\ref{fig:PrecomputedRankBlockSize_Select_BranchExe}.

The amount of TLB misses across block sizes seen in Figure~\ref{fig:PrecomputedRankBlockSize_Select_TLB} is similar to the TLB misses for rank queries in Figure~\ref{fig:PrecomputedRankBlockSize_Rank_TLB} as we again see that the biggest reduction in TLB misses comes from using concatenated bitmaps and that page-aligning the blocks does make a difference when using non-concatenated bitmaps but no difference when using concatenated bitmaps.

Just as with rank queries, we see higher level 2 data cache miss rate at lower block sizes in Figure~\ref{fig:PrecomputedRankBlockSize_Select_L2CacheMissRate}, and again we expect the level 1 data cache misses are the cause as we see level 1 misses matches up with level 2 hits in Figure~\ref{fig:PrecomputedRankBlockSize_Select_L1CacheMiss} and Figure~\ref{fig:PrecomputedRankBlockSize_Select_L2CacheHits}.

However, unlike for the rank queries, the amount of level 2 data cache misses are not lower for smaller block sizes, in fact they are higher than at 1 page per block, where each type of wavelet tree has its minimum, with Naive having the largest drop there.
We see this drop again in level 3 total cache misses in Figure~\ref{fig:PrecomputedRankBlockSize_Select_L3CacheMiss}.
For both level 2 and level 3 cache misses, we also see a dip at 2 pages per block and lesser dips at 0.5 pages and 1.5 pages per block. 
These sizes correspond to where blocks most often align with full pages as a block of size 2 pages will align with two pages and two blocks of size half a page will align with one page and two blocks of size 1.5 page will align with three pages.

The fact that the level 2 and level 3 performances are so near-identical can be explained by the fact that they are inclusive, meaning that everything contained in level 2 is also in level 3 and
if all the cache misses in level 2 are from the prefetcher not being able to figure out what data is needed next, and it loads directly into level 2 then the level 3 cache will never have the correct data while 2 does not.

Why this results in fewer level 2 and level 3 cache misses, we do not know.






\begin{figure}[h!]\tiny
\begin{adjustwidth}{-1cm}{-1cm}
\begin{subfigure}{0.48\linewidth}
	\input{PrecomputedRankBlockSize_Rank_WallTime}
	\caption{Wall Time}
	\label{fig:PrecomputedRankBlockSize_Rank_WallTime}
\end{subfigure}
\hfill
\begin{subfigure}{0.48\linewidth}
	\input{PrecomputedRankBlockSize_Rank_BranchMiss}
	\caption{Branch Mispredictions}
	\label{fig:PrecomputedRankBlockSize_Rank_BranchMiss}
\end{subfigure}

\begin{subfigure}{0.48\linewidth}
	\input{PrecomputedRankBlockSize_Rank_BranchExe}
	\caption{Branches Executed}
	\label{fig:PrecomputedRankBlockSize_Rank_BranchExe}
\end{subfigure}
\hfill
\begin{subfigure}{0.48\linewidth}
	\input{PrecomputedRankBlockSize_Rank_BranchMissRate}
	\caption{Branch Misprediction Rate}
	\label{fig:PrecomputedRankBlockSize_Rank_BranchMissRate}
\end{subfigure}
\caption{Various measurements of Rank queries on Wavelet Trees with Precomputed Rank Values for varying block sizes, part 1}
\label{fig:PrecomputedRankBlockSize_Rank}
\end{adjustwidth}
\end{figure}


\newgeometry{left=2cm,right=2cm, top=2cm, bottom=3cm}

\begin{figure}\tiny
\begin{subfigure}{0.48\textwidth}
	\input{PrecomputedRankBlockSize_Rank_TLB}
	\caption{TLB Misses}
	\label{fig:PrecomputedRankBlockSize_Rank_TLB}
\end{subfigure}
\hfill
\begin{subfigure}{0.48\textwidth}
	\input{PrecomputedRankBlockSize_Rank_L1CacheMiss}
	\caption{Level 1 Data Cache Misses}
	\label{fig:PrecomputedRankBlockSize_Rank_L1CacheMiss}
\end{subfigure}
\begin{subfigure}{0.48\textwidth}
	\input{PrecomputedRankBlockSize_Rank_L2CacheMiss}
	\caption{Level 2 Data Cache Misses}
	\label{fig:PrecomputedRankBlockSize_Rank_L2CacheMiss}
\end{subfigure}
\hfill
\begin{subfigure}{0.48\textwidth}
	\input{PrecomputedRankBlockSize_Rank_L2CacheHits}
	\caption{Level 2 Data Cache Hits}
	\label{fig:PrecomputedRankBlockSize_Rank_L2CacheHits}
\end{subfigure}
\begin{subfigure}{0.48\textwidth}
	\input{PrecomputedRankBlockSize_Rank_L2CacheMissRate}
	\caption{Level 2 Data Cache Miss Rate}
	\label{fig:PrecomputedRankBlockSize_Rank_L2CacheMissRate}
\end{subfigure}
\hfill
\begin{subfigure}{0.48\textwidth}
	\input{PrecomputedRankBlockSize_Rank_L3CacheMiss}
	\caption{Level 3 Total Cache Misses}
	\label{fig:PrecomputedRankBlockSize_Rank_L3CacheMiss}
\end{subfigure}

\caption{Various measurements of Rank queries on Wavelet Trees with Precomputed Rank Values of varying block sizes, part 2}
\label{fig:PrecomputedRankBlockSize_Rank2}
\end{figure}






\begin{figure}\tiny
\begin{subfigure}{0.48\textwidth}
	\input{PrecomputedRankBlockSize_Select_WallTime}
	\caption{Wall Time}
	\label{fig:PrecomputedRankBlockSize_Select_WallTime}
\end{subfigure}
\hfill
\begin{subfigure}{0.48\textwidth}
	\input{PrecomputedRankBlockSize_Select_BranchMiss}
	\caption{Branch Mispredictions. Notice the y-axis not starting at 0.}
	\label{fig:PrecomputedRankBlockSize_Select_BranchMiss}
\end{subfigure}

\begin{subfigure}{0.48\textwidth}
	\input{PrecomputedRankBlockSize_Select_BranchExe}
	\caption{Branches Executed.}
	\label{fig:PrecomputedRankBlockSize_Select_BranchExe}
\end{subfigure}
\hfill
\begin{subfigure}{0.48\textwidth}
	\input{PrecomputedRankBlockSize_Select_BranchMissRate}
	\caption{Branch Misprediction Rate}
	\label{fig:PrecomputedRankBlockSize_Select_BranchMissRate}
\end{subfigure}
\caption{Various measurements of Select queries on Wavelet Trees with Precomputed Rank Values of varying block sizes, part 1}
\label{fig:PrecomputedRankBlockSize_Select}
\end{figure}

\begin{figure}\tiny
\begin{subfigure}{0.48\textwidth}
	\input{PrecomputedRankBlockSize_Select_TLB}
	\caption{TLB Misses}
	\label{fig:PrecomputedRankBlockSize_Select_TLB}
\end{subfigure}
\hfill
\begin{subfigure}{0.48\textwidth}
	\input{PrecomputedRankBlockSize_Select_L1CacheMiss}
	\caption{Level 1 Data Cache Misses}
	\label{fig:PrecomputedRankBlockSize_Select_L1CacheMiss}
\end{subfigure}

\begin{subfigure}{0.48\textwidth}
	\input{PrecomputedRankBlockSize_Select_L2CacheMiss}
	\caption{Level 2 Data Cache Misses}
	\label{fig:PrecomputedRankBlockSize_Select_L2CacheMiss}
\end{subfigure}
\hfill
\begin{subfigure}{0.48\textwidth}
	\input{PrecomputedRankBlockSize_Select_L2CacheHits}
	\caption{Level 2 Data Cache Hits}
	\label{fig:PrecomputedRankBlockSize_Select_L2CacheHits}
\end{subfigure}

\begin{subfigure}{0.48\textwidth}
	\input{PrecomputedRankBlockSize_Select_L2CacheMissRate}
	\caption{Level 2 Data Cache Miss Rate}
	\label{fig:PrecomputedRankBlockSize_Select_L2CacheMissRate}
\end{subfigure}
\hfill
\begin{subfigure}{0.48\textwidth}
	\input{PrecomputedRankBlockSize_Select_L3CacheMiss}
	\caption{Level 3 Total Cache Misses}
	\label{fig:PrecomputedRankBlockSize_Select_L3CacheMiss}
\end{subfigure}
\caption{Various measurements of Select queries on Wavelet Trees with Precomputed Rank Values of varying block sizes, part 2}
\label{fig:PrecomputedRankBlockSize_Select2}
\end{figure}


\begin{figure}\tiny
	\begin{subfigure}{0.48\textwidth}
		\input{PrecomputedRankBlockSize_Build_Memory}
		\caption{Reported by PAPI}
		\label{fig:PrecomputedRankBlockSize_PAPI}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.48\textwidth}
		\input{PrecomputedRankBlockSize_MemoryUsage}
		\caption{Reported by Massif.}
		\label{fig:PrecomputedRankBlockSize_Massif}
	\end{subfigure}
	\caption{Difference in Memory Usage of wavelet trees with precomputed ranks of varying block size. Notice the y-axis not starting at 0 and at different values.}
	\label{fig:PrecomputedRankBlockSize_Memory}
\end{figure}


\begin{figure}\tiny
	\begin{subfigure}{0.48\textwidth}
		\input{PrecomputedRankBlockSize_vsNaiveInteger_Rank}
		\caption{Rank}
		\label{fig:PrecomputedRankBlockSize_vsNaiveInteger_Rank}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.48\textwidth}
		\input{PrecomputedRankBlockSize_vsNaiveInteger_Select}
		\caption{Select}
		\label{fig:PrecomputedRankBlockSize_vsNaiveInteger_Select}
	\end{subfigure}
	\caption{Comparison of wall time of rank and select queries between SimpleNaive not using precomputed values and UnalignedNaive using precomputed values.}
	\label{fig:PrecomputedRankBlockSize_vsNaiveInteger}
\end{figure}

\restoregeometry




\subsubsection{Memory Usage of Precomputed Rank Values}
We have used both Massif, the heap profiler included in the Valgrind Suite and PAPI to record the memory usage of our programs.
We focus on the output from Massif instead of PAPI, because the memory usage values we could extract using PAPI made little sense, with values indicating that the trees using precomputed values used less memory than the SimpleNaive tree with no extra precomputed values.
We have plotted the values from PAPI in Figure~\ref{fig:PrecomputedRankBlockSize_PAPI}.

We do not know why PAPI gives us these nonsensical results, but it might have something to do with Linux's aggressive caching and buffering.
We are not entirely sure whether \texttt{delete}ed memory might still be counted as used memory by PAPI because of such caching.
Massif, on the other hand, manually counts calls to such functions as \texttt{new} and \texttt{delete} and calculates the memory usage from these, making it unaffected by any caching scheme employed by the Linux kernel.
Massif is also designed for the purpose of profiling program memory usage, whereas PAPI is simply an API to access performance metrics from the underlying OS.

Massif only counts heap allocations and deallocations per default and not for stack, data, BSS or code segments.
We manually tell Massif to count our stacks as well and include that in our calculations.
The remaining uncounted data, BSS and code segments should not affect our results noticeably as those segments are usually tiny compared to the size our program is using.
But it might be the case that it is in fact these segments causing the strange results from PAPI, and that using concatenated bitmaps uses more memory than not and storing more information in the tree somehow uses less memory, sometimes, though we find this unlikely and choose to trust Massif's output more.

Massif outputs 'snapshots' of the memory usage taken at certain points in the code where it deems them useful to the user.
We have not simply used the snapshots produced automatically by Massif, but rather used the Client Request mechanism~\footnote{\url{http://valgrind.org/docs/manual/manual-core-adv.html}}of valgrind to send a \texttt{snapshot} command to the valgrind gdbserver, telling it to take a snapshot right after we have finished building the tree.
The values presented in Figure~\ref{fig:PrecomputedRankBlockSize_Massif} is thus calculated from such Massif snapshots taken right after the tree has completed building.

Looking at Figure~\ref{fig:PrecomputedRankBlockSize_Massif} we see that the memory usage of the trees using precomputed values but not concatenated bitmaps, Naive and UnalignedNaive use more memory than the tree not using precomputed values, SimpleNaive, but only about 0.5\,\%.
We also see that memory is indeed saved by using the concatenated bitmaps in Preallocated and UnalignedPreallocated, though only about 1.5\,\% compared to the other trees using precomputed values.

Our attempts to reduce the memory usage by concatenating the bitmaps seems to have succeeded, but looking back at the running times for the rank and select queries in Figure~\ref{fig:PrecomputedRankBlockSize_Rank_WallTime} and Figure~\ref{fig:PrecomputedRankBlockSize_Select_WallTime}, we suspect the few percentage reduction in memory usage is not worth the massive decrease in rank query performance and the slight decrease in select query performance.


\subsubsection{Improvement of using precomputed values}
We have found that our UnalignedNaive precomputed tree using block size = 0.5 page size is the fastest for rank queries for input size $n = 10^8$ characters, and about as fast as the Naive tree for select queries.
We therefore consider UnalignedNaive at block size 0.5 page size to be our best wavelet tree implementation so far, even though it uses more memory than SimpleNaive or the Preallocated variants.

We have compared the running time of 1000 rank and select queries for UnalignedNaive and SimpleNaive in Figure~\ref{fig:PrecomputedRankBlockSize_vsNaiveInteger}.
The wall time for rank queries on the UnalignedNaive tree is only 0.27\,\% of the wall time of rank queries on the SimpleNaive tree as can be seen in Figure~\ref{fig:PrecomputedRankBlockSize_vsNaiveInteger_Rank}.
The wall time for select queries on the UnalignedNaive tree is only 0.73\,\% of the wall time of select queries on the SimpleNaive tree as can be seen in Figure~\ref{fig:PrecomputedRankBlockSize_vsNaiveInteger_Select}.

We can see that it is a great improvement to use precomputed rank values in both rank and select queries, with more gain for rank.

\subsubsection{The Dependence of Optimal Block Size on Input Size}
\label{sec:expOptimalBlockSizeDependsOnInputSize}
We have run our experiment for rank queries on the UnalignedNaive wavelet tree using varying block sizes, for 4 different input sizes, $n = [10^5, 10^6, 10^7, 10^8]$, to show that the optimal block size $b$ depends on $n$, and to show that the optimal block size $b$ is less that the theoretically optimal $b$ for the root bitmap, when using a fixed block size throughout the wavelet tree.
We did not run this experiment for select queries, because of problems choosing appropriate query parameters for the smaller input sizes, and because we felt little additional valuable information would be gained.

In Figure~\ref{fig:PrecomputedRankBlockSizeVaryN_Rank_WallTime} we have plotted the wall times for various block sizes for four different values of $n$.
The x-axis is logarithmic as we tested for block size values as powers of 2, to reach a wide range of block sizes without having to run hundreds of tests yet still have several tests at low values.

For all four graphs in Figure~\ref{fig:PrecomputedRankBlockSizeVaryN_Rank_WallTime} we can see that the performance at the theoretical optimal block size for the root bitmap  at $\sqrt{n}$ is good, and close to the minimum in wall time.
Therefore, the wavelet tree using precomputed rank values in blocks should compute its block size based in the size of the input for better performance.
A further improvement might be to compute the block size for each node of the tree individually.
We have, however, not done this in our implementation, because of some problems with the implementation and time constraints.

Surprisingly, we find that the minimum in wall time is at a block size not below the theoretical optimal block size, but instead slightly above it if anything at all.
We had expected that the other bitmaps, smaller than the root bitmap and therefore having smaller optimal block sizes, would have skewed the optimal block size downward.
The difference in performance between the theoretically optimal block size of $\sqrt{n}$ and the measured optimal block size is small, especially for the larger input sizes, and using a block size of $\sqrt{n}$ would be a sufficient optimization in most cases.



\newgeometry{left=2cm,right=2cm, top=2cm, bottom=3cm}

\begin{figure}\tiny

\begin{subfigure}{0.48\textwidth}
	\input{PrecomputedRankBlockSizeVaryN_5_Rank_WallTime}
	\caption{$n = 10^5$}
	\label{fig:PrecomputedRankBlockSizeVaryN_5_Rank_WallTime}
\end{subfigure}
\hfill
\begin{subfigure}{0.48\textwidth}
	\input{PrecomputedRankBlockSizeVaryN_6_Rank_WallTime}
	\caption{$n = 10^6$}
	\label{fig:PrecomputedRankBlockSizeVaryN_6_Rank_WallTime}
\end{subfigure}

\begin{subfigure}{0.48\textwidth}
	\input{PrecomputedRankBlockSizeVaryN_7_Rank_WallTime}
	\caption{$n = 10^7$}
	\label{fig:PrecomputedRankBlockSizeVaryN_7_Rank_WallTime}
\end{subfigure}
\hfill	
\begin{subfigure}{0.48\textwidth}
	\input{PrecomputedRankBlockSizeVaryN_8_Rank_WallTime}
	\caption{$n = 10^8$}
	\label{fig:PrecomputedRankBlockSizeVaryN_8_Rank_WallTime}
\end{subfigure}
\caption{Walltimes for varying block sizes are four different input sizes. The vertical blue line is at $\sqrt{n}$, the theoretical optimal block size for the root bitmap. Notice that the x-axis is logarithmic.}
\label{fig:PrecomputedRankBlockSizeVaryN_Rank_WallTime}
\end{figure}

\restoregeometry