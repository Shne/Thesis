\section{Precomputing Binary Rank in Blocks}
During our experiment of skewing the tree, we concluded that most of the work during queries is performed inside each node, calculating the binary rank of each bitmap.
Rank queries are simply summing up results of popcounting each word, and we considered whether precomputing these sums for blocks spanning several words, covering the bitmaps could improve the query times.
When a block doesn't line up with the position a rank query is for, the algorithm can simply fall back to doing popcounting of either the remaining uncovered words or the excess covered words, whichever has fewer words.
See Section~\ref{sec:rankQueriesWithPrecomputedRanksEdgeCases} for more explanation of this.

We can precompute the rank values easily and cheaply by doing so as we build the tree where we already need to compute and store each individual bit of the bitmap.
We simply increment a counter for the corresponding block each time we set a bit to 1 in the bitmap.

The size of the precomputed blocks is a new variable that could have influence on the running time and memory usage.
Some advantages of larger blocks is less memory usage and fewer precomputed value lookups for the same part of the bitmap.
Some advantages of smaller blocks are that they can cover more precisely the part of the bitmap that is relevant to the query, leading to fewer calls to popcount.
We will experiment to find an optimal value for our system.

To further reduce the space used by the precomputed values, we considered concatenating all the bitmaps into one big bitmap as that would eliminate the many cases where the length of a bitmap doesn't align with the block size and the last precomputed value for that bitmap will therefore not cover an entire block, leading to more precomputed values than minimally needed to cover all the bitmaps.

We also considered the cost of TLB misses and how ensuring that entire pages are skipped as often as possible might increase the query performance. We try to achieve this by page-aligning the blocks.

We will test and compare the Rank and Select running times and memory usage of three wavelet trees using precomputed rank values for blocks: one using concatenated bitmaps and aligned blocks, one only using aligned blocks and one using neither.




\subsection{Concatenating the Bitmaps}
We allocate the bitmaps as one giant bitmap the size of the maximum possible size required to store all the bitmaps for all the nodes. The sum of the size of all bitmaps on one layer of the tree can at most be $n$ and we can at most have $h$ layers, so the maximum size becomes
\[n \cdot h\]
where $n$ is the number of characters in the string and $h$ is the max height of the binary wavelet tree which is
\[ h = log(\sigma) \]
We then store an offset and a size for the bitmap in each node, so we can index into the giant bitmap and access the bits corresponding to the node.
This should also cause an decrease in memory usage as the offset and size are stored in an unsigned long and unsigned int respectively, taking a total of $64+32=96$ bits per node, where each individual bitmap requires storage of at least a pointer to it, a point to where its internal array starts and a pointer to where it ends, taking up $3*64=192$ bits per node.
Additionally, when using an individual bitmap for each node, they would have been word-aligned, and the bits between the last used bit and the end of the last used word would have gone unused and so, wasted.

We also allocate a vector for holding the precomputed block values of size
\[ \frac{BitmapSize}{BlockSize} \]
Instead of storing a pointer to the bitmap and precomputed values vector in each node, we store them once for the whole tree and then pass them down through the query methods when the tree is queried.

We use integer division of the bitmap offset and the block size to index into said vector. It is an efficient and simple way to precompute the rank values of blocks of fixed size of the bitmap.


\subsubsection{Edge Cases}
\label{sec:rankQueriesWithPrecomputedRanksEdgeCases}
The rank of a string can be expressed as the sum of the rank of any number and various sizes of subparts as long as they together perfectly cover the string and don't overlap.
Because the blocks must perfectly cover the string and not overlap, and the bitmaps of each node are not of same size, nor multiples of some single value, we have a problem if we want to use uniformly sized and distributed blocks.
The problem exists at the boundary between bitmaps, where the precomputed rank value will be the sum of the rank of the end of the first bitmaps and the rank of the beginning of the second bitmap.

Looking at a single bitmap for a node, there is an edge case for the first and last part of the bitmap, because they don't fill an entire block, so we can't simply use the corresponding precomputed value as-is.
Instead, we can compute the rank of the part of the block that our part doesn't fill and subtract that from the precomputed rank value.
This is only worth doing when our part fills more than half a block, because then the other part is smaller than half a block and therefore quicker to compute.
Figure ~\ref{fig:PrecomputePopcountBlock} illustrates this.

\figureBegin
\caption{Rank value of a part of a bitmap is equal to the precomputed value for the block minus the rank of the other remaining part.}
\label{fig:PrecomputePopcountBlock}
\includegraphics[width=\textwidth]{PrecomputePopcountBlock.pdf}
\figureEnd

\textbf{[TODO: pseudocode of how precomputed values are used]}





\subsubsection{Page-aligning the Blocks}
Translation Lookaside Buffer misses are expensive and to avoid those, we can try to reduce the number of pages we load.
Using concatenated bitmaps and the precomputed vector of ranks, we only need to load pages of the bitmap at the beginning and end of each node's bitmap, to compute the popcount binary rank directly on the bitmap, and only within one block at each end.
If the blocks are not aligned with the memory pages, then, even if the block size is less than a page, it might span more than one page and thus more than one page of memory must be loaded into the TLB.
More precisely, we might at most load $2 \cdot \left( \lceil\frac{blockSize}{pageSize}\rceil +1 \right)$ pages to do the popcount binary rank computation at the beginning and end of each node.

If we page-align the blocks, and use block sizes divisible by the page size or that the page is divisible by, the extra $+1$ will disappear, because a block can no longer span more pages than its size is a multiple of the page size.
This means we can ensure that at most $2 \cdot \lceil\frac{blockSize}{pageSize}\rceil$ memory pages of the bitmap are loaded for each node by page-aligning the blocks.
With an alphabet size of $2^{16}$ this amounts to saving 16 page loads per query.



\subsection{Select Queries with Precomputed Ranks}
Select queries, although they don't return a rank value, can still utilize the precomputed rank values to skip much computation directly on the bitmap by iterating though them.
Partway in a select query, if the sum of the occurrences found so far and the rank value of the current block of the bitmap is more than the queried-for occurrence, we can add that rank value to our occurrences seen so far and skip ahead to the next block and perform the same test.
If the sum is less than the queried-for occurrence, we know we will find the occurrence in the current block and switch back to the previous method of select querying using popcount starting at this block.

\textbf{[TODO: pseudocode]}


\subsubsection{Edge Cases}
As with rank queries, there are edge cases at the beginning and end of each bitmap.
However, in this case, the edge case at the end is easily handled as the test of sum of rank and occurrence-so-far should fail, sending the algorithm into the block with the previous select query method, finding the occurrence with no problem and no specific handling of the edge case.
This is assuming the input occurrence parameter is valid, that is, at least that many occurrences of that character is in the original input string.

For the other case, at the beginning of the bitmap, we do almost exactly the same as in the case for the rank query.
In fact, we use a rank query to calculate the rank of the first part of the bitmap, using the trick of subtracting from the precomputed value if larger than half a block size, to figure out whether the occurrence is in the first part of the bitmap, and therefore whether we should skip it or not.

\paragraph{Using Rank Queries in Select Queries}~\\
We would like to analyse whether it is worth using a rank query to find out whether we should do a select query on the first part of the bitmap, as the rank query is purely extra work in the cases where the occurrence is in that part of the bitmap.

We will assume an equal number of occurrences in the string of each character in the alphabet, a uniform distribution of each character in the string, and an equal probability of each valid parameter for the select query.
A valid character parameter is one that exists in the input string and a valid occurrence parameter is an integer above 0 and below or equal to the number occurrences appearing in the input string.

The rank query is a computation of worst-case cost $\frac{blockSize}{2}$ because it will at most popcount half of the block, because we utilize the precomputed rank value when advantageous.
The popcount select query into the first partial block is an operation of word-case cost $blockSize$ because it can at most popcount the entire block.
So, in the worst case, when the sought-after occurrence is in the first partial block of the bitmap, $\frac{blockSize}{2}$ work is wasted, yet when the occurrence is elsewhere, $blockSize$ work is saved.
This means that the boundary between where using the rank query becomes a gain or a loss in performance is where the ratio between times the occurrence is to be found in and outside the first partial block of the bitmap is $\nicefrac{2}{3}$.
That is, when the occurrence is to be found outside the first partial block of the bitmap less than two-thirds of the time, using the rank query first to see if that partial block should be select queried is a gain in performance, when considering the worst-case query time for both queries.

Though, this is giving the select query a disadvantage in the analysis, as, even when the partial block is close to a block in size, it might terminate early if it found the sought-after occurrence of the character early in the partial block.

It is not entirely clear whether it is an advantage to use the binary rank call, or it would be better to use popcount BinarySelect on the first partial block at all times.
We will test this in section~\ref{sec:experimentPrecomputedBlocksRankInSelect}



\subsection{Extra Space Used by Precomputed Values}
Since each precomputed value cannot exceed the block size in bits, assuming we don't use block sizes exceeding $2^{16} = 65536$ bits, or $\frac{2^{16}}{8} = 8192$ bytes, we can store them in 16-bit unsigned integers, called \texttt{unsigned short int} in C++ on our machines.
Since the page size on our machines are 4096 bytes we should not use a block size larger than $\frac{8192}{4096} = 2$ pages if we want to use 16-bit unsigned integers.

In Section[TODO:reference] we find that the optimal block size is ? bits = ? bytes = ? pages.

Assuming we then store the precomputed values as 16-bit unsigned short integers it will only consume an extra 16 bits or 2 bytes per block of which there are $\frac{BitmapSize}{BlockSize}$.
This means, assuming ? bytes per block, a relative extra space consumption of
\[ \frac{ 2 \cdot \frac{BitmapSize}{BlockSize} }{BitmapSize} = \frac{2}{BlockSize} = \frac{2}{??blockSize??} = 0.XXXXXXXXX = 0.XXX\% \]
of the bitmaps.
Which is even less when considering the total space used including the nodes, and would be less for larger block sizes.


\subsection{Experiments}
We will test and compare the Rank and Select running times and memory usage of three wavelet trees using precomputed rank values for blocks: one using concatenated bitmaps and aligned blocks, one only using aligned blocks and one using neither.

\subsubsection{Query Running Time for Bitmap with Precomputed Blocks for different Block Sizes}

\figureBegin
\caption{Rank queries on wavelet trees with precomputed ranks of varying block sizes}
\label{fig:PrecomputedRankBlockSize_Rank_WallTime}
\input{PrecomputedRankBlockSize_Rank_WallTime}
\figureEnd

\figureBegin
\caption{Select queries on wavelet trees with precomputed ranks of varying block sizes}
\label{fig:PrecomputedRankBlockSize_Select_WallTime}
\input{PrecomputedRankBlockSize_Select_WallTime}
\figureEnd




\subsubsection{Memory Usage of Concatenated vs. Individual Bitmaps}

\figureBegin
\caption{Difference in Memory Usage of wavelet trees with precomputed ranks of varying block size}
\label{fig:PrecomputedRankBlockSize_MemoryUsage}
\input{PrecomputedRankBlockSize_MemoryUsage}
\figureEnd