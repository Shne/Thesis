\section{Precomputing Binary Rank in Blocks}
During our experiment of skewing the tree, we concluded that most of the work during queries is performed inside each node, calculating the binary rank of each bitmap.
Rank queries are simply summing up results of popcounting each word, and we considered whether precomputing these sums for blocks spanning several words, covering the bitmaps could improve the query times.
When a block doesn't line up with the position a rank query is for, the algorithm can simply fall back to doing popcounting of either the remaining uncovered words or the excess covered words, whichever has fewer words.
See Section~\ref{sec:rankQueriesWithPrecomputedRanksEdgeCases} for more explanation of this.

We can precompute the rank values easily and cheaply by doing so as we build the tree where we already need to compute and store each individual bit of the bitmap.
We simply increment a counter for the corresponding block each time we set a bit to 1 in the bitmap.

The size of the precomputed blocks is a new variable that could have influence on the running time and memory usage.
Some advantages of larger blocks is less memory usage and fewer precomputed value lookups for the same part of the bitmap.
Some advantages of smaller blocks are that they can cover more precisely the part of the bitmap that is relevant to the query, leading to fewer calls to popcount.
We will experiment to find an optimal value for our system.

To further reduce the space used by the precomputed values, we considered concatenating all the bitmaps into one big bitmap and keeping a single vector of precomputed rank values for blocks for the entire bitmap. 
That would eliminate the many cases where the length of a bitmap doesn't align with the block size and the last precomputed value for that bitmap will therefore not cover an entire block, leading to more precomputed values than minimally needed to cover all the bitmaps.

We also considered the cost of TLB misses and how ensuring that entire pages are skipped as often as possible might increase the query performance. We try to achieve this by page-aligning the blocks.

We will test and compare the Rank and Select running times and memory usage of four wavelet trees using precomputed rank values for blocks: one using concatenated bitmaps and aligned blocks, one using concatenated bitmaps but not aligned blocks, one only using aligned blocks and one using neither.





\subsection{Concatenating the Bitmaps}
We allocate the bitmaps as one giant bitmap the size of the maximum possible size required to store all the bitmaps for all the nodes. The sum of the size of all bitmaps on one layer of the tree can at most be $n$ and we can at most have $h$ layers, so the maximum size becomes
\[n \cdot h\]
where $n$ is the number of characters in the string and $h$ is the max height of the binary wavelet tree which is
\[ h = log(\sigma) \]
Luckily for us, memory allocation in Linux doesn't actually take up space, only when it is accessed will the actual physical memory be used.
So over-allocating the bitmap should not take up any more space than will actually be needed.

We then store an offset and a size for the bitmap in each node, so we can index into the giant bitmap and access the bits corresponding to the node.
This should also cause an decrease in memory usage as the offset and size are stored in an unsigned long and unsigned int respectively, taking a total of $64+32=96$ bits per node, where each individual bitmap requires storage of at least a pointer to it, a point to where its internal array starts and a pointer to where it ends, taking up $3*64=192$ bits per node.
Additionally, when using an individual bitmap for each node, they would have been word-aligned, and the bits between the last used bit and the end of the last used word would have gone unused and so, wasted.

We also allocate a vector for holding the precomputed block values of size
\[ \frac{BitmapSize}{BlockSize} \]
Instead of storing a pointer to the bitmap and precomputed values vector in each node, we store them once for the whole tree and then pass them down through the query methods when the tree is queried.

We use integer division of the bitmap offset and the block size to index into said vector. It is an efficient and simple way to precompute the rank values of blocks of fixed size of the bitmap.


\subsubsection{Edge Cases}
\label{sec:rankQueriesWithPrecomputedRanksEdgeCases}
The rank of a string can be expressed as the sum of the rank of any number and various sizes of subparts as long as they together perfectly cover the string and don't overlap.
Because the blocks must perfectly cover the string and not overlap, and the bitmaps of each node are not of same size, nor multiples of some single value, we have a problem if we want to use uniformly sized and distributed blocks.
The problem exists at the boundary between bitmaps, where the precomputed rank value will be the sum of the rank of the end of the first bitmaps and the rank of the beginning of the second bitmap.

Looking at a single bitmap for a node, there is an edge case for the first and last part of the bitmap, because they don't fill an entire block, so we can't simply use the corresponding precomputed value as-is.
Instead, we can compute the rank of the part of the block that our part doesn't fill and subtract that from the precomputed rank value.
This is only worth doing when our part fills more than half a block, because then the other part is smaller than half a block and therefore quicker to compute.
Figure ~\ref{fig:PrecomputePopcountBlock} illustrates this.



\figureBegin
\caption{Rank value of a part of a bitmap is equal to the precomputed value for the block minus the rank of the other remaining part.}
\label{fig:PrecomputePopcountBlock}
\includegraphics[width=\textwidth]{PrecomputePopcountBlock.pdf}
\figureEnd




%\begin{algorithm}
%\caption{Rank Query for PreallocatedPrecomputed}
%\label{dsfdsfsd}
%\begin{algorithmic}
%\State \Comment{\textproc{PopcountRank} is a function that calculates the rank of a given part of a bitmap using the popcount cpu instruction}
%\Function{Rank}{Position, Bitmap, PrecomputedRanks}
%\If{Position $<$ BlockSize/2}
%\State \Return \Call{PopcountRank}{This bitmap}
%\EndIf
%\If{Position $<$ BlockSize \textit{and} Position $<$ LengthToNextBlock}
%\State PreRank $\gets$ \Call{PopcountRank}{Part of block before this bitmap}
%\State PostRank $\gets$ \Call{PopcountRank}{Part of block after this bitmap}
%\State BlockIndex $\gets$ \textit{Index of the block this bitmap is in}
%\State \Return PrecomputedRanks[BlockIndex] - PreRank - PostRank
%\EndIf
%\If{
%\EndFunction
%\end{algorithmic}
%\end{algorithm}




\subsubsection{Page-aligning the Blocks}
Translation Lookaside Buffer misses are expensive and to avoid those, we can try to reduce the number of pages we load.
Using concatenated bitmaps and the precomputed vector of ranks, we only need to load pages of the bitmap at the beginning and end of each node's bitmap, to compute the popcount binary rank directly on the bitmap, and only within one block at each end.
If the blocks are not aligned with the memory pages, then, even if the block size is less than a page, it might span more than one page and thus more than one page of memory must be loaded into the TLB.
More precisely, we might at most load $2 \cdot \left( \lceil\frac{blockSize}{pageSize}\rceil +1 \right)$ pages to do the popcount binary rank computation at the beginning and end of each node.

If we page-align the blocks, and use block sizes divisible by the page size or that the page is divisible by, the extra $+1$ will disappear, because a block can no longer span more pages than its size is a multiple of the page size.
This means we can ensure that at most $2 \cdot \lceil\frac{blockSize}{pageSize}\rceil$ memory pages of the bitmap are loaded for each node by page-aligning the blocks.
With an alphabet size of $2^{16}$ this amounts to saving up to 16 page loads per query.

While we expect that this will save some expensive TLB misses it also has some drawbacks, especially when not using concatenated bitmaps.
For a wavelet tree not using concatenated bitmaps, using page-aligned blocks will cause the first precomputed value of each non-page-aligned bitmap to not cover an entire block, increasing the number of precomputed values needed to cover the bitmap as well as additional computations to calculate exactly which part of the bitmap it covers.
For the wavelet trees using concatenated bitmaps, these computations are needed regardless of block alignment, as the blocks are already not aligned with the individual bitmaps, and so we believe it can only be an improvement to page-align the blocks in this case.
We test this by implementing a variation of the wavelet tree using concatenated bitmaps that doesn't page-align the blocks.

We will test whether it increases the performance of rank and select queries when not using concatenated bitmaps in section~\ref{sec:queryRunTimePrecomputedBlockSizes}.


\subsection{Select Queries with Precomputed Ranks}
Select queries, although they don't return a rank value, can still utilize the precomputed rank values to skip much computation directly on the bitmap by iterating though them.
Partway in a select query, if the sum of the occurrences found so far and the rank value of the current block of the bitmap is more than the queried-for occurrence, we can add that rank value to our occurrences seen so far and skip ahead to the next block and perform the same test.
If the sum is less than the queried-for occurrence, we know we will find the occurrence in the current block and use our previously implemented method of calculating select using popcount, starting at this block.


\subsubsection{Edge Cases}
As with rank queries, there are edge cases at the beginning and end of each bitmap.
However, in this case, the edge case at the end is easily handled as the test of sum of rank and occurrence-so-far should fail, sending the algorithm into the block with the previous select query method, finding the occurrence with no problem and no specific handling of the edge case.
This is assuming the input occurrence parameter is valid, that is, at least that many occurrences of that character is in the original input string.

For the other case, at the beginning of the bitmap, we do almost exactly the same as in the case for the rank query.
In fact, we use a rank query to calculate the rank of the first part of the bitmap, using the trick of subtracting from the precomputed value if larger than half a block size, to figure out whether the occurrence is in the first part of the bitmap, and therefore whether we should skip it or not.

\paragraph{Using Rank Queries in Select Queries}~\\
We would like to analyse whether it is worth using a rank query to find out whether we should do a select query on the first part of the bitmap, as the rank query is purely extra work in the cases where the occurrence is in that part of the bitmap.

We will assume an equal number of occurrences in the string of each character in the alphabet, a uniform distribution of each character in the string, and an equal probability of each valid parameter for the select query.
A valid character parameter is one that exists in the input string and a valid occurrence parameter is an integer above 0 and below or equal to the number occurrences appearing in the input string.

The rank query is a computation of worst-case cost $\frac{blockSize}{2}$ because it will at most popcount half of the block, because we utilize the precomputed rank value when advantageous.
The popcount select query into the first partial block is an operation of worst-case cost $blockSize$ because it can at most popcount the entire block.
So, in the worst case, when the sought-after occurrence is in the first partial block of the bitmap, $\frac{blockSize}{2}$ work is wasted, yet when the occurrence is elsewhere, $blockSize$ work is saved.
This means that the boundary between where using the rank query becomes a gain or a loss in performance is where the ratio between times the occurrence is to be found in and outside the first partial block of the bitmap is $\nicefrac{2}{3}$.
That is, when considering the worst-case query time for both queries, if the occurrence is to be found outside the first partial block of the bitmap more than one-third of the time, using the rank query first to see if that partial block should be select queried is a gain in performance.

Though, this is giving the select query a disadvantage in the analysis, as, even when the partial block is close to a block in size, it might terminate early if it found the sought-after occurrence of the character early in the partial block.

It is not entirely clear whether it is an advantage to use the binary rank call, or it would be better to use popcount BinarySelect on the first partial block at all times.
We will test this in section~\ref{sec:experimentPrecomputedBlocksRankInSelect}
\textbf{[TODO]}



\subsection{Extra Space Used by Precomputed Values}
Since each precomputed value cannot exceed the block size in bits, assuming we don't use block sizes exceeding $2^{16} = 65536$ bits, or $\frac{2^{16}}{8} = 8192$ bytes, we can store them in 16-bit unsigned integers, called \texttt{unsigned short int} in C++ on our machines.
Since the page size on our machines are 4096 bytes we should not use a block size larger than $\frac{8192}{4096} = 2$ pages if we want to use 16-bit unsigned integers.

In Section~\ref{sec:queryRunTimePrecomputedBlockSizes} we find that the optimal block size is 16384 bits = 2048 bytes = $\nicefrac{1}{2}$ page.

Assuming we then store the precomputed values as 16-bit unsigned short integers it will only consume an extra 16 bits or 2 bytes per block of which there are $\frac{BitmapSize}{BlockSize}$ when we concatenate the bitmaps.
This means, assuming a block size of 2048 bytes, a relative extra space consumption of
\[ \frac{ 2 \cdot \frac{BitmapSize}{BlockSize} }{BitmapSize} = \frac{2}{BlockSize} = \frac{2}{2048} = 0.0009765625 = 0.098\% \]
of the bitmaps.
Which is even less when considering the total space used including the nodes.

When we don't concatenate the bitmaps there is a higher space consumption by the precomputed values, as each precomputed value don't cover an entire block and more precomputed values is therefore needed to cover all the bitmaps.
When the blocks are not page-aligned, each node has potentially one precomputed value not covering an entire block at the end of its bitmap.
When blocks are page-aligned, there is another precomputed value potentially not covering an entire block at the beginning of each bitmap.
The extra space consumption by the precomputed values when not concatenating the bitmaps is therefore limited proportionally to the number of nodes, which is at most $2 \sigma - 1$, making it limited proportionally by the alphabet size.

We expect to see a difference in memory usage between using concatenated bitmaps and non-concatenated bitmaps as well as between using page-aligned and non-page-aligned blocks.
However, we expect most of the difference to come from the space used by the bitmaps themselves, and therefore a noticeable difference between the data structure concatenating the bitmaps and the others, with a little difference between using page-aligned and non-page-aligned blocks, with the one using page-aligned blocks using most memory.



\subsection{Experiments}
We will test and compare the Rank and Select running times of three wavelet trees using precomputed rank values for varying block sizes: one using concatenated bitmaps and aligned blocks named Preallocated, one using concatenated bitmaps but not aligned blocks named UnalignedPreallocated, one using aligned blocks but not concatenated bitmaps, called Naive,
and one using unaligned blocks and non-concatenated bitmaps called UnalignedNaive.
In table-form:\\
\begin{tabular}{|lcc|}
\hline
Name						& Concatenated Bitmaps	& Page-aligned Blocks	\\ \hline
Preallocated				& yes					& yes					\\ \hline
UnalignedPreallocated	& yes					& no						\\ \hline
Naive					& no						& yes					\\ \hline
UnalignedNaive			& no						& no						\\ \hline
\end{tabular}\\
Later, we will compare the memory usage and query times with the non-precomputed version, called NaiveInteger.

\subsubsection{Query Running Time for Bitmap with Precomputed Blocks for different Block Sizes}
\label{sec:queryRunTimePrecomputedBlockSizes}

\paragraph{Rank Queries}~\\
In Figure~\ref{fig:PrecomputedRankBlockSize_Rank_WallTime} we have plotted the rank query wall time in $\mu s$ for the wavelet trees using precomputed rank values.

We see that both concatenating the bitmaps and page-aligning the blocks is consistently slower, which was expected for concatenating the bitmaps, but not entirely so for page-aligning the blocks.
Preallocated is on average about 6.22\% slower than UnalignedPreallocated, so page-aligning the blocks when using concatenated bitmaps is a 6.22\% performance hit.
Preallocated is on average about 13.85\% slower than Naive, meaning concatenating the bitmaps when using page-aligned blocks is a 13.85\% performance hit.

UnalignedNaive is the surprising outlier in this graph with a much lower wall time, especially for smaller block sizes.
At block size = 0.5 page size, where UnalignedNaive is fastest, it only takes 65.58\% of the time that Naive does at block size = 1 page size, where Naive is fastest.

Much of the increased running time of Rank queries on the two Preallocated wavelet trees can likely be explained by the increased amount of instructions needed to calculate the rank of the first part inside the first block of each bitmap because the precomputed value includes part of the preceding bitmap, as well as the ineffectiveness of utilizing the precomputed rank values for small bitmaps, for the same reason.
However, by examining Figure~\ref{fig:PrecomputedRankBlockSize_Rank_BranchMiss} and \ref{fig:PrecomputedRankBlockSize_Rank_BranchMissRate} we can conclude that part of the increase, at least in comparison with NaiveInteger, is also due to the increased number of branch misses.

In Figure~\ref{fig:PrecomputedRankBlockSize_Rank_BranchMissRate} we also see a surprising increase in the branch misprediction rate of UnalignedNaive for smaller block sizes.
When looking at Figure~\ref{fig:PrecomputedRankBlockSize_Rank_BranchMiss} we see that it follows somewhat the same increase in amount for smaller block sizes as the others, making us conclude that UnalignedNaive must simply run less branching code than the others for smaller block sizes.
This is likely a result of UnalignedNaive not having to do any calculations to figure out which part and how much of the bitmap the first precomputed value covers, as it always covers an entire block of the bitmap and is perfectly aligned with the start of the bitmap instead of a memory page.

Looking at Figure~\ref{fig:PrecomputedRankBlockSize_Rank_TLB} we can see that our expressed goal of reducing TLB misses when using page-aligned blocks is only barely achieved when not using concatenated bitmaps, and not really when they are.
On the other hand, we see that TLB misses are reduced to about 29.80\% when using concatenated bitmaps in the page-aligned version and to about 26.39\% when not page-aligning the blocks.
We also notice that the two trees using non-concatenated bitmaps, Naive and UnalignedNaive has a noticeable drop in TLB misses at a block size of 1 page size.

Looking at the level 1 data cache misses, it is clear that there are fewest cache misses at block size = 0.5 page size, which is also likely the main reason that UnalignedNaive exhibits the best running time at at that block size.
The strange thing is that the others also seems to have the best level data cache performance for rank queries at half a page per block, while the wall time is best at a full page per block.
Something else must be impacting the performance enough to skew the result to 1 page block size.
But when we look at the other figures, we don't see any significant improvement for the others at 1 page per block versus half a page per block.
The level 2 data cache miss rate plotted in Figure~\ref{fig:PrecomputedRankBlockSize_Rank_L2CacheMissRate} is worst for block size = 0.5 page size, but looking at the raw cache misses in Figure~\ref{fig:PrecomputedRankBlockSize_Rank_L2CacheMiss} we generally see more cache misses for higher block sizes, meaning that the level 2 data cache miss rate can't explain the better performance at block size = 1 page size for every tree other than UnalignedNaive.

Looking at Level 2 data cache misses in Figure~\ref{fig:PrecomputedRankBlockSize_Rank_L2CacheMiss} and the level 2 data cache miss rate in Figure~\ref{fig:PrecomputedRankBlockSize_Rank_L2CacheMissRate}, we don't see much difference between the different tree implementations, except that the Naive tree has a noticeable drop in both the raw amount and the rate at block size = 1 page size, just like it had for TLB misses.
It is interesting, though, that they all have a somewhat high, $0.35-0.4$, level 2 data cache miss rate around 0.5-0.75 page per block, where they all have good wall time performance.
This is likely because of the good level 1 data cache performance.
To explain what we mean, let's assume there is a fixed amount of operations accessing memory that is hard for the cache to have prefetched or otherwise loaded beforehand, independent of the block size.
E.g. when the queried-for position is reached in a bitmap and the rank algorithm jumps to a different node and a different bitmap.
When the first cache level can handle more and more of the 'easy' memory operations, fewer of those are left to be handled by the second cache level, yet the same amount of 'hard' memory accesses are hitting the second cache level, and so the miss rate will increase for the second cache level, while the total amount of cache misses for level 2 could easily decrease, as the first level handles more accesses.


\paragraph{Select Queries}~\\\\
In Figure~\ref{fig:PrecomputedRankBlockSize_Select_WallTime} we see the wall time of 1000 Select queries for the different wavelet trees using precomputed rank values.
We can see that all have the best running time at half a page per block, though some of them have about the same speed at 0.75 page per block.
The main reason for this is likely to be found in the level 1 data cache performance data again, which can be found in Figure~\ref{fig:PrecomputedRankBlockSize_Select_L1CacheMiss} as we can see that it displays better level 1 data cache performance at lower values, except for Naive, which has its best performance at 0.5 page per block below which the cache misses increase again.

In much the same way as for rank queries, the branch mispredictions as ploted in Figure~\ref{fig:PrecomputedRankBlockSize_Select_BranchMiss} decrease as block size increases, but looking at Figure~\ref{fig:PrecomputedRankBlockSize_Select_BranchMissRate} we can see that the branch misprediction rate is highest at about 0.75 page per block and much smaller at smaller block sizes, meaning much more branching code, that is correctly predicted, must be executed at block sizes below 0.75 pages per block.

The amount of TLB misses across block sizes seen in Figure~\ref{fig:PrecomputedRankBlockSize_Select_TLB} is very similar to the TLB misses for rank queries as we again see that the biggest reduction in TLB misses comes from using concatenated bitmaps and that page-aligning the blocks only makes little difference when using non-concatenated bitmaps and no difference when using concatenated bitmaps.

Just as with rank queries, we see higher level 2 data cache miss rate at lower block sizes in Figure~\ref{fig:PrecomputedRankBlockSize_Select_L2CacheMissRate}, and again the level 1 data cache misses likely the cause, but unlike for rank queries, the amount of level 2 data cache misses are not lower for smaller block sizes, in fact they are higher than at 1 page per block, where each type of wavelet tree has its minimum, with Naive having the largest drop there.
\textbf{[TODO: speculate why]}



\newgeometry{left=2cm,right=2cm, top=2cm, bottom=3cm}
\begin{figure}
	\input{PrecomputedRankBlockSize_Rank_WallTime}
	\caption{Wall Time of Rank queries on Wavelet Trees with Precomputed Rank Values for varying block sizes}
	\label{fig:PrecomputedRankBlockSize_Rank_WallTime}
\end{figure}

\begin{figure}
\input{PrecomputedRankBlockSize_Select_WallTime}
\caption{Wall Time of Select queries on wavelet trees with precomputed ranks of varying block sizes}
\label{fig:PrecomputedRankBlockSize_Select_WallTime}
\end{figure}


\begin{figure}\tiny
\begin{subfigure}{0.48\textwidth}
	\input{PrecomputedRankBlockSize_Rank_BranchMiss}
	\caption{Branch Mispredictions}
	\label{fig:PrecomputedRankBlockSize_Rank_BranchMiss}
\end{subfigure}
\hfill
\begin{subfigure}{0.48\textwidth}
	\input{PrecomputedRankBlockSize_Rank_BranchMissRate}
	\caption{Branch Misprediction Rate}
	\label{fig:PrecomputedRankBlockSize_Rank_BranchMissRate}
\end{subfigure}
\begin{subfigure}{0.48\textwidth}
	\input{PrecomputedRankBlockSize_Rank_TLB}
	\caption{TLB Misses}
	\label{fig:PrecomputedRankBlockSize_Rank_TLB}
\end{subfigure}
\hfill
\begin{subfigure}{0.48\textwidth}
	\input{PrecomputedRankBlockSize_Rank_L1CacheMiss}
	\caption{Level 1 Data Cache Misses}
	\label{fig:PrecomputedRankBlockSize_Rank_L1CacheMiss}
\end{subfigure}
\begin{subfigure}{0.48\textwidth}
	\input{PrecomputedRankBlockSize_Rank_L2CacheMiss}
	\caption{Level 2 Data Cache Misses}
	\label{fig:PrecomputedRankBlockSize_Rank_L2CacheMiss}
\end{subfigure}
\hfill
\begin{subfigure}{0.48\textwidth}
	\input{PrecomputedRankBlockSize_Rank_L2CacheMissRate}
	\caption{Level 2 Data Cache Miss Rate}
	\label{fig:PrecomputedRankBlockSize_Rank_L2CacheMissRate}
\end{subfigure}
\caption{Various measurements of Rank queries on Wavelet Trees with Precomputed Rank Values for varying block sizes}
\label{fig:PrecomputedRankBlockSize_Rank}
\end{figure}

%\input{PrecomputedRankBlockSize_Rank_TotalCycles}
%\caption{Total Cycles of Rank queries on wavelet trees with precomputed ranks of varying block sizes}
%\label{fig:PrecomputedRankBlockSize_Rank_TotalCycles}

%\caption{Level 3 Total Cache Misses for Rank queries on wavelet trees with precomputed ranks of varying block sizes}
%\label{fig:PrecomputedRankBlockSize_Rank_L3CacheMiss}
%\input{PrecomputedRankBlockSize_Rank_L3CacheMiss}



\begin{figure}\tiny
\begin{subfigure}{0.48\textwidth}
	\input{PrecomputedRankBlockSize_Select_BranchMiss}
	\caption{Branch Mispredictions. Notice the y-axis not starting at 0.}
	\label{fig:PrecomputedRankBlockSize_Select_BranchMiss}
\end{subfigure}
\hfill
\begin{subfigure}{0.48\textwidth}
	\input{PrecomputedRankBlockSize_Select_BranchMissRate}
	\caption{Branch Misprediction Rate}
	\label{fig:PrecomputedRankBlockSize_Select_BranchMissRate}
\end{subfigure}
\begin{subfigure}{0.48\textwidth}
	\input{PrecomputedRankBlockSize_Select_TLB}
	\caption{TLB Misses}
	\label{fig:PrecomputedRankBlockSize_Select_TLB}
\end{subfigure}
\hfill
\begin{subfigure}{0.48\textwidth}
	\input{PrecomputedRankBlockSize_Select_L1CacheMiss}
	\caption{Level 1 Data Cache Misses}
	\label{fig:PrecomputedRankBlockSize_Select_L1CacheMiss}
\end{subfigure}
\begin{subfigure}{0.48\textwidth}
	\input{PrecomputedRankBlockSize_Select_L2CacheMiss}
	\caption{Level 2 Data Cache Misses}
	\label{fig:PrecomputedRankBlockSize_Select_L2CacheMiss}
\end{subfigure}
\hfill
\begin{subfigure}{0.48\textwidth}
	\input{PrecomputedRankBlockSize_Select_L2CacheMissRate}
	\caption{Level 2 Data Cache Miss Rate for }
	\label{fig:PrecomputedRankBlockSize_Select_L2CacheMissRate}
\end{subfigure}
\caption{Various measurements of Select queries on Wavelet Trees with Precomputed Rank Values of varying block sizes}
\label{fig:PrecomputedRankBlockSize_Select}
%\hfill
%\begin{subfigure}{0.48\textwidth}
%	\input{PrecomputedRankBlockSize_Select_L3CacheMiss}
%	\caption{Level 3 Total Cache Misses for Select queries on wavelet trees with precomputed ranks of varying block sizes}
%	\label{fig:PrecomputedRankBlockSize_Select_L3CacheMiss}
%\end{subfigure}
\end{figure}

\begin{figure}\tiny
	\begin{subfigure}{0.5\textwidth}
		\input{PrecomputedRankBlockSize_Build_Memory}
		\caption{Reported by PAPI}
		\label{fig:PrecomputedRankBlockSize_PAPI}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.5\textwidth}
		\input{PrecomputedRankBlockSize_MemoryUsage}
		\caption{Reported by Massif.}
		\label{fig:PrecomputedRankBlockSize_Massif}
	\end{subfigure}
	\caption{Difference in Memory Usage of wavelet trees with precomputed ranks of varying block size. Notice the y-axis not starting at 0 and at different values.}
	\label{fig:PrecomputedRankBlockSize_Memory}
\end{figure}

\restoregeometry




\subsubsection{Memory Usage of Precomputed Rank Values}
We have used both Massif, the heap profiler included in the Valgrind Suite and PAPI to record the memory usage of our programs.
We focus on the output from Massif instead of PAPI, because the memory usage values we could extract using PAPI made little sense, with values indicating that the trees using precomputed values used less memory than the NaiveInteger tree with no extra precomputed values.
We have plotted the values from PAPI in Figure~\ref{fig:PrecomputedRankBlockSize_PAPI}.

We don't know why PAPI gives us these nonsensical results, but it might have something to do with Linux's aggressive caching and buffering.
We're not entirely sure whether \texttt{delete}ed memory might still be counted as used memory by PAPI because of such caching.
Massif, on the other hand, manually counts calls to such functions as \texttt{new} and \texttt{delete} and calculates the memory usage from these, making it unaffected by any caching scheme employed by the Linux kernel.
Massif is also designed for the purpose of profiling program memory usage, whereas PAPI is simply an API to access performance metrics from the underlying OS.

Massif only counts heap allocations and deallocations per default and not for stack, data, BSS or code segments.
We manually tell Massif to count our stacks as well and include that in our calculations.
The remaining uncounted data, BSS and code segments should not affect our results noticeably as those segments are usually very small.
But it might be the case that it is in fact these segments causing the strange results from PAPI, and that using concatenated bitmaps uses more memory than not and storing more information in the tree somehow uses less memory, sometimes, though we find this unlikely and choose to trust Massif's output more.

Massif outputs 'snapshots' of the memory usage taken at certain points in the code where it deems them useful to the user.
We have not simply used the snapshots produced automatically by Massif, but rather used the Client Request mechanism~\footnote{\url{http://valgrind.org/docs/manual/manual-core-adv.html}}of valgrind to send a \texttt{snapshot} command to the valgrind gdbserver, telling it to take a snapshot right after we've finished building the tree.
The values presented in Figure~\ref{fig:PrecomputedRankBlockSize_Massif} is thus calculated from such Massif snapshots taken right after the tree has completed building.

Looking at Figure~\ref{fig:PrecomputedRankBlockSize_Massif} we see that the memory usage of the trees using precomputed values but not concatenated bitmaps, Naive and UnalignedNaive use more memory than the tree not using precomputed values, NaiveInteger, but only about 0.5\%.
We also see that memory is indeed saved by using the concatenated bitmaps in Preallocated and UnalignedPreallocated, though only by about 1.5\% compared to the other trees using precomputed values.

Our attempts to reduce the memory usage by concatenating the bitmaps seems to have succeeded, but looking back at the running times for the rank and select queries in Figure~\ref{fig:PrecomputedRankBlockSize_Rank_WallTime} and \ref{fig:PrecomputedRankBlockSize_Select_WallTime}, we suspect the few percentage reduction in memory usage isn't worth the massive decrease in rank query performance and the slight decrease in select query performance.

Looking ahead, we have another improvement in mind for the non-concatenated trees that might very well improve the select query performance massively.


