\section{The Wavelet Tree}
The Wavelet Tree is a binary balanced tree structure, that was invented by Grossi, Grupta and Vitter \citep{Grossi:2003:HET:644108.644250} in 2003. 
It has applications in many areas, from string processing to geometry, and can for instance be used to represent a sequence of elements, a reordering of elements or a grid of points. 
When \citep{Grossi:2003:HET:644108.644250} invented the Wavelet Tree, it was a milestone in compressed full-text indexing even though it is mentioned very little in the paper.

\subsection{Constructing the Wavelet Tree}
\subsubsection{Informal Definition}
A Wavelet Tree stores a string in a binary tree of bitmaps, each bitmap indicating for each character in the corresponding string in which half of the alphabet it resides.

At the root node is stored a bitmap with the length of the input string, and each bit in this bitmap represents which half of the alphabet the character at the same position resides in.
At each node, the alphabet is split in half and the symbols in the left half are assigned bit value 0 in the bitmap and the symbols to the right are assigned bit value 1 so that there is a bit for each symbol in the string in the bitmap. 
The symbols of the string that has bit value 0 are concatenated in the order they appear in the string and are passed as input string to the left sub-tree and the ones with bit value 1 are passed as input string to the right sub-tree.
Only the bitmaps are stored in each node.
The alphabet of the input string can be stored in the tree.
This process continues recursively in each sub tree until only one character is left in the alphabet of the input string of a sub tree.
The sub tree is then considered a leaf node and needs not store a bitmap.

An example of a Wavelet Tree can be seen in Figure \ref{fig:WaveletTreeExample}.


\figureBegin
\Tree
%root
[.adsfadaadsfaads\\001100000110001 !\qsetw{5cm} 
	%left child
	[.adadaadaad\\0101001001 !\qsetw{5cm}
		%left -> left,right child 
		[.aaaaaa !\qsetw{5cm} ] [.dddd !\qsetw{5cm} ]] 
	%right child
	[.sfsfs\\10101 !\qsetw{5cm} 
		%right -> left,right child
		[.ff !\qsetw{5.3cm} ] [.sss !\qsetw{5.3cm} ]]] 
\caption{Wavelet Tree on string \textit{adsfadaadsfaads} with alphabet $\sigma = adfs$. Note that only the bitmaps are actually stored in the tree. The characters are annotations for ease of understanding.}	
\label{fig:WaveletTreeExample}
\figureEnd

\subsubsection{Formal Definition}
A Wavelet Tree for a sequence $S[1,n]$ over alphabet $[1 .. \sigma]$ can be described recursively over a sub-alphabet range $[a .. b] \subseteq [1 .. \sigma]$.
A Wavelet Tree over alphabet $[a .. b]$ (also called $\sigma$) is a binary balanced tree with $b - a + 1$ leaves. If $a = b$ then the Wavelet Tree is simply a leaf labelled a. 
Otherwise it has an internal root node $v_{root}$ that represents the string $S[1,n]$. 

Definition \ref{def:SplittingTheString} defines more formally how the substrings of the input string passed to the left and right sub tree are constructed.


\vspace{0.5 cm}
\begin{mdframed}[nobreak, linecolor=lightgray, linewidth=2pt]
\begin{definition} Splitting the string. \\\\
Let $S_0[1,n_0] =$ subsequence of $S[1,n]$ formed by symbols $c \leq \sigma/2$.\\
Let $S_1[1,n_1] =$ subsequence of $S[1,n]$ formed by symbols $c > \sigma/2$.\\\\
Then the left child of $v_{root}$ is a Wavelet Tree for $S_0[1,n_0]$ over alphabet $[a .. \lfloor (a + b)/2 \rfloor]$ and right child of $v_{root}$ is a Wavelet Tree for $S_1[1,n_1]$ over alphabet $[1 + \lfloor (a + b)/2 \rfloor .. b]$.
\label{def:SplittingTheString}
\end{definition}
\end{mdframed}
\vspace{0.5 cm}


\subsubsection{Complexity}
The height of a filled Wavelet Tree is  $\lceil \log \sigma \rceil$ and it has $\sigma$ leaves and $\sigma - 1$ internal nodes. 
At each level in the tree at most \textit{n} bits are stored in the bitmaps in total.
If some characters of the alphabet do not occur in the input string, the sub trees of the Wavelet tree corresponding to those missing characters may be omitted depending on implementation.
$n \lceil \log \sigma \rceil$ is an upper bound to the total number of bits that the Wavelet Tree stores in its bitmaps.
The Wavelet Tree can theoretically be constructed in $O(n \log \sigma)$ time. 
\textbf{[TODO: references to why or possibly explain it ourselves]}\\


\subsubsection{Pseudocode}
\label{sec:nodeconstruction}
The Wavelet Tree construction algorithm is recursively defined, calling itself to construct the left and right sub-tree from the root node and down. At each recursion the algorithm splits the given alphabet in two halves and traverses the given string putting each character into a left or right partition based on whether the character was in the left or right half of the alphabet.

\begin{algorithm}
\caption{Construction of nodes in the Wavelet Tree}
\label{alg:ConstructNode}
\begin{algorithmic}
\Function {ConstructNode} {$String, Alphabet$}
\If{$Alphabet.Size() = 1$ or $String.Length() = 0$}
	\State \Return
\EndIf
\State Split $Alphabet$ into $LeftAlphabet$ and $RightAlphabet$
\State $Split \gets$ middle character in $Alphabet$
\ForAll {$Character$ in $String$}
	\If {$Character < Split$}
		\State $LeftString.Append(Character)$
		\State $Bitmap.Append(0)$
	\Else
		\State $RightString.Append(Character)$
		\State $Bitmap.Append(1)$
	\EndIf
\EndFor
\State $LeftNode \gets$ \Call {ConstructNode} {$LeftAlphabet, LeftString$}
\State $RightNode \gets$ \Call {ConstructNode} {$RightAlphabet, RightString$}
\EndFunction

\State \Call {ConstructNode} {InputString, InputAlphabet}
\end{algorithmic}
\end{algorithm}

\subsection{Rank Query}
\begin{algorithm}
\caption{Rank}
\label{alg:rank}
\begin{algorithmic} 
\Function {Rank} {$Character, Position$}
\If{$Self.IsLeaf()$}
\State \Return $Position$
\EndIf
\State $CharBit \gets$ bit representing $Character$ in bitmap of current node
\State $Occourance \gets$ \Call {BinaryRank} {$CharBit, Position, BitMap$}
\If{$CharBit = 1$}
	\State $Rank \gets$ RightChildNode.\Call {Rank} {$CharBit, Position$}
\Else
	\State $Rank \gets$ LeftChildNode.\Call {Rank} {$CharBit, Occourance$}
\EndIf
\State \Return $Rank$ 
\EndFunction
\State RootNode.\Call {Rank} {$Character, Position$}
\end{algorithmic}
\end{algorithm}

Rank counts the number of occurrences of the specified character up to and including the specified position. 
It starts from the root of the Wavelet Tree and moves down through the tree until it hits the leaf of the input character, \citep[Section 2.2]{Claude08practicalrankselect}. 
In each level of the tree \textproc{BinaryRank} is calculated for the character. 
The result of \textproc{BinaryRank} is saved as the position to find rank for in the next recursive call. 
When the leaf of the input symbol is reached the \textit{Position} variable contains the rank of the input symbol up to the original input position.
Whether we query towards the left or right child in the recursive call is decided by what the symbol in represented as in the bitmap of the current node.
A way to calculate this would be to calculate the split character of the current alphabet and the check whether the input symbol is lexicographically less than or equal to the split character. 
If this is the case the input symbol is represented by 0 since it would be in the alphabet of the left child.
If it is not the case then the input symbol is represented as 1.
This is the way we do it in our implementation. 

\textproc{Rank($Character, Position$)} in our implementation is defined on each node, which is why it is called on the root node and the right and left child nodes in stead of specifying the node as a parameter to the \textproc{Rank} function, in the pseudo-code.

\subsubsection{Binary Rank} 
\label{sec:TheoryBinaryRank}
\begin{algorithm}
\caption{BinaryRank}
\label{alg:binaryrank}
\begin{algorithmic}
\Function {BinaryRank} {$Position, Bitmap$}
\State $i \gets 0$
\State $rankValue \gets 0$
\For{$bit$ in $Bitmap$}
\If{$i > Position$}
\State \Return $rankValue$
\EndIf
\If{$bit == 1$}
\State $rankValue \gets rankValue + 1$
\EndIf
\State $i \gets i + 1$
\EndFor
\EndFunction
\end{algorithmic}
\end{algorithm}


\subsection{Select query}
\begin{algorithm}
\caption{Select}
\label{alg:select}
\begin{algorithmic} 
\Function {Select} {$Character, Occurrence$}
\State $Leaf \gets GetLeaf(Character)$
\If{$Leaf$ is $RightChild$}
	\State $CharBit \gets 1$
\Else
	\State $CharBit \gets 0$
\EndIf
\State \Return $Leaf.Parent.$\textproc{SelectRec}$(CharBit, Occurrence)$
\EndFunction

\vspace{1cm}

\Function {SelectRec} {$CharBit, Occurrence$}
\If{$CurrentNode$ is $root$}
	\State \Return \textproc{BinarySelect}$(CharBit, Occurrence)$
\EndIf
\State $Position \gets $\textproc{BinarySelect}$(CharBit, Occurrence)$
\If{$CurrentNode$ is $RightChild$}
	\State $CharBit \gets 1$
\Else
	\State $CharBit \gets 0$
\EndIf
\State \Return $Parent.$\textproc{SelectRec}$(CharBit, Position)$
\EndFunction

\vspace{1cm}

\Function {GetLeaf} {$Character$}
\State $CharBit \gets character > alphabet/2$
\If{$CharBit == 1$ AND this node has a right child}
	\State RightChild.GetLeaf($Character$)
\ElsIf{$CharBit == 0$ AND this node has a left child}
	\State LeftChild.GetLeaf($Character$)
\EndIf
\State \Return CurrentNode
\EndFunction
\end{algorithmic}
\end{algorithm}

\textproc{Select} queries the Wavelet Tree for the position of the $i$th occurrence of the specified character.
It starts from the leaf of the character and queries up through the tree, which means it is necessary to know the leaf of the character. \citep[Section~2.2]{Claude08practicalrankselect}. 
This is accomplished using the \textproc{GetLeaf} method, which works by recursing down the tree from the root and choosing the right or left path based on whether the character is represented as a 0 or a 1 in the bitmap of the current node.
The recursion stops when the leaf of the character is found which is indicated by the fact that there are no left or right children of the current node.
After having found the leaf the \textproc{Select} function calculates the \textit{CharBit} based on whether the found leaf is a right or left child of it's parent.

\textproc{SelectRec} is then called on the parent of the leaf and recurses up the tree towards the root to find the position of the specified occurrence of the input character.
It does so by calling \textproc{BinarySelect} which returns the position of the specified occurrence of the \textproc{CharBit} in the bitmap of the current node. 
This position is then used as the occurrence to find the position of in the next recursive call.
When the \textit{root} is reached the result of \textproc{BinarySelect} gives us the position of the specified occurrence of the character in the string.

Select is split into the \textproc{Select} and \textproc{SelectRec} functions because it saves us a check for whether the current node is a leaf or not at each recursive call and since we start out in a leaf and will never meet one again that check would only be true once during the execution.

Figuring out the \textproc{CharBit} is a lot easier for \textproc{Select} than for \textproc{Rank} since we query bottom-up and can just check whether the current node is a left child or a right child. 
In \textproc{Rank} one needs to calculate the split character and then check whether the input character is before or after it in the alphabet in every recursive step.

\subsubsection{Binary Select}
\begin{algorithm}
\caption{BinarySelect}
\label{alg:binaryselect}
\begin{algorithmic}
\Function {BinarySelect} {$Occourance, Bitmap, BitToCount$}
\State $Occ \gets 0$
\For{$bit$ in $Bitmap$}
\If{$bit == BitToCount$}
\If{$Occ == Occourance$}
\State \Return position of $bit$ in $Bitmap$
\EndIf
\State $Occ \gets Occ + 1$
\EndIf
\EndFor
\EndFunction
\end{algorithmic}
\end{algorithm}

\textproc{BinarySelect} returns the position of the i'th occurrence of 1 or 0 (\textproc{CharBit}) in a bitset. 
It does so by looping through the bitset and when \textit{i} occurrences of either 1 or 0 (depending on whether we are in a left child or a right child) has been seen then the position of the last bit that was seen is returned.
\textproc{BinarySelect} takes O(n) time.
In section~\ref{sec:ImplBinarySelect} we describe how to improve the practical running time of \textproc{BinarySelect}.

\subsection{Compression}
\vspace{0.5 cm}
\begin{mdframed}[nobreak, linecolor=lightgray, linewidth=2pt]
\begin{definition}: String representation in a Wavelet Tree\\\\
Let $S[1,n] = S_1 S_2 ... S_n$ be a sequence of symbols where $s_i \in \Sigma$ and $\Sigma = [1 .. \sigma]$ is the alphabet. $S$ can then be represented in plain form using $n \lceil \log \sigma \rceil = n \log \sigma = O(n)$ bits.
\end{definition}
\textbf{[TODO: use and reference this]}
\end{mdframed}
\vspace{0.5 cm}
The Wavelet Tree is used a lot for compression of data. Two of the main compression techniques that are used are specific encodings on bitmaps and changing the shape of the tree \citep[Section~3]{Navjda13}.
The type of encoding on bitmaps that is used most often is entropy coding.

Entropy coding can be explained in two parts: Modelling and Coding.
Modelling assigns probabilities to the symbols of the string, and coding produces a bit sequence from these probabilities.
The probability of a symbol is based on how often it appears in a given data set. 
Higher frequency gives a higher probability.
Coding can then for instance be done with a coding scheme, that uses a discrete number of bits for each symbol, for example Huffman Coding \citep{HuffmanCoding}.

The main advantage of the wavelet tree with regards to compression is that it supports entropy bounds in the attained space complexity of the various wavelet tree compression methods \citep[Section~2.1]{WTSurvey}.

\subsubsection{Entropy}
According to \citep[Section~1]{WTSurvey} entropy can be defined as:

\begin{mdframed}[nobreak, linecolor=lightgray, linewidth=2pt]
\begin{definition}: Entropy \\\\
Let \textit{S} be a sequence of \textit{n} symbols from an alphabet $\Sigma = \lbrace c_1 ... c_\sigma \rbrace$ with alphabet $\sigma$.
Then \textit{entropy} H is defined as
\begin{center}
$H = \sum_{i=1}^{\sigma} p_i log_2(\frac{1}{p_i})$
\end{center}
where $p_i$ is the probability of the i-th symbol in the alphabet appearing in \textit{S}.
\end{definition} 
\end{mdframed}

Entropy represents a lower bound to the average numbers of bits needed to represent each symbol in S according to the coding theorem of Shannon \textbf{[TODO: reference]} and so is the value that researchers of compression compare their results to.

This theoretical definition of entropy is often replaced in scientific literature by a more practical definition: \textit{empirical entropy}.
There are two versions: \textit{empirical zero-order entropy} $H_0$ and \textit{empirical k-th order entropy} $H_k$. $H_k$ takes the size \textit{k} context of the symbol appearances into account while $H_0$ does not and treats symbols independently instead. 

\begin{mdframed}[nobreak, linecolor=lightgray, linewidth=2pt]
\begin{definition}: \textit{Empirical zero-order entropy}, $H_0$ \\
Let \textit{S} be a sequence of \textit{n} symbols from an alphabet $\Sigma = \lbrace c_1 ... c_\sigma \rbrace$ of cardinality $\sigma$.
Then \textit{entropy} $H_0$ is defined as
\begin{center}
$H_0 = \sum\limits_{c_i \in \Sigma} \frac{n_i}{n} log_2(\frac{n}{n_i})$
\end{center}
where $n_i$ is the number of appearances of character $c_i$ in $S$.
\end{definition}
\begin{definition}: \textit{Empirical k-th order entropy}, $H_k$ \\
For a string $w \in \Sigma^k$ let us define $w_S$ as the subsequence of characters that follow \textit{w} in \textit{S}. 
Then the k-th order empirical entropy of \textit{S}, is defined as follows:
\begin{center}
$H_k(S) = \frac{1}{n} \sum\limits_{w \in \Sigma^k} | w_S |H_0(w_S)$
\end{center}
\end{definition}
\end{mdframed}
$H_k$ thus defines a lower bound for bit space usage that is smaller than the lower bound for $H_0$.

Using the Burrows-Wheeler transformation on the input the we can reduce the problem of achieving $H_k$ compression to achieving $H_0$ compression.
In other words, if we have a good compression algorithm that achieves compression within the $H_0$ lower bound, then by using that algorithm on the Burrows-Wheeler transformation of the input we can achieve compression within the $H_k$ lower bound. (Todo: explain the theory behind this)
The problem for a long time was that there existed no good way to achieve compression within the $H_0$ lower bound or at least it was a problem before the wavelet tree was invented. \textbf{[TODO: describe why it was a problem]} 

To achieve \textit{zero-order entropy} a Huffman shaped wavelet tree \textbf{[TODO: reference]} can be used. But when the alphabet is very large that is not a good approach with regards to compression, because storing the huffman symbol assignments and wavelet tree pointers ends up using too much space . \citep[Section~3]{Claude08practicalrankselect} describes a way to also have \textit{zero-order entropy} space usage for large alphabets. 
It is therefore possible to get space usage within \textit{zero-order entropy} even for large alphabets using the wavelet tree. 
\textit{Zero-order entropy} can also be achieved by run-length encoding the bitmaps in the wavelet tree, which is also the approach that is used when compressing the bwt of the input string using the wavelet tree \citep[Introduction (\textbf{B})]{waveletTreeEntropy}.
This means that using a combination of run-length encoding of bitmaps and taking the Burrows-Wheeler transformation of the input string using the wavelet tree it is possible to achieve a lower bound within \textit{k-th order entropy}.

\subsubsection{Run-Length encoding}
Run-length encoding (RLE) is a simple process where the number of consecutive repeats of each symbol is stored in stead of storing the symbols them selves. 
If we have the string \textit{aaaaacccaaaaabbbaa} we can run-length encode this to \textit{a5c3a5b3a2} which is a smaller string containing the same information.
It is necessary to store the symbol and it's number of consecutive repeating occurrences because we need to be able to identify which symbol occurs where and how many times in order to be able to reproduce the original string.
The longer the sequence of a repeating symbol is the less space is used, since it can be stored as one number plus the related symbol.

When representing the string using a wavelet tree problem gets reduced to run-length encoding a string of bits (the bitmap in each node).
Since a binary number only has an alphabet with the cardinality of two we do not need to the symbol and it's occurrence but only the occurrence.
If we define that the first number is always the amount of 0s and the second number is always the amount of 1s and so forth. 
Then by looking at the bitmap of \textit{aaaaacccaaaaabbbaa} which is \textit{000001110000000000} when stored in a wavelet tree we can encode it into $(5,3,10)$.
Figure~\ref{fig:RleWaveletTree} shows an example of a wavelet tree with run-length encoded bitmaps.

If we do not consider how a computer saves numbers but only consider the amount of numbers that needs to be stored then $RLE(000001110000000000)$ achieves a great reduction in space.
From 18 numbers to only 3 numbers, which contain the same information.

If we do consider how a computer saves numbers then the reduction is not that great because if we assume that each number is represented as an integer, then the run-length encoded bitmap uses more space than just storing the original bitmap. 
This is because an integer uses 4 bytes of space which is 32 bits and we need to store three integers giving us a total of $32 bits \times 3 = 96 bits$ which is significantly larger than just the 18 bits we need to store for the original bitmap assuming we can store 1 and 0 using only 1 bit \footnote{This can be accomplished using c++ and vector$<$bool$>$}. 
This means that a symbol needs to repeat consecutively more than 32 times before RLE achieves better space usage than just storing the bitmaps.

RLE is still very useful despite of this because you usually want to compress very large data and if that data uses an alphabet that is small enough then, as previously stated RLE can achieve compression close to the \textit{zero order entropy} when working with binary alphabets.

If the Burrows-Wheeler transformation is applied to the string before it is saved in the wavelet tree and run-length encoded then the number of consecutive repeats of a symbol is increased which enables even greater compression.

\subsubsection{Burrows-Wheeler transformation}
Having a string \textit{S} of \textit{n} characters the Burrows-Wheeler transformation \citep{BWToriginalArticle} transforms \textit{S} by forming \textit{n} cyclic shifts of \textit{S}. 
These \textit{n} permutations of \textit{S} are then sorted in lexicographical order.
The \# is added to keep track of the end of the original string.
The \textit{bwt} of \textit{S} is then the concatenation of the last characters in each permutation excluding $\#$.

\begin{center}
$\begin{bmatrix}
	bananahat\#\\
	ananahat\#b\\
	nanahat\#ba\\
	anahat\#ban\\
	nahat\#bana\\
	ahat\#banan\\
	hat\#banana\\
	at\#bananah\\
	t\#bananaha\\
	\#bananahat\\
\end{bmatrix} \Rightarrow
\begin{bmatrix}
	\#bananaha\textbf{t}\\
	ahat\#bana\textbf{n}\\
	anahat\#ba\textbf{n}\\
	ananahat\#\textbf{b}\\
	at\#banana\textbf{h}\\
	bananahat\#\\
	hat\#banan\textbf{a}\\
	nahat\#ban\textbf{a}\\
	nanahat\#b\textbf{a}\\
	t\#bananah\textbf{a}
\end{bmatrix}$
\end{center}

The list to the left is the permuted versions of \textit{S} and the list to the right is the lexicographically sorted permutations of \textit{S}.
Taking the character at the last index in each column the Burrows-Wheeler transformation of $S=bananahat$ becomes $bwt(S) = tnnbhaaaa$. The index of the the original string in the right list is also returned. 
The original string is identified by having a \# at the end.

Looking at \textit{bwt(S)} we can see that equal characters are now grouped together.
This effect enables better compression when using compression techniques like run-length encoding.
One could argue that simply sorting \textit{S} would enable better compression using run-length encoding, which is true. 
But a sorted sequence is not reversible but BWT is, which is the main reason for using BWT. 
It would not make very much sense to compress something without being able to decompress it again.
\citep[Section~2]{BWToriginalArticle} describes an algorithm for getting the original string from the Burrows-Wheeler transformed string.

Figure~\ref{fig:BwtRleWaveletTree} depicts a RLE Wavelet tree on \textit{bwt(abcbadbd)}. We can see, as indicated by the numbers in bold that fewer Run-length encoded values needs to be stored than for the non-Burrows-Wheeler transformed case as shown in figure~\ref{fig:RleWaveletTree}.
\figureBegin
      \begin{minipage}{0.45\textwidth}      
			\Tree
			%root
			[.bananahat\\2,1,1,1,3,1 !\qsetw{3cm} 
				%left child
				[.baaaha\\1,2,1,1 !\qsetw{3cm}
					[.baaaa\\0,1,4 !\qsetw{3cm}
						[.aaaa !\qsetw{3cm} ]
						[.b !\qsetw{3cm} ]		
					] 
					[.h !\qsetw{3cm} ]
				] 
				%right child
				[.nnt\\2,1 !\qsetw{3cm}	
					[.nn !\qsetw{3cm} ] 
					[.t !\qsetw{3cm} ]
				]
			]
		\caption{RLE Wavelet Tree on string \textit{bananahat} with alphabet $\sigma = abhnt$}
      \label{fig:RleWaveletTree}
	\end{minipage}
	\hfill
	\begin{minipage}{0.45\textwidth}	
			\Tree
			%root
			[.tnnbhaaaa\\\textbf{0,3,6} !\qsetw{3cm} 
				%left child
				[.bhaaaa\\\textbf{1,1,4} !\qsetw{3cm} 
					[.baaaa\\0,1,4 !\qsetw{3cm} 
						[.aaaa !\qsetw{3cm} ]
						[.b !\qsetw{3cm} ]		
					] 
					[.h !\qsetw{3cm} ]
				] 
				%right child
				[.tnn\\0,1,2 !\qsetw{3cm}		
					[.nn !\qsetw{3cm} ] 
					[.t !\qsetw{3cm} ]
				]
			] 
		\caption{RLE Wavelet Tree on $BWT(bananahat)=tnnbhaaaa$ with alphabet $\sigma = abhnt$}
		\label{fig:BwtRleWaveletTree}
	\end{minipage}
\figureEnd

One might wonder how \textproc{Rank} and \textproc{Select} queries can be useful when the string is Burrows-Wheeler transformed and therefore permuted resulting in the queries values that is essentially unrelated to the original string \textit{S}. 

The queries does have uses when working with the \textit{FM-index} which is a self-index based on the Burrows-Wheeler transformation $BWT(S)$ that is able to find occurrences and positions of patterns (sub-strings) in \textit{S} by looking at $BWT(S)$. 
Such a procedure is described in \citep[Section 2]{FMcountOnBWT}.

\subsubsection{Huffman-shaped Wavelet Trees}
\citep[Section 2]{FMcountOnBWT} describes a Huffman Shaped Wavelet Tree which places symbols with higher frequencies closer to the root than those that have a lower frequency, in such a way that the path from root to symbol leaf corresponds to the binary Huffman code of the symbol. Using a Huffman Shaped Wavelet Tree is an alternative to run-length encoding.
The Huffman code of a symbol is a small binary value for symbols with high frequency and a larger binary value for symbols with the small frequency.
This approach would make the tree skewed and as a result increase the height of the tree but decrease query time for symbols with high frequency.
If one then queries the Huffman shaped Wavelet Tree for a symbol that has a high frequency then one would find it faster than if one was looking for a symbol that was less frequent.

Since the Huffman encoding is based on frequency of symbols it achieves the best performance and space complexity when symbols are non-uniformly distributed.
If the data is uniformly distributed then the length of all Huffman codes would be similar resulting in a balanced tree having performance and space complexity similar to a normal Wavelet Tree.
Since we have used uniformly distributed data in our experiments we have not done any experiments on the Huffman Shaped Wavelet Tree.
In section~\ref{sec:choiseOfInputString} we discuss why we only consider uniform data.


\subsection{Information Retrieval}
1. Wavelet tree as a Range Searching Data Structure.\\


\subsubsection{Range Quantile Query}
The wavelet tree can be used to support efficient range quantile queries on a sequence \textit{S} of \textit{n} numbers in $O(\log(\sigma))$ time if $rank_b$ is supported in $O(1)$ time \citep[Section 3]{RangeQuantileQueryWT}. 
A range quantile query returns the k-th smallest number within a given range of \textit{S}, $S[l ... r]$.
It works by computing two rank queries on the bitmap of the root. 

\begin{algorithm}
\caption{Range Quantile Query}
\label{alg:RangeQuantileQuery}
\begin{algorithmic}
\Function {RangeQuantileQuery} {$k,l,r$}
\If{current node is leaf}
	\State \Return number in leaf
\EndIf
\State $\textit{0}sInRange \gets rank_0(S \lbrace l..r \rbrace) = rank_0(r) - rank_0(l-1)$
\If{$\textit{0}sInRange \leq k$}
	\State $l = rank_0(l-1))+1$
	\State $r = rank_0(r)$
	\State \Return $LeftNode.RangeQuantileQuery(k,l,r)$
\Else
	\State $k = k - \textit{0}sInRange)$
	\State $l = rank_1(l-1)+1$
	\State $r = rank_1(r)$
	\State \Return $RightNode.RangeQuantileQuery(k,l,r)$
\EndIf

\EndFunction
\end{algorithmic}
\end{algorithm}

The two queries are $rank_b(l-1)$ and $rank_b(r)$ where $rank_b()$ refers to binary rank.
$rank_b(l-1)$ is used to find the number of 1s and 0s in $b[1..(l-1)]$ and $rank_b(r) - rank_b(l-1)$ gives the number 1s and 0s in $b[l..r]$. 
We go to the left if there are more than \textit{k} copies of 0s in $b[l..r]$ and set $l = ($number of 0s in $b[1..(l-1)])+1$ and $r=($number of 0s in $b[1..r])$. 
We go to the right if there are less than \textit{k} copies of 0s in $b[l..r]$ and subtract the number of 0s in $b[l..r]$ from \textit{k} and set $l = ($number of 1s in $b[1..(l-1)])+1$ and set $r=($number of 1s in $b[1..r])$. 
This procedure continues recursively until it hits a leaf and then returns the number stored in the leaf which corresponds to the k-th smallest number in $S[l..r]$.

Algorithm~\ref{alg:RangeQuantileQuery} describes the pseudo-codo for a range quantile query where $rank_1$ is a binary rank query which returns the number of 1s in the bitmap of $S$ in each node generated by the wavelet tree. 
$rank_0$ return the number of 0s.
The argument to $rank_0$ and $rank_1$ is the position to find the number of occurrences up to.
This means that $rank_1(r)$ for instance returns the number of 1s in the bitmap until position $r$.

\figureBegin
\begin{minipage}{.45\linewidth}
\begin{flushleft}                           
\Tree
%root
[.6,2,\textbf{0,7,9,3,1,8,5},4\\1001100110 !\qsetw{5cm} 
	%left child
	[.2,0,3,1,4\\00101  !\qsetw{5cm}
		%left -> left
		[.2,0,1\\100 !\qsetw{5cm} 
			[.0,1\\01 
				[.0 ]
				[.1 ] 			
			]
			[.2 ]		
		] 
		%left -> right
		[.3,4\\01 !\qsetw{5cm} 
			[.3 ]
			[.4 ]		
		]
	] 
	%right child
	[.6-\textbf{7,9,8,5}\\00110 !\qsetw{5cm} 
		%right -> left
		[.6,\textbf{7,5}\\010 !\qsetw{5.3cm}
			%right -> left -> left
			[.6,5\\10 
				[.5 ]
				[.6 ]			
			] 
			%right -> left -> right
			[.\textbf{7} ] 
		]
		%right -> right	 
		[.9,8\\10 !\qsetw{5.3cm} 
			[.8 ]
			[.9 ]		
		]
	]
] 
\end{flushleft} 
\end{minipage}
\hfill
\begin{minipage}{.45\linewidth}
\begin{flushright}
Level 1:\\$k=5$\\$l=3$\\$r=9$\\ \vspace{0.5cm}
Level 2:\\$k=2$\\$l=2$\\$r=5$\\ \vspace{0.5cm}
level 3:\\$k=2$\\$l=2$\\$r=3$\\ \vspace{0.5cm}
Level 4:\\$k=1$\\$l=1$\\$r=1$
\end{flushright} 
\end{minipage}
\caption{Range Quantile Query on a Wavelet Tree. $S=\lbrace 6,2,0,7,9,3,1,8,5,4 \rbrace, k=5, l=3, r=9$.}
\label{fig:RQQWaveletTree}
\figureEnd

An example of a range quantile query can be seen in figure~\ref{fig:RQQWaveletTree}. 
The numbers in bold indicates the range $S[l..r]$ where $S= \lbrace 6,2,0,7,9,3,1,8,5,4 \rbrace$ and $l=3$, $r=9$ and $k=5$.
$k=5$ means that we are looking for the 5th smallest number within $S[l..r]$ which is \textit{7} indicated by the leaf that the query ends up in before terminating.
$l$ and $r$ indicates the range to look within.
The right side of the figure shows how \textit{k, l} and \textit{r} develops in each recursive call of the Range Quantile Query.







