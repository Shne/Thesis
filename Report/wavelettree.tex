\section{The Wavelet Tree}
The Wavelet Tree is a binary balanced tree structure, that was invented by Grossi, Grupta and Vitter \citep{Grossi:2003:HET:644108.644250} in 2003. 
It has applications in many areas; from string processing to geometry, and can for instance be used to represent; a sequence of elements, a reordering of elements and a grid of points. 
When \citep{Grossi:2003:HET:644108.644250} invented the Wavelet Tree, it was a milestone in compressed full-text indexing even though it is mentioned very little in the paper.
\textbf{[TODO: The Idea behind it, as well as for rank and select]}

\subsection{Constructing the Wavelet Tree}
A Wavelet Tree stores a string by creating a bitmap that describes the string using the alphabet of the string. The alphabet is split in the middle and the symbols to the left gets bit value 0 and the symbols to the right gets bit value 1 so that there is a bit for each symbol in the string in the bitmap. 
The symbols of the string that has bit value 0 is concatenated in the order they have in the string and is added to the left sub-tree and the ones with bit value 1 is added to the right sub-tree. 

This process continues in each sub tree until we end up in the leaves where the string only consists of one unique symbol from the alphabet. 
An example of a Wavelet Tree can be seen in Figure \ref{fig:WaveletTreeExample}. 
We now go into more detail about how it all works as described by \citep{Navjda13}.

\vspace{0.5 cm}
\begin{mdframed}[nobreak, linecolor=lightgray]
\textbf{Definition 1:} String representation in a Wavelet Tree

Let $S[1,n] = S_1 S_2 ... S_n$ be a sequence of symbols where $s_i \in \Sigma$ and $\Sigma = [1 .. \sigma]$ is the alphabet. $S$ can then be represented in plain form using $n \lceil \log \sigma \rceil = n \log \sigma = O(n)$ bits.
\end{mdframed}
\vspace{0.5 cm}


\figureBegin
\caption{Wavelet Tree on string \textit{adsfadaadsfaads}}				
\Tree
%root
[.adsfadaadsfaads\\001100000110001 !\qsetw{5cm} 
	%left child
	[.adadaadaad\\0101001001 !\qsetw{5cm}
		%left -> left,right child 
		[.aaaaaa\\000000 !\qsetw{5cm} ] [.dddd\\1111 !\qsetw{5cm} ]] 
	%right child
	[.sfsfs\\10101 !\qsetw{5cm} 
		%right -> left,right child
		[.ff\\00 !\qsetw{5.3cm} ] [.sss\\111 !\qsetw{5.3cm} ]]] 
\vspace{1 cm}
\label{fig:WaveletTreeExample}
\figureEnd

		
A Wavelet Tree can be described recursively over a sub-alphabet range $[a .. b] \subseteq [1 .. 0]$, for a sequence $S[1,n]$ over alphabet $[1 .. \sigma]$. 
A Wavelet Tree over alphabet $[a .. b]$ is a binary balanced tree with $b - a + 1$ leaves. If $a = b$ then the Wavelet Tree is simply a leaf labelled a. 
Otherwise it has an internal root node $v_{root}$ that represents the string $S[1,n]$. 
\textproc{BitmapConstruction} (Algorithm~\ref{alg:BitmapConstruction}) describes how $v_{root}$ constructs bitmap $B_{v_{root}}$. $B_{v_{root}}$ is stored in $v_{root}$.

\begin{algorithm}
\caption{Construction of a bitmap from a string \textit{S}}
\label{alg:BitmapConstruction}
\begin{algorithmic}
\Function{BitmapConstruction}{$S$}
\If{ $S[i] \leq (a + b)/2$ }
	\State $B_{v_{root}}[i] \gets 0$
\Else
	\State $B_{v_{root}}[i] \gets 1$
\EndIf
\EndFunction
\end{algorithmic}
\end{algorithm}


Knowing how to construct the bitmap in the root of the Wavelet Tree we now define (Definition 2) how string is split in two and saved in the roots of right and left sub=trees which are also Wavelet Trees.


\vspace{0.5 cm}
\begin{mdframed}[nobreak]
\textbf{Definition 2:} Splitting the string. \\

\noindent
Let $S_0[1,n_0] =$ subsequence of $S[1,n]$ formed by symbols $c \leq (a + b)$/2.

\noindent
Let $S_1[1,n_1] =$ subsequence of $S[1,n]$ formed by symbols $c > (a + b)$/2.
\\ \linebreak
\noindent
Then the left child of $v_{root}$ is a Wavelet Tree for $S_0[1,n_0]$ over alphabet $[a .. \lfloor (a + b)/2 \rfloor]$ and right child of $v_{root}$ is a Wavelet Tree for $S_1[1,n_1]$ over alphabet $[1 + \lfloor (a + b)/2 \rfloor .. b]$. 
\end{mdframed}
\vspace{0.5 cm}

Having generated the left and right sub-trees the construction process (splitting of string and bitmap construction) continues recursively on the root of each of the sub-trees until it end up in a leaf.

\subsubsection{Complexity}
The height of the Wavelet Tree is  $\lceil \log \sigma \rceil$ and it has $\sigma$ leaves and $\sigma - 1$ internal nodes. 
At each level in the tree n bits are stored and in the last level at most n bits are stored. $n \lceil \log \sigma \rceil$ is an upper bound to the total number of bits that the Wavelet Tree stores. 
The Wavelet Tree can be constructed in $O(n \log \sigma)$ time.

\subsection{Rank Query}
Rank counts the number of occurrences of the specified character up to and including the specified position. 
It starts from the root of the Wavelet Tree and moves down through the tree until it hits the leaf of the input character, \citep[Section 2.2]{Claude08practicalrankselect}.

\begin{algorithm}
\caption{Rank}
\label{alg:rank}
\begin{algorithmic} 
\Function {Rank} {$Character, Position$}
\If{$Self.IsLeaf()$}
\State \Return $Position$
\EndIf
\State $CharBit \gets$ bit representing $Character$ in bitmap of current node
\State $Position \gets$ \Call {BinaryRank} {$CharBit, Position, BitMap$}
\If{$CharBit = 1$}
	\State $Rank \gets$ RightChildNode.\Call {Rank} {$CharBit, Position$}
\Else
	\State $Rank \gets$ LeftChildNode.\Call {Rank} {$CharBit, Position$}
\EndIf
\State \Return $Rank$ 
\EndFunction
\State RootNode.\Call {Rank} {$Character, Position$}
\end{algorithmic}
\end{algorithm}


\noindent \textproc{Rank($Character, Position$)} in our implementation is defined on each node, which is why it is called on the root node and the right and left child nodes in stead of specifying the node as a parameter to the \textproc{Rank} function, here in the pseudo-code.

\subsubsection{Binary Rank} 
\label{sec:TheoryBinaryRank}
A cornerstone of the rank algorithm for a Wavelet Tree is calculating the binary rank of a character on a bitmap at a node.
The naive implementation of \textproc{BinaryRank} where you loop through the binary number and count the number of 1s or 0s takes O(N) time, which is too much because the running time of the rank algorithm depends on the running time of \textproc{BinaryRank} since \textproc{BinaryRank} is called once per layer of the tree in order to answer a single rank query on the tree.

To make binary rank fast, we later used the \texttt{popcount} instruction that counts the number of 1s in the binary represenation of a given number. See section~\ref{sec:popcountBinaryRank}.

\subsection{Select query}
\textproc{Select} queries the Wavelet Tree for the position of the i'th occurrence of the specified character.
It starts from the leaf of the character and queries up through the tree, which means you need to know the leaf of the character in advance \citep[Section 2.2]{Claude08practicalrankselect}. 


\begin{algorithm}
\caption{Select}
\label{alg:select}
\begin{algorithmic} 
\Function {Select} {$Character, Occurrence$}
\If{$CurrentNode$ is $RightChild$}
	\State $CharBit \gets 1$
\Else
	\State $CharBit \gets 0$
\EndIf
\State \Return $Parent.$\textproc{SelectRec}$(CharBit, Occurrence)$
\EndFunction

\vspace{1cm}

\Function {SelectRec} {$CharBit, Occurrence$}
\If{$CurrentNode$ is $root$}
	\State \Return \textproc{BinarySelect}$(CharBit, Occurrence)$
\EndIf
\State $Position \gets $\textproc{BinarySelect}$(CharBit, Occurrence)$
\If{$CurrentNode$ is $RightChild$}
	\State $CharBit \gets 1$
\Else
	\State $CharBit \gets 0$
\EndIf
\State \Return $Parent.$\textproc{SelectRec}$(CharBit, Position)$
\EndFunction
\end{algorithmic}
\end{algorithm}


We split Select into two functions; \textproc{Select} which is called on the leaf and \textproc{SelectRec} which is called from \textproc{Select} on the parent of the leaf and continues recursively until the root is reached. 
This makes sense because it saves us a check for whether we are a leaf or not at each recursive call and since we start out in a leaf and will never meet one again that check would only be true once. 

Figuring out the \textproc{CharBit} is a lot easier for \textproc{Select} than for \textproc{Rank} since we query bottom-up and can just check whether the current node is a left child or a right child. 
In \textproc{Rank} one needs to calculate the split character and then check whether the input character is before or after it in the alphabet.

\subsubsection{Binary Select}
\textproc{BinarySelect} returns the position of the i'th occurrence of 1 or 0 (\textproc{CharBit}) in a bitset. 
The simple implementation is to loop through the bitset and when $i$ occurrences of either 1 or 0 has been seen then return the position of the last one. 
This takes O(N) time making \textproc{BinarySelect} a huge bottleneck for \textproc{Select} since it is called for each level in the Wavelet Tree. 
In \ref{sec:ImplBinarySelect} we describe how to improve the practical running time of \textproc{BinarySelect} using the \textit{popcount} instruction.

\subsection{Applications}
\subsubsection{Compression}
The Wavelet Tree is used for compression of data in a lot of ways where two of the main techniques are using specific encodings on bitmaps and changing the shape of the tree \citep[Section~3]{Navjda13}.
The type of encoding on bitmaps that is used most often is entropy coding.

Entropy coding can be explained by splitting it into two parts: Modelling and Coding.
Modelling assigns probabilities to the symbols of the string, and coding produces a bit sequence from these probabilities.
The probability of a symbol is based on how often it appears in a given data set. 
Higher frequency gives a higher probability.
Entropy coding can be done with a coding scheme, which uses a discrete number of bits for each symbol, for example Huffman coding. 
Entropy can be defined as:

\begin{mdframed}[nobreak, linecolor=lightgray]
\textbf{Definition}: Entropy. \\
Let \textit{S} be a sequence of \textit{n} symbols from an alphabet $\Sigma = \lbrace c_1 ... c_\sigma \rbrace$ of cardinality $\sigma$.
Then \textit{entropy} H is defined as
\begin{center}
$H = \sum_{i=1}^{\sigma} p_i log_2(\frac{1}{p_i})$
\end{center}
where $p_i$ is the probability of the i-th symbol in the alphabet appearing in \textit{S}. 
\end{mdframed}

Entropy represents a lower bound to the average numbers of bits needed to represent each symbol in S, according to the coding theorem of Shannon (todo reference) and so is a useful value to compare with when trying to achieve good compression.

But this theoretical definition of entropy is often replaced in scientific literature by a more practical definition: \textit{empirical entropy}.
There are two versions: \textit{empirical zero-order entropy} $H_0$ and \textit{empirical k-th order entropy} $H_k$. $H_k$ takes the size \textit{k} context of the symbol appearances into account while $H_0$ does not and treats symbols independently instead. 

\begin{mdframed}[nobreak, linecolor=lightgray]
\textbf{Definition}: \textit{Empirical zero-order entropy}, $H_0$ \\
Let \textit{S} be a sequence of \textit{n} symbols from an alphabet $\Sigma = \lbrace c_1 ... c_\sigma \rbrace$ of cardinality $\sigma$.
Then \textit{entropy} $H_0$ is defined as
\begin{center}
$H_0 = \sum\limits_{c_i \in \Sigma} \frac{n_i}{n} log_2(\frac{n}{n_i})$
\end{center}
where $n_i$ is the number of appearances of character $c_i$ in $S$.
\end{mdframed}

\begin{mdframed}[nobreak, linecolor=lightgray]
\textbf{Definition}: \textit{Empirical k-th order entropy}, $H_k$ \\
For a string $w \in \Sigma^k$ let us denote with $w_S$ the subsequence of characters that follow \textit{w} in \textit{S}, then the k-th order empirical entropy of \textit{S}, is defined as follows:
\begin{center}
$H_k(S) = \frac{1}{n} \sum\limits_{w \in \Sigma^k} | w_S |H_0(w_S)$
\end{center}
\end{mdframed}

The wavelet tree can be used to both achieve space within \textit{zero-order entropy} and \textit{k-th order entropy}. To achieve \textit{zero-order entropy} a Huffman shaped wavelet tree (todo: reference) can be used. But when the alphabet is very large that is not a good approach with regards to compression, because storing the huffman symbol assignments and wavelet tree pointers ends up using too much space . \citep[Section~3]{Claude08practicalrankselect} describes a way to also have \textit{zero-order entropy} space usage for large alphabets. 






