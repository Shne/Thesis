\section{The Wavelet Tree}
The Wavelet Tree is a binary balanced tree structure, that was invented by Grossi, Grupta and Vitter \citep{Grossi:2003:HET:644108.644250} in 2003. 
It has applications in many areas; from string processing to geometry, and can for instance be used to represent; a sequence of elements, a reordering of elements and a grid of points. 
When \citep{Grossi:2003:HET:644108.644250} invented the Wavelet Tree, it was a milestone in compressed full-text indexing even though it is mentioned very little in the paper.
\textbf{[TODO: The Idea behind it, as well as for rank and select]}

\subsection{Constructing the Wavelet Tree}
A Wavelet Tree stores a string by creating a bitmap that describes the string using the alphabet of the string. The alphabet is split in the middle and the symbols to the left gets bit value 0 and the symbols to the right gets bit value 1 so that there is a bit for each symbol in the string in the bitmap. 
The symbols of the string that has bit value 0 is concatenated in the order they have in the string and is added to the left sub-tree and the ones with bit value 1 is added to the right sub-tree. 

This process continues in each sub tree until we end up in the leaves where the string only consists of one unique symbol from the alphabet. 
An example of a Wavelet Tree can be seen in Figure \ref{fig:WaveletTreeExample}. 
We now go into more detail about how it all works as described by \citep{Navjda13}.

\vspace{0.5 cm}
\begin{mdframed}[nobreak, linecolor=lightgray]
\begin{definition}: String representation in a Wavelet Tree\\\\
Let $S[1,n] = S_1 S_2 ... S_n$ be a sequence of symbols where $s_i \in \Sigma$ and $\Sigma = [1 .. \sigma]$ is the alphabet. $S$ can then be represented in plain form using $n \lceil \log \sigma \rceil = n \log \sigma = O(n)$ bits.
\end{definition}
\end{mdframed}
\vspace{0.5 cm}


\figureBegin
\caption{Wavelet Tree on string \textit{adsfadaadsfaads}}				
\Tree
%root
[.adsfadaadsfaads\\001100000110001 !\qsetw{5cm} 
	%left child
	[.adadaadaad\\0101001001 !\qsetw{5cm}
		%left -> left,right child 
		[.aaaaaa\\000000 !\qsetw{5cm} ] [.dddd\\1111 !\qsetw{5cm} ]] 
	%right child
	[.sfsfs\\10101 !\qsetw{5cm} 
		%right -> left,right child
		[.ff\\00 !\qsetw{5.3cm} ] [.sss\\111 !\qsetw{5.3cm} ]]] 
\vspace{1 cm}
\label{fig:WaveletTreeExample}
\figureEnd

		
A Wavelet Tree can be described recursively over a sub-alphabet range $[a .. b] \subseteq [1 .. 0]$, for a sequence $S[1,n]$ over alphabet $[1 .. \sigma]$. 
A Wavelet Tree over alphabet $[a .. b]$ (also called $\sigma$) is a binary balanced tree with $b - a + 1$ leaves. If $a = b$ then the Wavelet Tree is simply a leaf labelled a. 
Otherwise it has an internal root node $v_{root}$ that represents the string $S[1,n]$. 
Algorithm~\ref{alg:BitmapConstruction}: \textproc{BitmapConstruction}  describes how a bitmap is constructed for every node of the tree.

\begin{algorithm}
\caption{Construction of a bitmap from a string \textit{S}}
\label{alg:BitmapConstruction}
\begin{algorithmic}
\Function{BitmapConstruction}{$S$}
\If{ $S[i] \in \sigma_{left}$ }
	\State $Bitmap[i] \gets 0$
\Else
	\State $Bitmap[i] \gets 1$
\EndIf
\EndFunction
\end{algorithmic}
\end{algorithm}

\textproc{BitmapConstruction} works by representing each symbol in the string is as either 1 or 0 in the bitmap based on whether it appears in the left or right alphabet. The symbols in the left alphabet gets represented as 0 and the ones in the right alphabet gets represented as 1.

Knowing how to construct the bitmap in each node of the Wavelet Tree we now define (Definition 2) how the string is split in two and saved in the roots of the right and left sub-trees which as a result also are Wavelet Trees by themselves.


\vspace{0.5 cm}
\begin{mdframed}[nobreak, linecolor=lightgray]
\begin{definition} Splitting the string. \\\\
Let $S_0[1,n_0] =$ subsequence of $S[1,n]$ formed by symbols $c \leq \sigma/2$.\\
Let $S_1[1,n_1] =$ subsequence of $S[1,n]$ formed by symbols $c > \sigma/2$.\\\\
Then the left child of $v_{root}$ is a Wavelet Tree for $S_0[1,n_0]$ over alphabet $[a .. \lfloor (a + b)/2 \rfloor]$ and right child of $v_{root}$ is a Wavelet Tree for $S_1[1,n_1]$ over alphabet $[1 + \lfloor (a + b)/2 \rfloor .. b]$. 
\end{definition}
\end{mdframed}
\vspace{0.5 cm}

The string is split by first splitting the alphabet in two pieces of equal size: one for the left child and one for the right child.
If the length of the alphabet is odd then the left alphabet has one more symbol than the right one. 
The symbols in the left alphabet are concatenated in the order they appear in $S$ and saved in $S_0$ and the symbols of the right alphabet is saved using the same procedure.
 
Having generated the left and right sub-trees the construction process (splitting of string and bitmap construction) continues recursively on the right and left children until there are no more children left which is indicated by a node only having one symbol in it's alphabet.
If this is the case then the node is a \textit{leaf}.

\subsubsection{Complexity}
The height of the Wavelet Tree is  $\lceil \log \sigma \rceil$ and it has $\sigma$ leaves and $\sigma - 1$ internal nodes. 
At each level in the tree exactly \textit{n} bits are stored and in the last level at most \textit{n} bits are stored. $n \lceil \log \sigma \rceil$ is an upper bound to the total number of bits that the Wavelet Tree stores. 
The Wavelet Tree can be constructed in $O(n \log \sigma)$ time. 
(Todo: references to why or possibly explain it ourselves)

\subsection{Rank Query}
\begin{algorithm}
\caption{Rank}
\label{alg:rank}
\begin{algorithmic} 
\Function {Rank} {$Character, Position$}
\If{$Self.IsLeaf()$}
\State \Return $Position$
\EndIf
\State $CharBit \gets$ bit representing $Character$ in bitmap of current node
\State $Occourance \gets$ \Call {BinaryRank} {$CharBit, Position, BitMap$}
\If{$CharBit = 1$}
	\State $Rank \gets$ RightChildNode.\Call {Rank} {$CharBit, Position$}
\Else
	\State $Rank \gets$ LeftChildNode.\Call {Rank} {$CharBit, Occourance$}
\EndIf
\State \Return $Rank$ 
\EndFunction
\State RootNode.\Call {Rank} {$Character, Position$}
\end{algorithmic}
\end{algorithm}

Rank counts the number of occurrences of the specified character up to and including the specified position. 
It starts from the root of the Wavelet Tree and moves down through the tree until it hits the leaf of the input character, \citep[Section 2.2]{Claude08practicalrankselect}. 
In each level of the tree \textproc{BinaryRank} is calculated for the character. 
The result of \textproc{BinaryRank} is saved as the position to find rank for in the next recursive call. 
When the leaf of the input symbol is reached the \textit{Position} variable contains the rank of the input symbol up to the original input position.
Whether we query towards the left or right child in the recursive call is decided by what the symbol in represented as in the bitmap of the current node.
A way to calculate this would be to calculate the split character of the current alphabet and the check whether the input symbol is lexicographically less than or equal to the split character. 
If this is the case the input symbol is represented by 0 since it would be in the alphabet of the left child.
If it is not the case then the input symbol is represented as 1.
This is the way we do it in our implementation. \\

\textproc{Rank($Character, Position$)} in our implementation is defined on each node, which is why it is called on the root node and the right and left child nodes in stead of specifying the node as a parameter to the \textproc{Rank} function, in the pseudo-code.

\subsubsection{Binary Rank} 
\label{sec:TheoryBinaryRank}
\begin{algorithm}
\caption{BinaryRank}
\label{alg:binaryrank}
\begin{algorithmic}
\Function {BinaryRank} {$Position, Bitmap$}
\State $i \gets 0$
\State $rankValue \gets 0$
\For{$bit$ in $Bitmap$}
\If{$i > Position$}
\State \Return $rankValue$
\EndIf
\If{$bit == 1$}
\State $rankValue \gets rankValue + 1$
\EndIf
\State $i \gets i + 1$
\EndFor
\EndFunction
\end{algorithmic}
\end{algorithm}
A bottleneck in the \textproc{Rank} query on the Wavelet Tree is calculating the \textproc{BinaryRank} of a character on a bitmap at a node.
\textproc{BinaryRank} loops through the binary number and counts the number of 1s or 0s and takes O(N) time. \textproc{BinaryRank} is called once per layer of the tree in order to answer a single rank query on the tree. 

To improve the running time of \textproc{Rank} we therefore try to make \textproc{BinaryRank} faster by using use the \texttt{popcount} instruction, which counts the number of 1s in the binary representation of a given binary number in O(1) time, to improve the running time of \textproc{BinaryRank}. 
This process in described in greater detail in section~\ref{sec:popcountBinaryRank}.

\subsection{Select query}
\begin{algorithm}
\caption{Select}
\label{alg:select}
\begin{algorithmic} 
\Function {Select} {$Character, Occurrence$}
\State $Leaf \gets GetLeaf(Character)$
\If{$Leaf$ is $RightChild$}
	\State $CharBit \gets 1$
\Else
	\State $CharBit \gets 0$
\EndIf
\State \Return $Leaf.Parent.$\textproc{SelectRec}$(CharBit, Occurrence)$
\EndFunction

\vspace{1cm}

\Function {SelectRec} {$CharBit, Occurrence$}
\If{$CurrentNode$ is $root$}
	\State \Return \textproc{BinarySelect}$(CharBit, Occurrence)$
\EndIf
\State $Position \gets $\textproc{BinarySelect}$(CharBit, Occurrence)$
\If{$CurrentNode$ is $RightChild$}
	\State $CharBit \gets 1$
\Else
	\State $CharBit \gets 0$
\EndIf
\State \Return $Parent.$\textproc{SelectRec}$(CharBit, Position)$
\EndFunction

\vspace{1cm}

\Function {GetLeaf} {$Character$}
\State $CharBit \gets character > alphabet/2$
\If{$CharBit == 1$ AND this node has a right child}
	\State RightChild.GetLeaf($Character$)
\ElsIf{$CharBit == 0$ AND this node has a left child}
	\State LeftChild.GetLeaf($Character$)
\EndIf
\State \Return CurrentNode
\EndFunction
\end{algorithmic}
\end{algorithm}

\textproc{Select} queries the Wavelet Tree for the position of the i-th occurrence of the specified character.
It starts from the leaf of the character and queries up through the tree, which means it is necessary to know the leaf of the character. \citep[Section 2.2]{Claude08practicalrankselect}. 
This is accomplished using the \textproc{GetLeaf} method, which works by recursing down the tree from the root and choosing the right or left path based on whether the character is represented as a 0 or a 1 in the bitmap of the current node.
The recursion stops when the leaf of the character is found which is indicated by the fact that there are no left or right children of the current node.
After having found the leaf the \textproc{Select} function calculates the \textit{CharBit} based on whether the found leaf is a right or left child of it's parent.

\textproc{SelectRec} is then called on the parent of the leaf and recurses up the tree towards the root to find the position of the specified occurrence of the input character.
It does so by calling \textproc{BinarySelect} which returns the position of the specified occurrence of the \textproc{CharBit} in the bitmap of the current node. 
This position is then used as the occurrence to find the position of in the next recursive call.
When the \textit{root} is reached the result of \textproc{BinarySelect} gives us the position of the specified occurrence of the character in the string.

Select is split into the \textproc{Select} and \textproc{SelectRec} functions because it saves us a check for whether the current node is a leaf or not at each recursive call and since we start out in a leaf and will never meet one again that check would only be true once during the execution.

Figuring out the \textproc{CharBit} is a lot easier for \textproc{Select} than for \textproc{Rank} since we query bottom-up and can just check whether the current node is a left child or a right child. 
In \textproc{Rank} one needs to calculate the split character and then check whether the input character is before or after it in the alphabet in every recursive step.

\subsubsection{Binary Select}
\begin{algorithm}
\caption{BinarySelect}
\label{alg:binaryselect}
\begin{algorithmic}
\Function {BinarySelect} {$Occourance, Bitmap, BitToCount$}
\State $Occ \gets 0$
\For{$bit$ in $Bitmap$}
\If{$bit == BitToCount$}
\If{$Occ == Occourance$}
\State \Return position of $bit$ in $Bitmap$
\EndIf
\State $Occ \gets Occ + 1$
\EndIf
\EndFor
\EndFunction
\end{algorithmic}
\end{algorithm}

\textproc{BinarySelect} returns the position of the i'th occurrence of 1 or 0 (\textproc{CharBit}) in a bitset. 
The simple implementation is to loop through the bitset and when \textit{i} occurrences of either 1 or 0 (depending on whether we are in a left child or a right child) has been seen then return the position of the last one that was seen.
This takes O(N) time making \textproc{BinarySelect} a large bottleneck for \textproc{Select} since it is called for each recursive call in the \textproc{SelectRec} algorithm.
In section~\ref{sec:ImplBinarySelect} we describe how to improve the practical running time of \textproc{BinarySelect} using the \textit{popcount} instruction.

\subsection{Applications}
\subsubsection{Compression}
The Wavelet Tree is used a lot for compression of data. Two of the main compression techniques that are used are specific encodings on bitmaps and changing the shape of the tree \citep[Section~3]{Navjda13}.
The type of encoding on bitmaps that is used most often is entropy coding.

Entropy coding can be explained in two parts: Modelling and Coding.
Modelling assigns probabilities to the symbols of the string, and coding produces a bit sequence from these probabilities.
The probability of a symbol is based on how often it appears in a given data set. 
Higher frequency gives a higher probability.
Coding can then for instance be done with a coding scheme, that uses a discrete number of bits for each symbol, for example Huffman Coding \citep{HuffmanCoding} where symbols that appear with a higher frequency gets a code value with a small number of bits and those symbols that appear with less frequency gets a code value with a higher number of bits.

The main advantage of the wavelet tree with regards to compression is that it supports entropy bounds in the attained space complexity of the various wavelet tree compression methods \citep[Section~2.1]{WTSurvey}.
According to \citep[Section~1]{WTSurvey} entropy can be defined as:

\begin{mdframed}[nobreak, linecolor=lightgray]
\begin{definition}: Entropy \\\\
Let \textit{S} be a sequence of \textit{n} symbols from an alphabet $\Sigma = \lbrace c_1 ... c_\sigma \rbrace$ with alphabet $\sigma$.
Then \textit{entropy} H is defined as
\begin{center}
$H = \sum_{i=1}^{\sigma} p_i log_2(\frac{1}{p_i})$
\end{center}
where $p_i$ is the probability of the i-th symbol in the alphabet appearing in \textit{S}.
\end{definition} 
\end{mdframed}

Entropy represents a lower bound to the average numbers of bits needed to represent each symbol in S according to the coding theorem of Shannon (todo reference) and so is the value that researchers of compression compare their results to.

This theoretical definition of entropy is often replaced in scientific literature by a more practical definition: \textit{empirical entropy}.
There are two versions: \textit{empirical zero-order entropy} $H_0$ and \textit{empirical k-th order entropy} $H_k$. $H_k$ takes the size \textit{k} context of the symbol appearances into account while $H_0$ does not and treats symbols independently instead. 

\begin{mdframed}[nobreak, linecolor=lightgray]
\begin{definition}: \textit{Empirical zero-order entropy}, $H_0$ \\
Let \textit{S} be a sequence of \textit{n} symbols from an alphabet $\Sigma = \lbrace c_1 ... c_\sigma \rbrace$ of cardinality $\sigma$.
Then \textit{entropy} $H_0$ is defined as
\begin{center}
$H_0 = \sum\limits_{c_i \in \Sigma} \frac{n_i}{n} log_2(\frac{n}{n_i})$
\end{center}
where $n_i$ is the number of appearances of character $c_i$ in $S$.
\end{definition}
\begin{definition}: \textit{Empirical k-th order entropy}, $H_k$ \\
For a string $w \in \Sigma^k$ let us define $w_S$ as the subsequence of characters that follow \textit{w} in \textit{S}. 
Then the k-th order empirical entropy of \textit{S}, is defined as follows:
\begin{center}
$H_k(S) = \frac{1}{n} \sum\limits_{w \in \Sigma^k} | w_S |H_0(w_S)$
\end{center}
\end{definition}
\end{mdframed}
$H_k$ thus defines a lower bound for bit space usage that is smaller than the lower bound for $H_0$.
The problem is that it is difficult to achieve compression within the $H_k$ lower bound or at least it was a problem before the wavelet tree was invented. 
Using the Burrows-Wheeler transformation on the input the problem gets reduced from achieving $H_k$ compression to achieving $H_0$ compression.
In other words, if we have a good compression algorithm that achieves compression within the $H_0$ lower bound, then by using that algorithm on the Burrows-Wheeler transformation of the input we can achieve compression within the $H_k$ lower bound. (Todo: explain the theory behind this)

The wavelet tree can be used to both achieve space usage within \textit{zero-order entropy} and \textit{k-th order entropy}. To achieve \textit{zero-order entropy} a Huffman shaped wavelet tree (todo: reference) can be used. But when the alphabet is very large that is not a good approach with regards to compression, because storing the huffman symbol assignments and wavelet tree pointers ends up using too much space . \citep[Section~3]{Claude08practicalrankselect} describes a way to also have \textit{zero-order entropy} space usage for large alphabets. 
We can therefore get space usage within \textit{zero-order entropy} even for large alphabets using the wavelet tree. 

\noindent\textbf{(Todo: describe what a Huffman shaped wavelet tree is)}\\
\textbf{(Todo: describe how the burrows wheeler transformation works and why)} \\
\textbf{(Todo: describe run length encoding)} \\
\textbf{(Todo: Figure out how to do queries when the input is Burrows-Wheeler transformed and the bitmaps are run length encoded.)}





