\section{Applications}
\subsection{What The Wavelet Tree Can Represent}
\textbf{[TODO: redo this subsection]}
The Wavelet Tree has multiple applications that each utilize the wavelet tree differently and use it for storage of, and queries on, different types of data.
These can be split into three main types: A sequence of values, a reordering or permutation, and a grid of points.

Using the Wavelet Tree to store a sequence of values is perhaps the most basic way to utilize the tree.
The Wavelet Tree stores the values and their sequence and supports access, rank, and select queries.

The Wavelet Tree can also be used to describe a stable reordering, e.g. of the symbols in a string S. 
If the leaves are traversed then all the occurrences of the smaller symbols are found first. \textbf{[TODO: describe the use of traversing the leaves like so]}
By tracking the position of a symbol downwards through the Wavelet Tree we will find the new position of the symbol after the permutation.
Tracking the position of a symbol upwards through the wavelet tree will where it is in the original string. 

A Wavelet Tree can also represent an $n \times n$ grid of $n$ points where no two points share the same row or column. 
One can map a general set of $n$ points to such a discrete grid and then store the real points somewhere else.

If we have points sorted by the $x$-coordinate and take only the $y$-coordinates $S_y[1,n] = y_1,y_2,...,y_n$ and save $S_y$ in a Wavelet Tree we can the find the \textit{i}th point in $x$-coordinate order by accessing the corresponding $y$-coordinate in the wavelet tree. 
If we want the \textit{i}th point in $y$-coordinate order we can access the leaf of a given $y$-coordinate and find its corresponding $x$-coordinate by querying up through the tree until we find the original position of $y$ in $S$. 
The corresponding $x$-coordinate will be at the same position.
Querying from a leaf gives the points in $y$-coordinate order because the leaves are sorted by $y$-coordinate.

\subsection{Compression}
\vspace{0.5 cm}
\begin{mdframed}[nobreak, linecolor=lightgray, linewidth=2pt]
\begin{definition}: String representation in a Wavelet Tree\\\\
Let $S[1,n] = S_1 S_2 ... S_n$ be a sequence of symbols where $s_i \in \Sigma$ and $\Sigma = [1 ... \sigma]$ is the alphabet. $S$ can then be represented in plain form using $n \lceil \log \sigma \rceil = n \log \sigma = O(n)$ bits.
\end{definition}
\textbf{[TODO: use and reference this]}
\end{mdframed}
\vspace{0.5 cm}
The Wavelet Tree has many uses for compression of data~\citep{Navjda13}. Some of the main compression techniques are different ways of encoding the bitmaps and changing the shape of the tree~\citep[Section~3]{Navjda13}.

\textbf{[TODO: either move or remove this paragraph]}
Entropy coding can be explained in two parts: Modelling and Coding.
Modelling assigns probabilities to the symbols of the string, and coding produces a bit sequence from these probabilities.
The probability of a symbol is based on how often it appears in a given data set. 
Higher frequency gives a higher probability.
Coding can then for instance be done with a coding scheme, that uses a discrete number of bits for each symbol, for example Huffman Coding~\citep{HuffmanCoding}.

The main advantage of the wavelet tree with regards to compression is that it supports entropy bounds in the attained space complexity of the various wavelet tree compression methods~\citep[Section~2.1]{WTSurvey}.

\subsubsection{Entropy}
According to~\citep[Introduction]{WTSurvey} entropy can be defined as:

\begin{mdframed}[nobreak, linecolor=lightgray, linewidth=2pt]
\begin{definition}: Entropy \\\\
Let \textit{S} be a sequence of \textit{n} symbols from an alphabet $\Sigma = \lbrace c_1 ... c_\sigma \rbrace$ with cardinality $\sigma$.
Then \textit{entropy} H is defined as
\begin{center}
$H = \sum_{i=1}^{\sigma} p_i log_2(\frac{1}{p_i})$
\end{center}
where $p_i$ is the probability of the $i$th symbol in the alphabet appearing in \textit{S}.
\end{definition} 
\end{mdframed}

Entropy represents a lower bound to the average number of bits needed to represent each symbol in $S$ according to the coding theorem of Shannon~\citep[Introduction]{WTSurvey} and is the bound that compression researchers compare their results to.

This theoretical definition of entropy is often replaced in scientific literature by a more practical definition: \textit{empirical entropy}.
There are two versions: \textit{empirical zero-order entropy} $H_0$ and \textit{empirical $k$th order entropy} $H_k$. $H_k$ takes into account a context of size \textit{k} of the symbol appearances, ie. the suffixes of length $k$ of each symbol appearance in the string, while $H_0$ does not and treats symbols independently instead. 

\begin{mdframed}[nobreak, linecolor=lightgray, linewidth=2pt]
\begin{definition}: \textit{Empirical zero-order entropy}, $H_0$ \\
Let \textit{S} be a sequence of \textit{n} symbols from an alphabet $\Sigma = \lbrace c_1 ... c_\sigma \rbrace$.
Then \textit{entropy} $H_0$ is defined as
\begin{center}
$H_0 = H_0(S) = \sum\limits_{c_i \in \Sigma} \frac{n_i}{n} log_2(\frac{n}{n_i})$
\end{center}
where $n_i$ is the number of appearances of character $c_i$ in $S$.
\end{definition}
\begin{definition}: \textit{Empirical k-th order entropy}, $H_k$ \\
For a string $w \in \Sigma^k$ let us define $w_S$ as the concatenation of characters that follow \textit{w} in \textit{S}. 
Then the $k$th order empirical entropy of \textit{S}, is defined as follows
\begin{center}
$H_k = H_k(S) = \frac{1}{n} \sum\limits_{w \in \Sigma^k} | w_S |H_0(w_S)$
\end{center}
\end{definition}
\end{mdframed}
$H_k$ often defines a lower bound for bit space usage that is smaller than the lower bound of $H_0$\citep[Section~2]{waveletTreeEntropy}.

Using the Burrows-Wheeler transformation on the input the we can reduce the problem of achieving $H_k$ compression to achieving $H_0$ compression.
In other words, if we have a good compression algorithm that achieves compression within the $H_0$ lower bound, then by using that algorithm on the Burrows-Wheeler transformation of the input we can achieve compression within the $H_k$ lower bound~\citep[Introduction]{waveletTreeEntropy}.
The problem for a long time was that there existed no good way to achieve compression within the $H_0$ lower bound or at least it was a problem before the wavelet tree was invented~\citep[Introduction]{waveletTreeEntropy}.

To achieve \textit{zero-order entropy} a Huffman shaped wavelet tree can be used~\citep[Section~4]{FMcountOnBWT}. Claude and Navarro~\citep[Section~3]{Claude08practicalrankselect} describes a way to also have \textit{zero-order entropy} space usage for large alphabets. 
It is therefore possible to get space usage within \textit{zero-order entropy} even for large alphabets using the wavelet tree. 
\textit{Zero-order entropy} can also be achieved by run-length encoding the bitmaps in the wavelet tree, which is also the approach that is used when compressing the BWT transformation of the input string using the wavelet tree~\citep[Introduction (\textbf{B})]{waveletTreeEntropy}.
This means that using a combination of run-length encoding of bitmaps and taking the Burrows-Wheeler transformation of the input string using the wavelet tree it is possible to achieve compression near the lower bound of \textit{$k$th order entropy}.

\subsubsection{Run-Length encoding}
Run-length encoding (RLE) is a simple process where the number of consecutive repeats of each symbol is stored instead of storing the symbols themselves. 
If we have the string \textit{aaaaacccaaaaabbbaa} we can run-length encode this to \textit{a5c3a5b3a2} which is a smaller string containing the same information.
It is necessary to store the symbol and its number of consecutive repeating occurrences because we need to be able to identify which symbol occurs where and how many times in order to be able to reproduce the original string.
The longer the sequence of a repeating symbol is the less space is used, since it can be stored as one number plus the related symbol.

When representing the string using a wavelet tree, the problem gets reduced to run-length encoding a string of bits (the bitmap in each node).
Since a binary number only has an alphabet of size two we do not need to store both the symbol and its occurrence but only the occurrence, if we adopt the convention that the first number is always the amount of 0s and the second number is always the amount of 1s, continuing this trend for the entire string so that even-index numbers correspond to 0 and odd-index to 1. 
As an example, if we look at the bitmap of an input string \textit{aaaaacccaaaaabbbaa} which is \textit{000001110000000000} when stored in a wavelet tree, then it can be encoded and stored as the numbers $5~3~10$.
Figure~\ref{fig:RleWaveletTree} shows an example of a wavelet tree with run-length encoded bitmaps.

If we do not consider how a computer saves numbers but only consider the amount of numbers that needs to be stored then $RLE(000001110000000000)$ achieves a great reduction in space.
From 18 numbers to only 3 numbers, which contain the same information.

If we do consider how a computer saves numbers then the reduction is not that great because if we assume that each number is represented as an integer, then the run-length encoded bitmap uses more space than just storing the original bitmap. 
This is because an integer uses 4 bytes of space which is 32 bits and we need to store three integers giving us a total of $32 bits \times 3 = 96 bits$ which is significantly larger than just the 18 bits we need to store for the original bitmap assuming we can store 1 and 0 using only 1 bit \footnote{This can be accomplished using C++ and Vector$<$bool$>$}. 
This means that the symbols in the string need to, on average, repeat consecutively more than 32 times before RLE achieves better space usage than just storing the bitmaps.
This is assuming 4-byte integers are used to store the RLE values. 
To be able to support storing the RLE of any bitmap, even one containing only 0s or only 1s, the RLE values should be able to store as high a value as the bitmap is long, which might require more bytes per value.
A 32-bit unsigned integer supports storing a value up to $2^{32} = \num{4294967296}$. 
The limit on bitmap length is also the maximum supported length of input string for a wavelet tree.

RLE is still useful despite of this limitation because you usually want to compress massive amounts of data and if that data uses an alphabet that is small enough then, as previously stated RLE can achieve compression close to the zero order entropy when working with binary alphabets.

If the Burrows-Wheeler transformation is applied to the string before it is saved in the wavelet tree and run-length encoded then the number of consecutive repeats of a symbol is increased which enables even greater compression.

\subsubsection{Burrows-Wheeler transformation}
The Burrows-Wheeler transformation transforms a string $S$ into a string of the same length with the same characters with the characteristic that characters are grouped into runs of similar characters.
This characteristic enables higher compression ratios when using techniques such as run-length encoding.
The transformation is reversible, meaning it is possible to produce the original string from the Burrows-Wheeler transformed string, without any other information.
Sorting $S$ would enable similar, or possibly better, compression ratios using run-length encoding, but it will not be reversible.

A string $S$ of $n$ characters is transformed by the Burrows-Wheeler transformation~\citep{BWToriginalArticle} by forming $n$ cyclic shifts of $S$. 
These $n$ permutations of $S$ are then sorted in lexicographical order.
An extra character (\#), not in the alphabet of $S$, is added to keep track of the end of the original string.
The BWT of $S$ is then the concatenation of the last character of each permutation in sorted order, excluding $\#$.

\begin{figure}
\begin{center}
$\begin{bmatrix}
	bananahat\#\\
	ananahat\#b\\
	nanahat\#ba\\
	anahat\#ban\\
	nahat\#bana\\
	ahat\#banan\\
	hat\#banana\\
	at\#bananah\\
	t\#bananaha\\
	\#bananahat\\
\end{bmatrix} \Rightarrow
\begin{bmatrix}
	\#bananaha\textbf{t}\\
	ahat\#bana\textbf{n}\\
	anahat\#ba\textbf{n}\\
	ananahat\#\textbf{b}\\
	at\#banana\textbf{h}\\
	bananahat\#\\
	hat\#banan\textbf{a}\\
	nahat\#ban\textbf{a}\\
	nanahat\#b\textbf{a}\\
	t\#bananah\textbf{a}
\end{bmatrix}$
\end{center}
\caption{Example of a Burrows-Wheeler transformation of the string \textit{bananahat}}
\label{fig:BWTExample}
\end{figure}
In Figure~\ref{fig:BWTExample} we present an example transformation of the string \textit{bananahat}.
The list to the left in Figure~\ref{fig:BWTExample} is the cyclically shifted permutations of \textit{S} and the list to the right contains the same permutations, but in lexicographically sorted order.
The result of the Burrows-Wheeler transformation is then the characters at the last index in each column, highlighted in bold in Figure~\ref{fig:BWTExample}.
The Burrows-Wheeler transformation of $S=bananahat$ becomes BWT($S$)$ = tnnbhaaaa$.
The original string is identified by having a \# at the end.

Looking at BWT($S$) we can see that equal characters are now grouped together.
It would not make much sense to compress something without being able to decompress it again.
Burrows et al.~\citep[Section~2]{BWToriginalArticle} describes an algorithm for getting the original string from the Burrows-Wheeler transformed string.

\textbf{[TODO: Describe, possibly with example, how to reverse the BWT]}

Figure~\ref{fig:RleBwtComparison} shows two small examples of wavelet trees using run-length encoding, one constructed on the string \textit{bananahat} (Figure~\ref{fig:RleWaveletTree}) and the other on the Burrows-Wheeler Transformation of \textit{bananahat} (Figure~\ref{fig:BwtRleWaveletTree}).
We can see in Figure~\ref{fig:BwtRleWaveletTree}, highlighted by the numbers in bold, that fewer Run-length encoded values needs to be stored than for the non-Burrows-Wheeler transformed string in Figure~\ref{fig:RleWaveletTree}.
\begin{figure}
      \begin{subfigure}{0.45\textwidth}      
			\Tree
			%root
			[.bananahat\\2,1,1,1,3,1 !\qsetw{3cm} 
				%left child
				[.baaaha\\1,2,1,1 !\qsetw{3cm}
					[.baaaa\\0,1,4 !\qsetw{3cm}
						[.aaaa !\qsetw{3cm} ]
						[.b !\qsetw{3cm} ]		
					] 
					[.h !\qsetw{3cm} ]
				] 
				%right child
				[.nnt\\2,1 !\qsetw{3cm}	
					[.nn !\qsetw{3cm} ] 
					[.t !\qsetw{3cm} ]
				]
			]
		\caption{RLE Wavelet Tree on string \textit{bananahat} with alphabet $\sigma = abhnt$}
      \label{fig:RleWaveletTree}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.45\textwidth}	
			\Tree
			%root
			[.tnnbhaaaa\\\textbf{0,3,6} !\qsetw{3cm} 
				%left child
				[.bhaaaa\\\textbf{1,1,4} !\qsetw{3cm} 
					[.baaaa\\0,1,4 !\qsetw{3cm} 
						[.aaaa !\qsetw{3cm} ]
						[.b !\qsetw{3cm} ]		
					] 
					[.h !\qsetw{3cm} ]
				] 
				%right child
				[.tnn\\0,1,2 !\qsetw{3cm}		
					[.nn !\qsetw{3cm} ] 
					[.t !\qsetw{3cm} ]
				]
			] 
		\caption{RLE Wavelet Tree on $BWT(bananahat)=tnnbhaaaa$ with alphabet $\sigma = abhnt$}
		\label{fig:BwtRleWaveletTree}
	\end{subfigure}
	\caption{Comparison of Wavelet Trees using Run-Length Encoding on a string and its Burrows-Wheeler Transformation}
	\label{fig:RleBwtComparison}
\end{figure}

One might wonder how \textproc{Rank} and \textproc{Select} queries can be useful when the input string is Burrows-Wheeler transformed, as the results of the queries become essentially unrelated to the original string $S$, without an obvious way of transforming the query results back to what they would have been on a tree constructed on the original non-transformed string.
We have not found any way to transform the results back, and the main use of constructing a Wavelet Tree on the BWT of a string seems to us, by far, to be compression.

Rank and select queries on the BWT of a string does, however, have uses when working with the \textit{FM-index}, named after its inventors Paolo Ferragina and Giovanni Manzini, which is a self-index based on the Burrows-Wheeler transformation $BWT(S)$ that is able to find occurrences and positions of patterns (sub-strings) in \textit{S} by looking at $BWT(S)$. 
Such a procedure is described in~\citep[Section 2]{FMcountOnBWT}.

\subsubsection{Huffman-shaped Wavelet Trees}
\citep[Section~4]{FMcountOnBWT} describes a Huffman Shaped Wavelet Tree skews the tree to one side and places symbols with higher frequencies towards the other side so that they are closer to the root than those that have a lower frequency.
More precisely, they are placed in the tree in such a way that the path from the root to a leaf corresponds to the binary Huffman Code~\citep[Introduction]{HuffmanCoding} of the symbol of that leaf.
Using a Huffman Shaped Wavelet Tree is an alternative to run-length encoding.

This approach skews the tree and as a result increases the height of the tree, which ordinarily would result in higher average query time, but by placing the most frequent symbols highest and least frequent symbols lowest, it decreases query time massively for symbols with high frequency.
Queries on a Huffman shaped Wavelet Tree for a symbol that has a high frequency then returns faster than queries for a symbol that was less frequent.
Assuming symbols that occur with high frequency are also queried for more often, the average query time could see a reduction when using a Huffman-shaped wavelet tree.

The Huffman Code~\citep[Introduction]{HuffmanCoding} of a symbol occurring with high frequency is a shorter binary string than the Huffman code of a symbol occurring with low frequency.
The most frequent symbol could be encoded in as little as one bit!
This entails that the storage space required for the many occurrences of the most frequent symbols would be massively reduced, while the space required for the least frequent symbols would be increased.
If the difference in frequency is sufficiently high, the reduction in space for the most frequent symbols would outweigh the increase in space for the least frequent and the overall storage requirement would be reduced.

Since the Huffman encoding is based on frequency of symbols it achieves the best performance and space complexity when symbols are non-uniformly distributed.
If the data is uniformly distributed then the length of all Huffman codes would be similar resulting in a balanced tree having performance and space complexity similar to a normal Wavelet Tree.
Since we have primarily used uniformly distributed data in our experiments, and because of limited time, we have not implemented and done experiments on a Huffman Shaped Wavelet Tree.
In Section~\ref{sec:choiceOfInputString} we discuss further why we mainly consider uniform data.


\subsection{Information Retrieval}
\textbf{[Perhaps TODO: Wavelet tree as a Range Searching Data Structure]}


\subsubsection{Range Quantile Query}
A range quantile query is a query that returns the $k$th smallest number within a subsequence of a given sequence of elements.
So if we are e.g. given a list of price changes on a laptop during the last year then a range quantile query is able to answer what the $k$-smallest price of the laptop was within for instance a month of that year.
It is therefore also easy to find quantiles like the 2-quantile (median) or the 3-quantile.
To e.g. find the median, $k$ can be defined as half the length of the subsequence.
This would return the middle element of the subsequence.
The 3-Quantile can be found by setting $k$ to $\frac{1}{3}$ of the length of the subsequence. 
Quantiles are important values within such fields as statistics and economics.

The wavelet tree can be used to support efficient range quantile queries on a sequence \textit{S} of \textit{n} numbers in $O(\log(\sigma))$ time if $rank_b$ is supported in $O(1)$ time~\citep[Section 3]{RangeQuantileQueryWT}. 
The range is denoted as $S[l ... r]$.
A range quantile query based on a wavelet tree works by computing two rank queries on the bitmap of each node in a traversal from the root to a leaf node. 

\begin{algorithm}
\caption{Range Quantile Query}
\label{alg:RangeQuantileQuery}
\begin{algorithmic}
\Function {RangeQuantileQuery} {$k,l,r$}
\If{current node is leaf}
	\State \Return number in leaf
\EndIf
\State $\textit{0}sInRange \gets rank_0(S \lbrace l...r \rbrace) = rank_0(r) - rank_0(l-1)$
\If{$\textit{0}sInRange \leq k$}
	\State $l = rank_0(l-1))+1$
	\State $r = rank_0(r)$
	\State \Return $LeftNode.RangeQuantileQuery(k,l,r)$
\Else
	\State $k = k - \textit{0}sInRange)$
	\State $l = rank_1(l-1)+1$
	\State $r = rank_1(r)$
	\State \Return $RightNode.RangeQuantileQuery(k,l,r)$
\EndIf

\EndFunction
\end{algorithmic}
\end{algorithm}

The two queries are $rank_b(l-1)$ and $rank_b(r)$ where $rank_b$ is the binary rank.
$rank_b(l-1)$ is used to find the number of 1s and 0s in $b[1...(l-1)]$ and $rank_b(r) - rank_b(l-1)$ gives the number 1s and 0s in $b[l...r]$. 
We go to the left if there are more than \textit{k} 0s in $b[l...r]$ and set $l = ($number of 0s in $b[1...(l-1)])+1$ and $r=($number of 0s in $b[1...r])$. 
We go to the right if there are less than \textit{k} 0s in $b[l...r]$ and subtract the number of 0s in $b[l...r]$ from \textit{k} and set $l = ($number of 1s in $b[1...(l-1)])+1$ and set $r=($number of 1s in $b[1...r])$. 
This procedure continues recursively until it hits a leaf and then returns the number stored in the leaf which corresponds to the $k$th smallest number in $S[l...r]$.

Algorithm~\ref{alg:RangeQuantileQuery} describes the pseudo-code for a range quantile query where $rank_1$ is a binary rank query which returns the number of 1s in the bitmap of $S$ in each node generated by the wavelet tree. 
$rank_0$ return the number of 0s.
The argument to $rank_0$ and $rank_1$ is the position to find the number of occurrences up to.
This means that $rank_1(r)$ for instance returns the number of 1s in the bitmap until position $r$.

\figureBegin
\begin{minipage}{.45\linewidth}
\begin{flushleft}                           
\Tree
%root
[.6,2,\textbf{0,7,9,3,1,8,5},4\\1001100110 !\qsetw{5cm} 
	%left child
	[.2,0,3,1,4\\00101  !\qsetw{5cm}
		%left -> left
		[.2,0,1\\100 !\qsetw{5cm} 
			[.0,1\\01 
				[.0 ]
				[.1 ] 			
			]
			[.2 ]		
		] 
		%left -> right
		[.3,4\\01 !\qsetw{5cm} 
			[.3 ]
			[.4 ]		
		]
	] 
	%right child
	[.6-\textbf{7,9,8,5}\\00110 !\qsetw{5cm} 
		%right -> left
		[.6,\textbf{7,5}\\010 !\qsetw{5.3cm}
			%right -> left -> left
			[.6,5\\10 
				[.5 ]
				[.6 ]			
			] 
			%right -> left -> right
			[.\textbf{7} ] 
		]
		%right -> right	 
		[.9,8\\10 !\qsetw{5.3cm} 
			[.8 ]
			[.9 ]		
		]
	]
] 
\end{flushleft} 
\end{minipage}
\hfill
\begin{minipage}{.45\linewidth}
\begin{flushright}
Level 1:\\$k=5$\\$l=3$\\$r=9$\\ \vspace{0.5cm}
Level 2:\\$k=2$\\$l=2$\\$r=5$\\ \vspace{0.5cm}
level 3:\\$k=2$\\$l=2$\\$r=3$\\ \vspace{0.5cm}
Level 4:\\$k=1$\\$l=1$\\$r=1$
\end{flushright} 
\end{minipage}
\caption{Range Quantile Query on a Wavelet Tree. $S=\lbrace 6,2,0,7,9,3,1,8,5,4 \rbrace, k=5, l=3, r=9$.}
\label{fig:RQQWaveletTree}
\figureEnd

An example of a range quantile query can be seen in Figure~\ref{fig:RQQWaveletTree}. 
The numbers in bold indicates the range $S[l...r]$ where $S= \lbrace 6,2,0,7,9,3,1,8,5,4 \rbrace$ and $l=3$, $r=9$ and $k=5$.
$k=5$ means that we are looking for the 5th smallest number within $S[l...r]$ which is \textit{7} indicated by the leaf that the query ends up in before terminating.
$l$ and $r$ indicates the range to look within.
The right side of the figure shows how \textit{k, l} and \textit{r} develops in each recursive call of the Range Quantile Query.