\section{Applications}
\subsection{What The Wavelet Tree Can Represent}
\textbf{[TODO: redo this subsection]}
The Wavelet Tree has multiple applications that each utilize the wavelet tree differently and use it for storage of, and queries on, different types of data.
These can be split into three main types: A sequence of values, a reordering or permutation, and a grid of points.

Using the Wavelet Tree to store a sequence of values is perhaps the most basic way to utilize the tree.
The Wavelet Tree stores the values and their sequence and supports access, rank, and select queries.

The Wavelet Tree can also be used to describe a stable reordering, e.g. of the symbols in a string S. 
If the leaves are traversed then all the occurrences of the smaller symbols are found first. \textbf{[TODO: describe the use of traversing the leaves like so]}
By tracking the position of a symbol downwards through the Wavelet Tree we will find the new position of the symbol after the permutation.
Tracking the position of a symbol upwards through the wavelet tree will where it is in the original string. 

A Wavelet Tree can also represent a $n \times n$ grid of $n$ points where no two points share the same row or column. 
One can map a general set of $n$ points to such a discrete grid and then store the real points somewhere else.

If we have points sorted by the $x$-coordinate and take only the $y$-coordinates $S_y[1,n] = y_1,y_2,...,y_n$ and save $S_y$ in a Wavelet Tree we can the find the \textit{i}th point in $x$-coordinate order by accessing the corresponding $y$-coordinate in the wavelet tree. 
If we want the \textit{i}th point in $y$-coordinate order we can access the leaf of a given $y$-coordinate and find its corresponding $x$-coordinate by querying up through the tree until we find the original position of $y$ in $S$. 
The corresponding $x$-coordinate will be at the same position.
Querying from a leaf gives the points in $y$-coordinate order because the leaves are sorted by $y$-coordinate.

\subsection{Compression}
\vspace{0.5 cm}
\begin{mdframed}[nobreak, linecolor=lightgray, linewidth=2pt]
\begin{definition}: String representation in a Wavelet Tree\\\\
Let $S[1,n] = S_1 S_2 ... S_n$ be a sequence of symbols where $s_i \in \Sigma$ and $\Sigma = [1 .. \sigma]$ is the alphabet. $S$ can then be represented in plain form using $n \lceil \log \sigma \rceil = n \log \sigma = O(n)$ bits.
\end{definition}
\textbf{[TODO: use and reference this]}
\end{mdframed}
\vspace{0.5 cm}
The Wavelet Tree has many uses for compression of data~\citep{Navjda13}. Some of the main compression techniques are different ways of encoding the bitmaps and changing the shape of the tree \citep[Section~3]{Navjda13}.

\textbf{[TODO: either move or remove this paragraph]}
Entropy coding can be explained in two parts: Modelling and Coding.
Modelling assigns probabilities to the symbols of the string, and coding produces a bit sequence from these probabilities.
The probability of a symbol is based on how often it appears in a given data set. 
Higher frequency gives a higher probability.
Coding can then for instance be done with a coding scheme, that uses a discrete number of bits for each symbol, for example Huffman Coding \citep{HuffmanCoding}.

The main advantage of the wavelet tree with regards to compression is that it supports entropy bounds in the attained space complexity of the various wavelet tree compression methods \citep[Section~2.1]{WTSurvey}.

\subsubsection{Entropy}
According to \citep[Introduction]{WTSurvey} entropy can be defined as:

\begin{mdframed}[nobreak, linecolor=lightgray, linewidth=2pt]
\begin{definition}: Entropy \\\\
Let \textit{S} be a sequence of \textit{n} symbols from an alphabet $\Sigma = \lbrace c_1 ... c_\sigma \rbrace$ with cardinality $\sigma$.
Then \textit{entropy} H is defined as
\begin{center}
$H = \sum_{i=1}^{\sigma} p_i log_2(\frac{1}{p_i})$
\end{center}
where $p_i$ is the probability of the i-th symbol in the alphabet appearing in \textit{S}.
\end{definition} 
\end{mdframed}

Entropy represents a lower bound to the average number of bits needed to represent each symbol in $S$ according to the coding theorem of Shannon~\citep[Introduction]{WTSurvey} and is the bound that compression researchers compare their results to.

This theoretical definition of entropy is often replaced in scientific literature by a more practical definition: \textit{empirical entropy}.
There are two versions: \textit{empirical zero-order entropy} $H_0$ and \textit{empirical k-th order entropy} $H_k$. $H_k$ takes into account a context of size \textit{k} of the symbol appearances, ie. the suffixes of length $k$ of each symbol appearance in the string, while $H_0$ does not and treats symbols independently instead. 

\begin{mdframed}[nobreak, linecolor=lightgray, linewidth=2pt]
\begin{definition}: \textit{Empirical zero-order entropy}, $H_0$ \\
Let \textit{S} be a sequence of \textit{n} symbols from an alphabet $\Sigma = \lbrace c_1 ... c_\sigma \rbrace$.
Then \textit{entropy} $H_0$ is defined as
\begin{center}
$H_0 = H_0(S) = \sum\limits_{c_i \in \Sigma} \frac{n_i}{n} log_2(\frac{n}{n_i})$
\end{center}
where $n_i$ is the number of appearances of character $c_i$ in $S$.
\end{definition}
\begin{definition}: \textit{Empirical k-th order entropy}, $H_k$ \\
For a string $w \in \Sigma^k$ let us define $w_S$ as the concatenation of characters that follow \textit{w} in \textit{S}. 
Then the k-th order empirical entropy of \textit{S}, is defined as follows
\begin{center}
$H_k = H_k(S) = \frac{1}{n} \sum\limits_{w \in \Sigma^k} | w_S |H_0(w_S)$
\end{center}
\end{definition}
\end{mdframed}
$H_k$ often defines a lower bound for bit space usage that is smaller than the lower bound of $H_0$\citep[Section~2]{waveletTreeEntropy}.

Using the Burrows-Wheeler transformation on the input the we can reduce the problem of achieving $H_k$ compression to achieving $H_0$ compression.
In other words, if we have a good compression algorithm that achieves compression within the $H_0$ lower bound, then by using that algorithm on the Burrows-Wheeler transformation of the input we can achieve compression within the $H_k$ lower bound~\citep[Introduction]{waveletTreeEntropy}.
The problem for a long time was that there existed no good way to achieve compression within the $H_0$ lower bound or at least it was a problem before the wavelet tree was invented~\citep[Introduction]{waveletTreeEntropy}.

To achieve \textit{zero-order entropy} a Huffman shaped wavelet tree can be used~\citep[Section~4]{FMcountOnBWT}. Claude and Navarro~\citep[Section~3]{Claude08practicalrankselect} describes a way to also have \textit{zero-order entropy} space usage for large alphabets. 
It is therefore possible to get space usage within \textit{zero-order entropy} even for large alphabets using the wavelet tree. 
\textit{Zero-order entropy} can also be achieved by run-length encoding the bitmaps in the wavelet tree, which is also the approach that is used when compressing the \textit{bwt} transformation of the input string using the wavelet tree \citep[Introduction (\textbf{B})]{waveletTreeEntropy}.
This means that using a combination of run-length encoding of bitmaps and taking the Burrows-Wheeler transformation of the input string using the wavelet tree it is possible to achieve compression near the lower bound of \textit{k-th order entropy}.

\subsubsection{Run-Length encoding}
Run-length encoding (RLE) is a simple process where the number of consecutive repeats of each symbol is stored in stead of storing the symbols them selves. 
If we have the string \textit{aaaaacccaaaaabbbaa} we can run-length encode this to \textit{a5c3a5b3a2} which is a smaller string containing the same information.
It is necessary to store the symbol and it's number of consecutive repeating occurrences because we need to be able to identify which symbol occurs where and how many times in order to be able to reproduce the original string.
The longer the sequence of a repeating symbol is the less space is used, since it can be stored as one number plus the related symbol.

When representing the string using a wavelet tree, the problem gets reduced to run-length encoding a string of bits (the bitmap in each node).
Since a binary number only has an alphabet with the cardinality of two we do not need to store both the symbol and its occurrence but only the occurrence.
If we define that the first number is always the amount of 0s and the second number is always the amount of 1s and so forth. 
Then by looking at the bitmap of \textit{aaaaacccaaaaabbbaa} which is \textit{000001110000000000} when stored in a wavelet tree we can encode it into $(5,3,10)$.
Figure~\ref{fig:RleWaveletTree} shows an example of a wavelet tree with run-length encoded bitmaps.

If we do not consider how a computer saves numbers but only consider the amount of numbers that needs to be stored then $RLE(000001110000000000)$ achieves a great reduction in space.
From 18 numbers to only 3 numbers, which contain the same information.

If we do consider how a computer saves numbers then the reduction is not that great because if we assume that each number is represented as an integer, then the run-length encoded bitmap uses more space than just storing the original bitmap. 
This is because an integer uses 4 bytes of space which is 32 bits and we need to store three integers giving us a total of $32 bits \times 3 = 96 bits$ which is significantly larger than just the 18 bits we need to store for the original bitmap assuming we can store 1 and 0 using only 1 bit \footnote{This can be accomplished using C++ and Vector$<$bool$>$}. 
This means that the symbols in the string need to, on average, repeat consecutively more than 32 times before RLE achieves better space usage than just storing the bitmaps.

RLE is still very useful despite of this because you usually want to compress very large data and if that data uses an alphabet that is small enough then, as previously stated RLE can achieve compression close to the \textit{zero order entropy} when working with binary alphabets.

If the Burrows-Wheeler transformation is applied to the string before it is saved in the wavelet tree and run-length encoded then the number of consecutive repeats of a symbol is increased which enables even greater compression.

\subsubsection{Burrows-Wheeler transformation}
Having a string \textit{S} of \textit{n} characters the Burrows-Wheeler transformation \citep{BWToriginalArticle} transforms \textit{S} by forming \textit{n} cyclic shifts of \textit{S}. 
These \textit{n} permutations of \textit{S} are then sorted in lexicographical order.
The \# is added to keep track of the end of the original string.
The \textit{bwt} of \textit{S} is then the concatenation of the last characters in each permutation excluding $\#$.

\begin{center}
$\begin{bmatrix}
	bananahat\#\\
	ananahat\#b\\
	nanahat\#ba\\
	anahat\#ban\\
	nahat\#bana\\
	ahat\#banan\\
	hat\#banana\\
	at\#bananah\\
	t\#bananaha\\
	\#bananahat\\
\end{bmatrix} \Rightarrow
\begin{bmatrix}
	\#bananaha\textbf{t}\\
	ahat\#bana\textbf{n}\\
	anahat\#ba\textbf{n}\\
	ananahat\#\textbf{b}\\
	at\#banana\textbf{h}\\
	bananahat\#\\
	hat\#banan\textbf{a}\\
	nahat\#ban\textbf{a}\\
	nanahat\#b\textbf{a}\\
	t\#bananah\textbf{a}
\end{bmatrix}$
\end{center}

The list to the left is the permuted versions of \textit{S} and the list to the right is the lexicographically sorted permutations of \textit{S}.
Taking the character at the last index in each column the Burrows-Wheeler transformation of $S=bananahat$ becomes $bwt(S) = tnnbhaaaa$. The index of the the original string in the right list is also returned. 
The original string is identified by having a \# at the end.

Looking at \textit{bwt(S)} we can see that equal characters are now grouped together.
This effect enables better compression when using compression techniques like run-length encoding.
One could argue that simply sorting \textit{S} would enable better compression using run-length encoding, which is true, but a sorted sequence is not reversible, which BWT is.
It would not make very much sense to compress something without being able to decompress it again.
Burrows et al.~\citep[Section~2]{BWToriginalArticle} describes an algorithm for getting the original string from the Burrows-Wheeler transformed string.

Figure~\ref{fig:BwtRleWaveletTree} depicts a RLE Wavelet tree on \textit{bwt(abcbadbd)}. We can see, as indicated by the numbers in bold that fewer Run-length encoded values needs to be stored than for the non-Burrows-Wheeler transformed case as shown in figure~\ref{fig:RleWaveletTree}.
\figureBegin
      \begin{minipage}{0.45\textwidth}      
			\Tree
			%root
			[.bananahat\\2,1,1,1,3,1 !\qsetw{3cm} 
				%left child
				[.baaaha\\1,2,1,1 !\qsetw{3cm}
					[.baaaa\\0,1,4 !\qsetw{3cm}
						[.aaaa !\qsetw{3cm} ]
						[.b !\qsetw{3cm} ]		
					] 
					[.h !\qsetw{3cm} ]
				] 
				%right child
				[.nnt\\2,1 !\qsetw{3cm}	
					[.nn !\qsetw{3cm} ] 
					[.t !\qsetw{3cm} ]
				]
			]
		\caption{RLE Wavelet Tree on string \textit{bananahat} with alphabet $\sigma = abhnt$}
      \label{fig:RleWaveletTree}
	\end{minipage}
	\hfill
	\begin{minipage}{0.45\textwidth}	
			\Tree
			%root
			[.tnnbhaaaa\\\textbf{0,3,6} !\qsetw{3cm} 
				%left child
				[.bhaaaa\\\textbf{1,1,4} !\qsetw{3cm} 
					[.baaaa\\0,1,4 !\qsetw{3cm} 
						[.aaaa !\qsetw{3cm} ]
						[.b !\qsetw{3cm} ]		
					] 
					[.h !\qsetw{3cm} ]
				] 
				%right child
				[.tnn\\0,1,2 !\qsetw{3cm}		
					[.nn !\qsetw{3cm} ] 
					[.t !\qsetw{3cm} ]
				]
			] 
		\caption{RLE Wavelet Tree on $BWT(bananahat)=tnnbhaaaa$ with alphabet $\sigma = abhnt$}
		\label{fig:BwtRleWaveletTree}
	\end{minipage}
\figureEnd

One might wonder how \textproc{Rank} and \textproc{Select} queries can be useful when the string is Burrows-Wheeler transformed and therefore permuted resulting in the queries values that is essentially unrelated to the original string \textit{S}. 

The queries does have uses when working with the \textit{FM-index}, named after its inventors Paolo Ferragina and Giovanni Manzini, which is a self-index based on the Burrows-Wheeler transformation $BWT(S)$ that is able to find occurrences and positions of patterns (sub-strings) in \textit{S} by looking at $BWT(S)$. 
Such a procedure is described in \citep[Section 2]{FMcountOnBWT}.

\subsubsection{Huffman-shaped Wavelet Trees}
\citep[Section~4]{FMcountOnBWT} describes a Huffman Shaped Wavelet Tree which places symbols with higher frequencies closer to the root than those that have a lower frequency, in such a way that the path from root to symbol leaf corresponds to the binary Huffman code of the symbol. Using a Huffman Shaped Wavelet Tree is an alternative to run-length encoding.
The Huffman code of a symbol is a small binary value for symbols with high frequency and a larger binary value for symbols with the small frequency.
This approach would make the tree skewed and as a result increase the height of the tree but decrease query time for symbols with high frequency.
If one then queries the Huffman shaped Wavelet Tree for a symbol that has a high frequency then one would find it faster than if one was looking for a symbol that was less frequent.

Since the Huffman encoding is based on frequency of symbols it achieves the best performance and space complexity when symbols are non-uniformly distributed.
If the data is uniformly distributed then the length of all Huffman codes would be similar resulting in a balanced tree having performance and space complexity similar to a normal Wavelet Tree.
Since we have primarily used uniformly distributed data in our experiments, and because of limited time, we have not implemented and done experiments on a Huffman Shaped Wavelet Tree.
In section~\ref{sec:choiceOfInputString} we discuss why we mainly consider uniform data.


\subsection{Information Retrieval}
\textbf{[Perhaps TODO: Wavelet tree as a Range Searching Data Structure]}


\subsubsection{Range Quantile Query}
A range quantile query is a query that returns the $k$th smallest number within a subrange of a given sequence of elements.
So if we are e.g. given a list of price changes on a laptop during the last year then a range quantile query is able to answer what the smallest or third smallest (indicated by the k value) price of the laptop was within for instance a month of that year.
It is therefore also quite easy to find quantiles like the 2-quantile (median) or the 3-quantile. 
To e.g. find the median, $k$ can be defined as half the length of the range of the sequence.
This would return the middle element of the subsequence.
The 3-Quantile can be found by setting $k$ to $\frac{1}{3}$ of the length of the subsequence. 
Quantiles are important values and are used a lot in statistics.

The wavelet tree can be used to support efficient range quantile queries on a sequence \textit{S} of \textit{n} numbers in $O(\log(\sigma))$ time if $rank_b$ is supported in $O(1)$ time \citep[Section 3]{RangeQuantileQueryWT}. 
The range is denoted as $S[l ... r]$.
A range quantile query based on a wavelet tree works by computing two rank queries on the bitmap of each node in a traversal from the root to a leaf node. 

\begin{algorithm}
\caption{Range Quantile Query}
\label{alg:RangeQuantileQuery}
\begin{algorithmic}
\Function {RangeQuantileQuery} {$k,l,r$}
\If{current node is leaf}
	\State \Return number in leaf
\EndIf
\State $\textit{0}sInRange \gets rank_0(S \lbrace l..r \rbrace) = rank_0(r) - rank_0(l-1)$
\If{$\textit{0}sInRange \leq k$}
	\State $l = rank_0(l-1))+1$
	\State $r = rank_0(r)$
	\State \Return $LeftNode.RangeQuantileQuery(k,l,r)$
\Else
	\State $k = k - \textit{0}sInRange)$
	\State $l = rank_1(l-1)+1$
	\State $r = rank_1(r)$
	\State \Return $RightNode.RangeQuantileQuery(k,l,r)$
\EndIf

\EndFunction
\end{algorithmic}
\end{algorithm}

The two queries are $rank_b(l-1)$ and $rank_b(r)$ where $rank_b$ is the binary rank.
$rank_b(l-1)$ is used to find the number of 1s and 0s in $b[1..(l-1)]$ and $rank_b(r) - rank_b(l-1)$ gives the number 1s and 0s in $b[l..r]$. 
We go to the left if there are more than \textit{k} 0s in $b[l..r]$ and set $l = ($number of 0s in $b[1..(l-1)])+1$ and $r=($number of 0s in $b[1..r])$. 
We go to the right if there are less than \textit{k} 0s in $b[l..r]$ and subtract the number of 0s in $b[l..r]$ from \textit{k} and set $l = ($number of 1s in $b[1..(l-1)])+1$ and set $r=($number of 1s in $b[1..r])$. 
This procedure continues recursively until it hits a leaf and then returns the number stored in the leaf which corresponds to the $k$th smallest number in $S[l..r]$.

Algorithm~\ref{alg:RangeQuantileQuery} describes the pseudo-code for a range quantile query where $rank_1$ is a binary rank query which returns the number of 1s in the bitmap of $S$ in each node generated by the wavelet tree. 
$rank_0$ return the number of 0s.
The argument to $rank_0$ and $rank_1$ is the position to find the number of occurrences up to.
This means that $rank_1(r)$ for instance returns the number of 1s in the bitmap until position $r$.

\figureBegin
\begin{minipage}{.45\linewidth}
\begin{flushleft}                           
\Tree
%root
[.6,2,\textbf{0,7,9,3,1,8,5},4\\1001100110 !\qsetw{5cm} 
	%left child
	[.2,0,3,1,4\\00101  !\qsetw{5cm}
		%left -> left
		[.2,0,1\\100 !\qsetw{5cm} 
			[.0,1\\01 
				[.0 ]
				[.1 ] 			
			]
			[.2 ]		
		] 
		%left -> right
		[.3,4\\01 !\qsetw{5cm} 
			[.3 ]
			[.4 ]		
		]
	] 
	%right child
	[.6-\textbf{7,9,8,5}\\00110 !\qsetw{5cm} 
		%right -> left
		[.6,\textbf{7,5}\\010 !\qsetw{5.3cm}
			%right -> left -> left
			[.6,5\\10 
				[.5 ]
				[.6 ]			
			] 
			%right -> left -> right
			[.\textbf{7} ] 
		]
		%right -> right	 
		[.9,8\\10 !\qsetw{5.3cm} 
			[.8 ]
			[.9 ]		
		]
	]
] 
\end{flushleft} 
\end{minipage}
\hfill
\begin{minipage}{.45\linewidth}
\begin{flushright}
Level 1:\\$k=5$\\$l=3$\\$r=9$\\ \vspace{0.5cm}
Level 2:\\$k=2$\\$l=2$\\$r=5$\\ \vspace{0.5cm}
level 3:\\$k=2$\\$l=2$\\$r=3$\\ \vspace{0.5cm}
Level 4:\\$k=1$\\$l=1$\\$r=1$
\end{flushright} 
\end{minipage}
\caption{Range Quantile Query on a Wavelet Tree. $S=\lbrace 6,2,0,7,9,3,1,8,5,4 \rbrace, k=5, l=3, r=9$.}
\label{fig:RQQWaveletTree}
\figureEnd

An example of a range quantile query can be seen in figure~\ref{fig:RQQWaveletTree}. 
The numbers in bold indicates the range $S[l..r]$ where $S= \lbrace 6,2,0,7,9,3,1,8,5,4 \rbrace$ and $l=3$, $r=9$ and $k=5$.
$k=5$ means that we are looking for the 5th smallest number within $S[l..r]$ which is \textit{7} indicated by the leaf that the query ends up in before terminating.
$l$ and $r$ indicates the range to look within.
The right side of the figure shows how \textit{k, l} and \textit{r} develops in each recursive call of the Range Quantile Query.