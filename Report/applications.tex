\section{Applications}
\subsection{What The Wavelet Tree Can Represent}
The Wavelet Tree has multiple applications that each utilize the wavelet tree differently and use it for storage of, and queries on, different types of data.
These applications uses the wavelet tree to achieve different representations which can be split into three main types: A sequence of values, a reordering or permutation, and a grid of points.

Using the Wavelet Tree to store a sequence of values is perhaps the most basic way to utilize the tree.
The Wavelet Tree stores the sequence and supports access, rank, and select queries on the sequence.

The Wavelet Tree can also be used to describe a stable reordering, e.g. of the symbols in a string S. 
If the leaves are traversed then all the occurrences of the smaller symbols are found first and within the leaf they are ordered by original position.
This means that the leaves of the symbols appear in ascending sorted order from left to right in the tree.
If one then has a permutation of a string e.g a string sorted in descending order and stores it in a wavelet tree, it is then possible to access the symbols in either ascending or descending order based on whether the symbol is tracked downwards through the tree until the corresponding leaf is found, or whether the symbol is tracked upwards from the leaf. 
The downward tracking would then result in an ascending order and the upward tracking would result in descending order.
The wavelet tree is therefore able to represent a reordering of a string and the order is based on how the alphabet is sorted.

A Wavelet Tree can also represent an $n \times n$ grid of $n$ points where no two points share the same row or column. 
One can map a general set of $n$ points to such a discrete grid and then store the real points somewhere else. If we have points sorted by the $x$-coordinate and take only the $y$-coordinates $S_y[1,n] = y_1,y_2,...,y_n$ and save $S_y$ in a Wavelet Tree we can the find the \textit{i}th point in $x$-coordinate order by accessing the corresponding $y$-coordinate in the wavelet tree. 
If we want the \textit{i}th point in $y$-coordinate order we can access the leaf of a given $y$-coordinate and find its corresponding $x$-coordinate by querying up through the tree until we find the original position of $y$ in $S$. 
The corresponding $x$-coordinate will be at the same position.
Querying from a leaf gives the points in $y$-coordinate order because the leaves are sorted by $y$-coordinate.

\subsection{Compression}
The Wavelet Tree has many uses for compression of data~\citeA{Navjda13}.
Some of the main compression techniques are different ways of encoding the bitmaps and changing the shape of the tree~\citeA[Section~3]{Navjda13}.

The main advantage of the wavelet tree with regards to compression is that it supports entropy bounds in the attained space complexity of the various wavelet tree compression methods~\citeA[Section~2.1]{WTSurvey}.

\subsubsection{Entropy}
According to~\citeA[Introduction]{WTSurvey} entropy can be defined as:

\begin{mdframed}[nobreak, linecolor=lightgray, linewidth=2pt]
\begin{definition}: Entropy \\\\
Let \textit{S} be a sequence of \textit{n} symbols from an alphabet $\Sigma = \lbrace c_1 ... c_\sigma \rbrace$ with cardinality $\sigma$.
Then \textit{entropy} H is defined as
\begin{center}
$H = \sum_{i=1}^{\sigma} p_i log_2(\frac{1}{p_i})$
\end{center}
where $p_i$ is the probability of the $i^{th}$ symbol in the alphabet appearing in \textit{S}.
\end{definition} 
\end{mdframed}

Entropy represents a lower bound to the average number of bits needed to represent each symbol in $S$ according to the coding theorem of Shannon~\citeA[Introduction]{WTSurvey} and is the bound that compression researchers compare their results to.

This theoretical definition of entropy is often replaced in scientific literature by a more practical definition: \textit{empirical entropy}.
There are two versions: \textit{empirical zero-order entropy} $H_0$ and \textit{empirical $k^{th}$-order entropy} $H_k$. $H_k$ takes into account a context of size \textit{k} of the symbol appearances, ie. the suffixes of length $k$ of each symbol appearance in the string, while $H_0$ does not and treats symbols independently instead. 

\begin{mdframed}[nobreak, linecolor=lightgray, linewidth=2pt]
\begin{definition}: Empirical zero-order entropy, $H_0$ \\
Let \textit{S} be a sequence of \textit{n} symbols from an alphabet $\Sigma = \lbrace c_1 ... c_\sigma \rbrace$.
Then \textit{entropy} $H_0$ is defined as
\begin{center}
$H_0 = H_0(S) = \sum\limits_{c_i \in \Sigma} \frac{n_i}{n} log_2(\frac{n}{n_i})$
\end{center}
where $n_i$ is the number of appearances of character $c_i$ in $S$.
\end{definition}
\begin{definition}: Empirical $k^{th}$-order entropy, $H_k$ \\
For a string $w \in \Sigma^k$ let us define $w_S$ as the concatenation of characters that follow \textit{w} in \textit{S}. 
Then the $k^{th}$-order empirical entropy of \textit{S}, is defined as follows
\begin{center}
$H_k = H_k(S) = \frac{1}{n} \sum\limits_{w \in \Sigma^k} | w_S |H_0(w_S)$
\end{center}
\end{definition}
\end{mdframed}
$H_k$ often defines a lower bound for bit space usage that is smaller than the lower bound of $H_0$~\citeA[Section~2]{waveletTreeEntropy}.

Using the Burrows-Wheeler transformation on the input we can reduce the problem of achieving $H_k$ compression to achieving $H_0$ compression.
In other words, if we have a good compression algorithm that achieves compression within the $H_0$ lower bound, then by using that algorithm on the Burrows-Wheeler transformation of the input we can achieve compression within the $H_k$ lower bound~\citeA[Introduction]{waveletTreeEntropy}.
The problem for a long time was that there existed no good way to achieve compression within the $H_0$ lower bound or at least it was a problem before the wavelet tree was invented~\citeA[Introduction]{waveletTreeEntropy}.

To achieve \textit{zero-order entropy} a Huffman shaped wavelet tree can be used~\citeA[Section~4]{FMcountOnBWT}. Claude and Navarro~\citeA[Section~3]{Claude08practicalrankselect} describes a way to also have \textit{zero-order entropy} space usage for large alphabets. 
It is therefore possible to get space usage within \textit{zero-order entropy} even for large alphabets using the wavelet tree. 
\textit{Zero-order entropy} can also be achieved by run-length encoding the bitmaps in the wavelet tree, which is also the approach that is used when compressing the BWT transformation of the input string using the wavelet tree~\citeA[Introduction (\textbf{B})]{waveletTreeEntropy}.
This means that using a combination of run-length encoding of bitmaps and taking the Burrows-Wheeler transformation of the input string using the wavelet tree it is possible to achieve compression near the lower bound of \textit{$k^{th}$-order entropy}.

\subsubsection{Run-Length encoding}
Run-length encoding (RLE) is a simple process where the number of consecutive repeats of each symbol is stored instead of storing the symbols themselves. 
If we have the string \textit{aaaaacccaaaaabbbaa} we can run-length encode this to \textit{a5c3a5b3a2} which is a smaller string containing the same information.
It is necessary to store the symbol and its number of consecutive repeating occurrences because we need to be able to identify which symbol occurs where and how many times in order to be able to reproduce the original string.
The longer the sequence of a repeating symbol is the less space is used, since it can be stored as one number plus the related symbol.

When representing the string using a wavelet tree, the problem gets reduced to run-length encoding a string of bits (the bitmap in each node).
Since a binary number only has an alphabet of size two it is not necessary to store both the symbol and its occurrence but only the occurrence, if we adopt the convention that the first number is always the amount of 0s and the second number is always the amount of 1s, continuing this trend for the entire string so that even-index numbers correspond to 0 and odd-index to 1. 
As an example, if we look at the bitmap of an input string \textit{aaaaacccaaaaabbbaa} which is \textit{000001110000000000} when stored in a wavelet tree, then it can be encoded and stored as the numbers $5~3~10$.
Figure~\ref{fig:RleWaveletTree} shows an example of a wavelet tree with run-length encoded bitmaps.

If we do not consider how a computer saves numbers but only consider the amount of numbers that needs to be stored then $RLE(000001110000000000)$ achieves a great reduction in space.
From 18 numbers to only 3 numbers, which contain the same information.

If we do consider how a computer saves numbers then the reduction is not that great because if we assume that each number is represented as an integer, then the run-length encoded bitmap uses more space than just storing the original bitmap. 
This is because an integer uses 4 bytes of space which is 32 bits and we need to store three integers giving us a total of $32 bits \times 3 = 96 bits$ which is significantly larger than just the 18 bits we need to store for the original bitmap assuming we can store 1 and 0 using only 1 bit \footnote{This can be accomplished using C++ and Vector$<$bool$>$}. 
This means that the symbols in the string need to, on average, repeat consecutively more than 32 times before RLE achieves better space usage than just storing the bitmaps.
This is assuming 4-byte integers are used to store the RLE values. 
To be able to support storing the RLE of any bitmap, even one containing only 0s or only 1s, the RLE values should be able to store as high a value as the bitmap is long, which might require more bytes per value.
A 32-bit unsigned integer supports storing a value up to $2^{32} = \num{4294967296}$. 
The limit on bitmap length is also the maximum supported length of input string for a wavelet tree.

RLE is still useful despite of this limitation because you usually want to compress massive amounts of data and if that data uses an alphabet that is small enough then, as previously stated RLE can achieve compression close to the zero-order entropy when working with binary alphabets.

If the Burrows-Wheeler transformation is applied to the string before it is saved in the wavelet tree and run-length encoded then the number of consecutive repeats of a symbol is increased which enables even greater compression.

\subsubsection{Burrows-Wheeler transformation}
The Burrows-Wheeler transformation transforms a string $S$ into a string of the same length with the same characters with the characteristic that characters are grouped into runs of similar characters.
This characteristic enables higher compression ratios when using techniques such as run-length encoding.
The transformation is reversible, meaning it is possible to produce the original string from the Burrows-Wheeler transformed string, without any other information.
Sorting $S$ would enable similar, or possibly better, compression ratios using run-length encoding, but it will not be reversible.

A string $S$ of $n$ characters is transformed by the Burrows-Wheeler transformation~\citeA[Section~2]{BWToriginalArticle} by forming $n$ cyclic shifts of $S$. 
These $n$ permutations of $S$ are then sorted in lexicographical order.
An extra character (\#), not in the alphabet of $S$, is added to keep track of the end of the original string.
The BWT of $S$ is then the concatenation of the last character of each permutation in sorted order, excluding $\#$.

\begin{figure}
\begin{center}
$\begin{bmatrix}
	bananahat\#\\
	ananahat\#b\\
	nanahat\#ba\\
	anahat\#ban\\
	nahat\#bana\\
	ahat\#banan\\
	hat\#banana\\
	at\#bananah\\
	t\#bananaha\\
	\#bananahat\\
\end{bmatrix} \Rightarrow
\begin{bmatrix}
	\#bananaha\textbf{t}\\
	ahat\#bana\textbf{n}\\
	anahat\#ba\textbf{n}\\
	ananahat\#\textbf{b}\\
	at\#banana\textbf{h}\\
	bananahat\#\\
	hat\#banan\textbf{a}\\
	nahat\#ban\textbf{a}\\
	nanahat\#b\textbf{a}\\
	t\#bananah\textbf{a}
\end{bmatrix}$
\end{center}
\caption{Example of a Burrows-Wheeler transformation of the string \textit{bananahat}}
\label{fig:BWTExample}
\end{figure}
In Figure~\ref{fig:BWTExample} we present an example transformation of the string \textit{bananahat}.
The list to the left in Figure~\ref{fig:BWTExample} is the cyclically shifted permutations of \textit{S} and the list to the right contains the same permutations, but in lexicographically sorted order.
The result of the Burrows-Wheeler transformation is then the characters at the last index in each column, highlighted in bold in Figure~\ref{fig:BWTExample}.
The Burrows-Wheeler transformation of $S=bananahat$ becomes BWT($S$)$ = tnnbhaaaa$.
The original string is identified by having a \# at the end.

Looking at BWT($S$) we can see that equal characters are now grouped together.
It would not make much sense to compress something without being able to decompress it again.
Burrows et al.~\citeA[Section~2]{BWToriginalArticle} describes an algorithm for getting the original string from the Burrows-Wheeler transformed string.
It works by taking the BWT, then sorting it and then adding the BWT in front of the sorted value and then sorting that. 
This procedure continues until the number of characters in each row is equal to the length of the BWT. 
The original string is then the value with the end of line character \# at the end. 
An example of the process in shown in Figure~\ref{fig:reverseBwt}.

\begin{figure}
\caption{Example of how to do reverse BWT on string "acd\#". The returned value is "dca\#".}
\label{fig:reverseBwt}
\center
\begin{tabular}{|l|l|l|l|l|l|l|}
\hline
Add 1 & Sort 1 & Add 2 & Sort 2 & Add 3 & Sort 3 & Add 4 \\
\hline
\begin{tabular}[c]{@{}l@{}}a\\ c\\ d\\ \#\end{tabular} & \begin{tabular}[c]{@{}l@{}}\#\\ a\\ c\\ d\end{tabular} & \begin{tabular}[c]{@{}l@{}}a\#\\ ca\\ dc\\ \#d\end{tabular} & \begin{tabular}[c]{@{}l@{}}\#d\\ a\#\\ ca\\ dc\end{tabular} & \begin{tabular}[c]{@{}l@{}}a\#d\\ ca\#\\ dca\\ \#dc\end{tabular} & \begin{tabular}[c]{@{}l@{}}\#dc\\ a\#d\\ ca\#\\ dca\end{tabular} & \begin{tabular}[c]{@{}l@{}}a\#dc\\ ca\#d\\ \textbf{dca\#}\\ \#dca\end{tabular} \\ \hline
\end{tabular}
\end{figure}


Figure~\ref{fig:RleBwtComparison} shows two small examples of wavelet trees using run-length encoding, one constructed on the string \textit{bananahat} (Figure~\ref{fig:RleWaveletTree}) and the other on the Burrows-Wheeler Transformation of \textit{bananahat} (Figure~\ref{fig:BwtRleWaveletTree}).
We can see in Figure~\ref{fig:BwtRleWaveletTree}, highlighted by the numbers in bold, that fewer Run-length encoded values needs to be stored than for the non-Burrows-Wheeler transformed string in Figure~\ref{fig:RleWaveletTree}.
\begin{figure}
      \begin{subfigure}{0.45\textwidth}      
			\Tree
			%root
			[.bananahat\\2,1,1,1,3,1 !\qsetw{3cm} 
				%left child
				[.baaaha\\1,2,1,1 !\qsetw{3cm}
					[.baaaa\\0,1,4 !\qsetw{3cm}
						[.aaaa !\qsetw{3cm} ]
						[.b !\qsetw{3cm} ]		
					] 
					[.h !\qsetw{3cm} ]
				] 
				%right child
				[.nnt\\2,1 !\qsetw{3cm}	
					[.nn !\qsetw{3cm} ] 
					[.t !\qsetw{3cm} ]
				]
			]
		\caption{RLE Wavelet Tree on string \textit{bananahat} with alphabet $\sigma = abhnt$}
      \label{fig:RleWaveletTree}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.45\textwidth}	
			\Tree
			%root
			[.tnnbhaaaa\\\textbf{0,3,6} !\qsetw{3cm} 
				%left child
				[.bhaaaa\\\textbf{1,1,4} !\qsetw{3cm} 
					[.baaaa\\0,1,4 !\qsetw{3cm} 
						[.aaaa !\qsetw{3cm} ]
						[.b !\qsetw{3cm} ]		
					] 
					[.h !\qsetw{3cm} ]
				] 
				%right child
				[.tnn\\0,1,2 !\qsetw{3cm}		
					[.nn !\qsetw{3cm} ] 
					[.t !\qsetw{3cm} ]
				]
			] 
		\caption{RLE Wavelet Tree on $BWT(bananahat)=tnnbhaaaa$ with alphabet $\sigma = abhnt$}
		\label{fig:BwtRleWaveletTree}
	\end{subfigure}
	\caption{Comparison of Wavelet Trees using Run-Length Encoding on a string and its Burrows-Wheeler Transformation}
	\label{fig:RleBwtComparison}
\end{figure}

One might wonder how \textproc{Rank} and \textproc{Select} queries can be useful when the input string is Burrows-Wheeler transformed, as the results of the queries become essentially unrelated to the original string $S$, without an obvious way of transforming the query results back to what they would have been on a tree constructed on the original non-transformed string.
We have not found any way to transform the results back, and the main use of constructing a Wavelet Tree on the BWT of a string seems to us, by far, to be compression.

Rank and select queries on the BWT of a string does, however, have uses when working with the \textit{FM-index}, named after its inventors Paolo Ferragina and Giovanni Manzini, which is a self-index based on the Burrows-Wheeler transformation $BWT(S)$ that is able to find occurrences and positions of patterns (sub-strings) in \textit{S} by looking at $BWT(S)$. 
Such a procedure is described by Mäkinen and Navarro~\citeA[Section 2]{FMcountOnBWT}.

\subsubsection{Huffman-shaped Wavelet Trees}
\label{sec:huffmanShapedWaveletTree}
Mäkinen and Navarro~\citeA[Section~4]{FMcountOnBWT} describes a Huffman Shaped Wavelet Tree which skews the tree to one side and places symbols with higher frequencies towards the other side so that they are closer to the root than those that have a lower frequency.
More precisely, they are placed in the tree in such a way that the path from the root to a leaf corresponds to the binary Huffman Code~\citeA[Introduction]{HuffmanCoding} of the symbol of that leaf.
Using a Huffman Shaped Wavelet Tree is an alternative to run-length encoding.

This approach skews the tree and as a result increases the height of the tree, which ordinarily would result in higher average query time, but by placing the most frequent symbols highest and least frequent symbols lowest, it decreases query time massively for symbols with high frequency.
Queries on a Huffman shaped Wavelet Tree for a symbol that has a high frequency then returns faster than queries for a symbol that was less frequent.
Assuming symbols that occur with high frequency are also queried for more often, the average query time could see a reduction when using a Huffman-shaped wavelet tree.

The Huffman Code~\citeA[Introduction]{HuffmanCoding} of a symbol occurring with high frequency is a shorter binary string than the Huffman code of a symbol occurring with low frequency.
The most frequent symbol could be encoded in as little as one bit!
This entails that the storage space required for the many occurrences of the most frequent symbols would be massively reduced, while the space required for the least frequent symbols would be increased.
If the difference in frequency is sufficiently high, the reduction in space for the most frequent symbols would outweigh the increase in space for the least frequent and the overall storage requirement would be reduced.

Since the Huffman encoding is based on frequency of symbols it achieves the best performance and space complexity when symbols are non-uniformly distributed.
If the data is uniformly distributed then the length of all Huffman codes would be similar resulting in a balanced tree having performance and space complexity similar to a normal Wavelet Tree.
Since we have primarily used uniformly distributed data in our experiments, and because of limited time, we have not implemented and done experiments on a Huffman Shaped Wavelet Tree.
In Section~\ref{sec:choiceOfInputString} we discuss further why we mainly consider uniform data.


\subsection{Information Retrieval}
A wavelet tree can be used to efficiently answer numerous queries in different problem domains.
In this section we describe in some detail a select number of information retrieval scenarios.

\subsubsection{Access, Rank, and Select Queries}
The wavelet tree supports access in $O(\log \sigma)$ time, but unless compression is also used, the access query is not the main reason to use a wavelet tree.
Instead, the more interesting queries are rank and select queries, that are useful in and of themselves e.g. when used as a dictionary and a self-index, but are also the building blocks of many other queries and algorithms.

The rank query for a symbol $s$ and a position $p$ will return how many of symbol $s$ occurs in the input string up to position $p$.
The select query for symbol $s$ and occurrence parameter $o$ will return the position of the $o^{th}$ occurrence of $s$ in the input string.

G. Navarro~\citeA[Section 5]{Navjda13} points to an application found by Ferragina and Manzini~\citeB[Section 3]{Opportunisticdatastructureswithapplications} that uses access and rank\footnote{Ferragina and Manzini calls rank queries "Occ" in their paper} queries to find the number of occurrences of a pattern $p$ in a string $S$ by storing and querying the Burrows-Wheeler transformation of the string $S^{BWT}$, enabling compression along with efficient query times.
G. Navarro~\citeA[Section 5]{Navjda13} also point to other similar results and improvements on the previous results by others, showing there is a wide interest in using wavelet trees to store a sequence and query for the occurrences of patterns within that sequence.

G. Navarro~\citeA[Section 5]{Navjda13} further points out the uses of a wavelet tree as a positional inverted index.
By storing the list of word identifiers in the wavelet tree both the text itself and the inverted index is stored.
Access queries will then return the word at the given position while select$_{c}$($S$, $i$) can be used to get the $i$th word in the inverted list of a word $c$ for the string $S$.
Rank queries can be used effectively in some list intersection algorithms.
The efficiency can be improved by using multi-ary wavelet trees or Huffman-shaped wavelet trees as the non-uniformity of word usage in language makes it a good candidate for Huffman coding.

The positional inverted index application can also be extended to \textit{document retrieval}~\citeA[Section 5]{Navjda13} by introducing a document boundary character such as \$ and storing the concatenation of all the documents with the document boundary character in between each.
The first document containing some word $c$ is document number $j = \textproc{Rank}_\$(S, p) + 1$ where $p = \textproc{Select}_c(S, 1)$.
Document $j$ ends at position $p' = \textproc{Select}_\$(S,j)$ and contains $o = \textproc{Rank}_c(S,p') - \textproc{Rank}_c(S,p)$ occurrences of the word $c$.
The next occurrence of the word $c$ in another document is at $p_{next} = \textproc{Select}_c(S, o+1)$.


\subsubsection{Range Quantile Query}
A range quantile query is a query that returns the $k^{th}$ smallest number within a subsequence of a given sequence of elements.
If we are e.g. given a list of price changes on a laptop during the last year then a range quantile query is able to answer what the $k$-smallest price of the laptop was within for instance a month of that year.
It is therefore also easy to find quantiles like the 2-quantile (median) or the 3-quantile.
To e.g. find the median, $k$ can be defined as half the length of the subsequence.
This would return the middle element of the subsequence.
The 3-Quantile can be found by setting $k$ to $\frac{1}{3}$ of the length of the subsequence. 
Quantiles are important values within such fields as statistics and economics.

Range Quantile queries especially are interesting to us because they do not require any changes to the wavelet tree and uses it in its simple form. Our optimizations can therefore be applied directly without modification.

Gagie et al.~\citeA{RangeQuantileQueries} show how the wavelet tree can be used to support efficient range quantile queries on a sequence \textit{S} of \textit{n} numbers in $O(\log(\sigma))$ time if $rank_b$ is supported in $O(1)$ time~\citeA[Section 3]{RangeQuantileQueries}. 
The range is denoted as $S[l ... r]$.
A range quantile query based on a wavelet tree works by computing two rank queries on the bitmap of each node in a traversal from the root to a leaf node. 

\begin{algorithm}
\caption{Range Quantile Query}
\label{alg:RangeQuantileQuery}
\begin{algorithmic}
\Function {RangeQuantileQuery} {$k,l,r$}
\If{current node is leaf}
	\State \Return number in leaf
\EndIf
\State $\textit{0}sInRange \gets rank_0(S \lbrace l...r \rbrace) = rank_0(r) - rank_0(l-1)$
\If{$\textit{0}sInRange \leq k$}
	\State $l = rank_0(l-1))+1$
	\State $r = rank_0(r)$
	\State \Return $LeftNode.RangeQuantileQuery(k,l,r)$
\Else
	\State $k = k - \textit{0}sInRange)$
	\State $l = rank_1(l-1)+1$
	\State $r = rank_1(r)$
	\State \Return $RightNode.RangeQuantileQuery(k,l,r)$
\EndIf

\EndFunction
\end{algorithmic}
\end{algorithm}

The two queries are $rank_b(l-1)$ and $rank_b(r)$ where $rank_b$ is the binary rank.
$rank_b(l-1)$ is used to find the number of 1s and 0s in $b[1...(l-1)]$ and $rank_b(r) - rank_b(l-1)$ gives the number 1s and 0s in $b[l...r]$. 
The algorithm goes to the left if there are more than \textit{k} 0s in $b[l...r]$ and set $l = ($number of 0s in $b[1...(l-1)])+1$ and $r=($number of 0s in $b[1...r])$. 
The algorithm goes to the right if there are less than \textit{k} 0s in $b[l...r]$ and subtract the number of 0s in $b[l...r]$ from \textit{k} and set $l = ($number of 1s in $b[1...(l-1)])+1$ and set $r=($number of 1s in $b[1...r])$. 
This procedure continues recursively until it hits a leaf and then returns the number stored in the leaf which corresponds to the $k^{th}$ smallest number in $S[l...r]$.

Algorithm~\ref{alg:RangeQuantileQuery} describes the pseudo-code for a range quantile query where $rank_1$ is a binary rank query which returns the number of 1s in the bitmap of $S$ in each node generated by the wavelet tree. 
$rank_0$ return the number of 0s.
The argument to $rank_0$ and $rank_1$ is the position to find the number of occurrences up to.
This means that $rank_1(r)$ for instance returns the number of 1s in the bitmap until position $r$.

\figureBegin
\begin{minipage}{.45\linewidth}
\begin{flushleft}                           
\Tree
%root
[.6,2,\textbf{0,7,9,3,1,8,5},4\\1001100110 !\qsetw{5cm} 
	%left child
	[.2,0,3,1,4\\00101  !\qsetw{5cm}
		%left -> left
		[.2,0,1\\100 !\qsetw{5cm} 
			[.0,1\\01 
				[.0 ]
				[.1 ] 			
			]
			[.2 ]		
		] 
		%left -> right
		[.3,4\\01 !\qsetw{5cm} 
			[.3 ]
			[.4 ]		
		]
	] 
	%right child
	[.6-\textbf{7,9,8,5}\\00110 !\qsetw{5cm} 
		%right -> left
		[.6,\textbf{7,5}\\010 !\qsetw{5.3cm}
			%right -> left -> left
			[.6,5\\10 
				[.5 ]
				[.6 ]			
			] 
			%right -> left -> right
			[.\textbf{7} ] 
		]
		%right -> right	 
		[.9,8\\10 !\qsetw{5.3cm} 
			[.8 ]
			[.9 ]		
		]
	]
] 
\end{flushleft} 
\end{minipage}
\hfill
\begin{minipage}{.45\linewidth}
\begin{flushright}
Level 1:\\$k=5$\\$l=3$\\$r=9$\\ \vspace{0.5cm}
Level 2:\\$k=2$\\$l=2$\\$r=5$\\ \vspace{0.5cm}
level 3:\\$k=2$\\$l=2$\\$r=3$\\ \vspace{0.5cm}
Level 4:\\$k=1$\\$l=1$\\$r=1$
\end{flushright} 
\end{minipage}
\caption{Range Quantile Query on a Wavelet Tree. $S=\lbrace 6,2,0,7,9,3,1,8,5,4 \rbrace, k=5, l=3, r=9$.}
\label{fig:RQQWaveletTree}
\figureEnd

An example of a range quantile query can be seen in Figure~\ref{fig:RQQWaveletTree}. 
The numbers in bold indicates the range $S[l...r]$ where $S= \lbrace 6,2,0,7,9,3,1,8,5,4 \rbrace$ and $l=3$, $r=9$ and $k=5$.
$k=5$ means that we are looking for the 5th smallest number within $S[l...r]$ which is \textit{7} indicated by the leaf that the query ends up in before terminating.
$l$ and $r$ indicates the range to look within.
The right side of the figure shows how \textit{k, l} and \textit{r} develops in each recursive call of the Range Quantile Query.